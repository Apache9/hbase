// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: ClusterStatus.proto

package org.apache.hadoop.hbase.protobuf.generated;

public final class ClusterStatusProtos {
  private ClusterStatusProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  /**
   * Protobuf enum {@code Option}
   */
  public enum Option
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>HBASE_VERSION = 0;</code>
     */
    HBASE_VERSION(0, 0),
    /**
     * <code>CLUSTER_ID = 1;</code>
     */
    CLUSTER_ID(1, 1),
    /**
     * <code>LIVE_SERVERS = 2;</code>
     */
    LIVE_SERVERS(2, 2),
    /**
     * <code>DEAD_SERVERS = 3;</code>
     */
    DEAD_SERVERS(3, 3),
    /**
     * <code>MASTER = 4;</code>
     */
    MASTER(4, 4),
    /**
     * <code>BACKUP_MASTERS = 5;</code>
     */
    BACKUP_MASTERS(5, 5),
    /**
     * <code>MASTER_COPROCESSORS = 6;</code>
     */
    MASTER_COPROCESSORS(6, 6),
    /**
     * <code>REGIONS_IN_TRANSITION = 7;</code>
     */
    REGIONS_IN_TRANSITION(7, 7),
    /**
     * <code>BALANCER_ON = 8;</code>
     */
    BALANCER_ON(8, 8),
    /**
     * <code>MASTER_INFO_PORT = 9;</code>
     */
    MASTER_INFO_PORT(9, 9),
    /**
     * <code>SERVERS_NAME = 10;</code>
     */
    SERVERS_NAME(10, 10),
    ;

    /**
     * <code>HBASE_VERSION = 0;</code>
     */
    public static final int HBASE_VERSION_VALUE = 0;
    /**
     * <code>CLUSTER_ID = 1;</code>
     */
    public static final int CLUSTER_ID_VALUE = 1;
    /**
     * <code>LIVE_SERVERS = 2;</code>
     */
    public static final int LIVE_SERVERS_VALUE = 2;
    /**
     * <code>DEAD_SERVERS = 3;</code>
     */
    public static final int DEAD_SERVERS_VALUE = 3;
    /**
     * <code>MASTER = 4;</code>
     */
    public static final int MASTER_VALUE = 4;
    /**
     * <code>BACKUP_MASTERS = 5;</code>
     */
    public static final int BACKUP_MASTERS_VALUE = 5;
    /**
     * <code>MASTER_COPROCESSORS = 6;</code>
     */
    public static final int MASTER_COPROCESSORS_VALUE = 6;
    /**
     * <code>REGIONS_IN_TRANSITION = 7;</code>
     */
    public static final int REGIONS_IN_TRANSITION_VALUE = 7;
    /**
     * <code>BALANCER_ON = 8;</code>
     */
    public static final int BALANCER_ON_VALUE = 8;
    /**
     * <code>MASTER_INFO_PORT = 9;</code>
     */
    public static final int MASTER_INFO_PORT_VALUE = 9;
    /**
     * <code>SERVERS_NAME = 10;</code>
     */
    public static final int SERVERS_NAME_VALUE = 10;


    public final int getNumber() { return value; }

    public static Option valueOf(int value) {
      switch (value) {
        case 0: return HBASE_VERSION;
        case 1: return CLUSTER_ID;
        case 2: return LIVE_SERVERS;
        case 3: return DEAD_SERVERS;
        case 4: return MASTER;
        case 5: return BACKUP_MASTERS;
        case 6: return MASTER_COPROCESSORS;
        case 7: return REGIONS_IN_TRANSITION;
        case 8: return BALANCER_ON;
        case 9: return MASTER_INFO_PORT;
        case 10: return SERVERS_NAME;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<Option>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<Option>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<Option>() {
            public Option findValueByNumber(int number) {
              return Option.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final Option[] VALUES = values();

    public static Option valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private Option(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:Option)
  }

  public interface RegionStateOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .RegionInfo region_info = 1;
    /**
     * <code>required .RegionInfo region_info = 1;</code>
     */
    boolean hasRegionInfo();
    /**
     * <code>required .RegionInfo region_info = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo getRegionInfo();
    /**
     * <code>required .RegionInfo region_info = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder getRegionInfoOrBuilder();

    // required .RegionState.State state = 2;
    /**
     * <code>required .RegionState.State state = 2;</code>
     */
    boolean hasState();
    /**
     * <code>required .RegionState.State state = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State getState();

    // optional uint64 stamp = 3;
    /**
     * <code>optional uint64 stamp = 3;</code>
     */
    boolean hasStamp();
    /**
     * <code>optional uint64 stamp = 3;</code>
     */
    long getStamp();
  }
  /**
   * Protobuf type {@code RegionState}
   */
  public static final class RegionState extends
      com.google.protobuf.GeneratedMessage
      implements RegionStateOrBuilder {
    // Use RegionState.newBuilder() to construct.
    private RegionState(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegionState(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegionState defaultInstance;
    public static RegionState getDefaultInstance() {
      return defaultInstance;
    }

    public RegionState getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegionState(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = regionInfo_.toBuilder();
              }
              regionInfo_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(regionInfo_);
                regionInfo_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State value = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                state_ = value;
              }
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              stamp_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionState_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionState_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder.class);
    }

    public static com.google.protobuf.Parser<RegionState> PARSER =
        new com.google.protobuf.AbstractParser<RegionState>() {
      public RegionState parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegionState(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegionState> getParserForType() {
      return PARSER;
    }

    /**
     * Protobuf enum {@code RegionState.State}
     */
    public enum State
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>OFFLINE = 0;</code>
       *
       * <pre>
       * region is in an offline state
       * </pre>
       */
      OFFLINE(0, 0),
      /**
       * <code>PENDING_OPEN = 1;</code>
       *
       * <pre>
       * sent rpc to server to open but has not begun
       * </pre>
       */
      PENDING_OPEN(1, 1),
      /**
       * <code>OPENING = 2;</code>
       *
       * <pre>
       * server has begun to open but not yet done
       * </pre>
       */
      OPENING(2, 2),
      /**
       * <code>OPEN = 3;</code>
       *
       * <pre>
       * server opened region and updated meta
       * </pre>
       */
      OPEN(3, 3),
      /**
       * <code>PENDING_CLOSE = 4;</code>
       *
       * <pre>
       * sent rpc to server to close but has not begun
       * </pre>
       */
      PENDING_CLOSE(4, 4),
      /**
       * <code>CLOSING = 5;</code>
       *
       * <pre>
       * server has begun to close but not yet done
       * </pre>
       */
      CLOSING(5, 5),
      /**
       * <code>CLOSED = 6;</code>
       *
       * <pre>
       * server closed region and updated meta
       * </pre>
       */
      CLOSED(6, 6),
      /**
       * <code>SPLITTING = 7;</code>
       *
       * <pre>
       * server started split of a region
       * </pre>
       */
      SPLITTING(7, 7),
      /**
       * <code>SPLIT = 8;</code>
       *
       * <pre>
       * server completed split of a region
       * </pre>
       */
      SPLIT(8, 8),
      /**
       * <code>FAILED_OPEN = 9;</code>
       *
       * <pre>
       * failed to open, and won't retry any more
       * </pre>
       */
      FAILED_OPEN(9, 9),
      /**
       * <code>FAILED_CLOSE = 10;</code>
       *
       * <pre>
       * failed to close, and won't retry any more
       * </pre>
       */
      FAILED_CLOSE(10, 10),
      /**
       * <code>MERGING = 11;</code>
       *
       * <pre>
       * server started merge a region
       * </pre>
       */
      MERGING(11, 11),
      /**
       * <code>MERGED = 12;</code>
       *
       * <pre>
       * server completed merge of a region
       * </pre>
       */
      MERGED(12, 12),
      /**
       * <code>SPLITTING_NEW = 13;</code>
       *
       * <pre>
       * new region to be created when RS splits a parent
       * </pre>
       */
      SPLITTING_NEW(13, 13),
      /**
       * <code>MERGING_NEW = 14;</code>
       *
       * <pre>
       * region but hasn't be created yet, or master doesn't
       * know it's already created
       * </pre>
       */
      MERGING_NEW(14, 14),
      ;

      /**
       * <code>OFFLINE = 0;</code>
       *
       * <pre>
       * region is in an offline state
       * </pre>
       */
      public static final int OFFLINE_VALUE = 0;
      /**
       * <code>PENDING_OPEN = 1;</code>
       *
       * <pre>
       * sent rpc to server to open but has not begun
       * </pre>
       */
      public static final int PENDING_OPEN_VALUE = 1;
      /**
       * <code>OPENING = 2;</code>
       *
       * <pre>
       * server has begun to open but not yet done
       * </pre>
       */
      public static final int OPENING_VALUE = 2;
      /**
       * <code>OPEN = 3;</code>
       *
       * <pre>
       * server opened region and updated meta
       * </pre>
       */
      public static final int OPEN_VALUE = 3;
      /**
       * <code>PENDING_CLOSE = 4;</code>
       *
       * <pre>
       * sent rpc to server to close but has not begun
       * </pre>
       */
      public static final int PENDING_CLOSE_VALUE = 4;
      /**
       * <code>CLOSING = 5;</code>
       *
       * <pre>
       * server has begun to close but not yet done
       * </pre>
       */
      public static final int CLOSING_VALUE = 5;
      /**
       * <code>CLOSED = 6;</code>
       *
       * <pre>
       * server closed region and updated meta
       * </pre>
       */
      public static final int CLOSED_VALUE = 6;
      /**
       * <code>SPLITTING = 7;</code>
       *
       * <pre>
       * server started split of a region
       * </pre>
       */
      public static final int SPLITTING_VALUE = 7;
      /**
       * <code>SPLIT = 8;</code>
       *
       * <pre>
       * server completed split of a region
       * </pre>
       */
      public static final int SPLIT_VALUE = 8;
      /**
       * <code>FAILED_OPEN = 9;</code>
       *
       * <pre>
       * failed to open, and won't retry any more
       * </pre>
       */
      public static final int FAILED_OPEN_VALUE = 9;
      /**
       * <code>FAILED_CLOSE = 10;</code>
       *
       * <pre>
       * failed to close, and won't retry any more
       * </pre>
       */
      public static final int FAILED_CLOSE_VALUE = 10;
      /**
       * <code>MERGING = 11;</code>
       *
       * <pre>
       * server started merge a region
       * </pre>
       */
      public static final int MERGING_VALUE = 11;
      /**
       * <code>MERGED = 12;</code>
       *
       * <pre>
       * server completed merge of a region
       * </pre>
       */
      public static final int MERGED_VALUE = 12;
      /**
       * <code>SPLITTING_NEW = 13;</code>
       *
       * <pre>
       * new region to be created when RS splits a parent
       * </pre>
       */
      public static final int SPLITTING_NEW_VALUE = 13;
      /**
       * <code>MERGING_NEW = 14;</code>
       *
       * <pre>
       * region but hasn't be created yet, or master doesn't
       * know it's already created
       * </pre>
       */
      public static final int MERGING_NEW_VALUE = 14;


      public final int getNumber() { return value; }

      public static State valueOf(int value) {
        switch (value) {
          case 0: return OFFLINE;
          case 1: return PENDING_OPEN;
          case 2: return OPENING;
          case 3: return OPEN;
          case 4: return PENDING_CLOSE;
          case 5: return CLOSING;
          case 6: return CLOSED;
          case 7: return SPLITTING;
          case 8: return SPLIT;
          case 9: return FAILED_OPEN;
          case 10: return FAILED_CLOSE;
          case 11: return MERGING;
          case 12: return MERGED;
          case 13: return SPLITTING_NEW;
          case 14: return MERGING_NEW;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<State>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static com.google.protobuf.Internal.EnumLiteMap<State>
          internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<State>() {
              public State findValueByNumber(int number) {
                return State.valueOf(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(index);
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDescriptor().getEnumTypes().get(0);
      }

      private static final State[] VALUES = values();

      public static State valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }

      private final int index;
      private final int value;

      private State(int index, int value) {
        this.index = index;
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:RegionState.State)
    }

    private int bitField0_;
    // required .RegionInfo region_info = 1;
    public static final int REGION_INFO_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo regionInfo_;
    /**
     * <code>required .RegionInfo region_info = 1;</code>
     */
    public boolean hasRegionInfo() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .RegionInfo region_info = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo getRegionInfo() {
      return regionInfo_;
    }
    /**
     * <code>required .RegionInfo region_info = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder getRegionInfoOrBuilder() {
      return regionInfo_;
    }

    // required .RegionState.State state = 2;
    public static final int STATE_FIELD_NUMBER = 2;
    private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State state_;
    /**
     * <code>required .RegionState.State state = 2;</code>
     */
    public boolean hasState() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .RegionState.State state = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State getState() {
      return state_;
    }

    // optional uint64 stamp = 3;
    public static final int STAMP_FIELD_NUMBER = 3;
    private long stamp_;
    /**
     * <code>optional uint64 stamp = 3;</code>
     */
    public boolean hasStamp() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint64 stamp = 3;</code>
     */
    public long getStamp() {
      return stamp_;
    }

    private void initFields() {
      regionInfo_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
      state_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State.OFFLINE;
      stamp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasRegionInfo()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasState()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegionInfo().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, regionInfo_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, state_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, stamp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, regionInfo_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, state_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, stamp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState) obj;

      boolean result = true;
      result = result && (hasRegionInfo() == other.hasRegionInfo());
      if (hasRegionInfo()) {
        result = result && getRegionInfo()
            .equals(other.getRegionInfo());
      }
      result = result && (hasState() == other.hasState());
      if (hasState()) {
        result = result &&
            (getState() == other.getState());
      }
      result = result && (hasStamp() == other.hasStamp());
      if (hasStamp()) {
        result = result && (getStamp()
            == other.getStamp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasRegionInfo()) {
        hash = (37 * hash) + REGION_INFO_FIELD_NUMBER;
        hash = (53 * hash) + getRegionInfo().hashCode();
      }
      if (hasState()) {
        hash = (37 * hash) + STATE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getState());
      }
      if (hasStamp()) {
        hash = (37 * hash) + STAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getStamp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code RegionState}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionStateOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionState_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionState_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRegionInfoFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (regionInfoBuilder_ == null) {
          regionInfo_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
        } else {
          regionInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        state_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State.OFFLINE;
        bitField0_ = (bitField0_ & ~0x00000002);
        stamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionState_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (regionInfoBuilder_ == null) {
          result.regionInfo_ = regionInfo_;
        } else {
          result.regionInfo_ = regionInfoBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.state_ = state_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.stamp_ = stamp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDefaultInstance()) return this;
        if (other.hasRegionInfo()) {
          mergeRegionInfo(other.getRegionInfo());
        }
        if (other.hasState()) {
          setState(other.getState());
        }
        if (other.hasStamp()) {
          setStamp(other.getStamp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasRegionInfo()) {
          
          return false;
        }
        if (!hasState()) {
          
          return false;
        }
        if (!getRegionInfo().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .RegionInfo region_info = 1;
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo regionInfo_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder> regionInfoBuilder_;
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public boolean hasRegionInfo() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo getRegionInfo() {
        if (regionInfoBuilder_ == null) {
          return regionInfo_;
        } else {
          return regionInfoBuilder_.getMessage();
        }
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public Builder setRegionInfo(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo value) {
        if (regionInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          regionInfo_ = value;
          onChanged();
        } else {
          regionInfoBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public Builder setRegionInfo(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder builderForValue) {
        if (regionInfoBuilder_ == null) {
          regionInfo_ = builderForValue.build();
          onChanged();
        } else {
          regionInfoBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public Builder mergeRegionInfo(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo value) {
        if (regionInfoBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              regionInfo_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance()) {
            regionInfo_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.newBuilder(regionInfo_).mergeFrom(value).buildPartial();
          } else {
            regionInfo_ = value;
          }
          onChanged();
        } else {
          regionInfoBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public Builder clearRegionInfo() {
        if (regionInfoBuilder_ == null) {
          regionInfo_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
          onChanged();
        } else {
          regionInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder getRegionInfoBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegionInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder getRegionInfoOrBuilder() {
        if (regionInfoBuilder_ != null) {
          return regionInfoBuilder_.getMessageOrBuilder();
        } else {
          return regionInfo_;
        }
      }
      /**
       * <code>required .RegionInfo region_info = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder> 
          getRegionInfoFieldBuilder() {
        if (regionInfoBuilder_ == null) {
          regionInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder>(
                  regionInfo_,
                  getParentForChildren(),
                  isClean());
          regionInfo_ = null;
        }
        return regionInfoBuilder_;
      }

      // required .RegionState.State state = 2;
      private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State state_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State.OFFLINE;
      /**
       * <code>required .RegionState.State state = 2;</code>
       */
      public boolean hasState() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .RegionState.State state = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State getState() {
        return state_;
      }
      /**
       * <code>required .RegionState.State state = 2;</code>
       */
      public Builder setState(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        state_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .RegionState.State state = 2;</code>
       */
      public Builder clearState() {
        bitField0_ = (bitField0_ & ~0x00000002);
        state_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.State.OFFLINE;
        onChanged();
        return this;
      }

      // optional uint64 stamp = 3;
      private long stamp_ ;
      /**
       * <code>optional uint64 stamp = 3;</code>
       */
      public boolean hasStamp() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 stamp = 3;</code>
       */
      public long getStamp() {
        return stamp_;
      }
      /**
       * <code>optional uint64 stamp = 3;</code>
       */
      public Builder setStamp(long value) {
        bitField0_ |= 0x00000004;
        stamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 stamp = 3;</code>
       */
      public Builder clearStamp() {
        bitField0_ = (bitField0_ & ~0x00000004);
        stamp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:RegionState)
    }

    static {
      defaultInstance = new RegionState(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:RegionState)
  }

  public interface RegionInTransitionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .RegionSpecifier spec = 1;
    /**
     * <code>required .RegionSpecifier spec = 1;</code>
     */
    boolean hasSpec();
    /**
     * <code>required .RegionSpecifier spec = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getSpec();
    /**
     * <code>required .RegionSpecifier spec = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getSpecOrBuilder();

    // required .RegionState region_state = 2;
    /**
     * <code>required .RegionState region_state = 2;</code>
     */
    boolean hasRegionState();
    /**
     * <code>required .RegionState region_state = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState getRegionState();
    /**
     * <code>required .RegionState region_state = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionStateOrBuilder getRegionStateOrBuilder();
  }
  /**
   * Protobuf type {@code RegionInTransition}
   */
  public static final class RegionInTransition extends
      com.google.protobuf.GeneratedMessage
      implements RegionInTransitionOrBuilder {
    // Use RegionInTransition.newBuilder() to construct.
    private RegionInTransition(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegionInTransition(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegionInTransition defaultInstance;
    public static RegionInTransition getDefaultInstance() {
      return defaultInstance;
    }

    public RegionInTransition getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegionInTransition(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = spec_.toBuilder();
              }
              spec_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(spec_);
                spec_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = regionState_.toBuilder();
              }
              regionState_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(regionState_);
                regionState_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionInTransition_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionInTransition_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder.class);
    }

    public static com.google.protobuf.Parser<RegionInTransition> PARSER =
        new com.google.protobuf.AbstractParser<RegionInTransition>() {
      public RegionInTransition parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegionInTransition(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegionInTransition> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .RegionSpecifier spec = 1;
    public static final int SPEC_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier spec_;
    /**
     * <code>required .RegionSpecifier spec = 1;</code>
     */
    public boolean hasSpec() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .RegionSpecifier spec = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getSpec() {
      return spec_;
    }
    /**
     * <code>required .RegionSpecifier spec = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getSpecOrBuilder() {
      return spec_;
    }

    // required .RegionState region_state = 2;
    public static final int REGION_STATE_FIELD_NUMBER = 2;
    private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState regionState_;
    /**
     * <code>required .RegionState region_state = 2;</code>
     */
    public boolean hasRegionState() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .RegionState region_state = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState getRegionState() {
      return regionState_;
    }
    /**
     * <code>required .RegionState region_state = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionStateOrBuilder getRegionStateOrBuilder() {
      return regionState_;
    }

    private void initFields() {
      spec_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
      regionState_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasSpec()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasRegionState()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getSpec().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegionState().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, spec_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, regionState_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, spec_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, regionState_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition) obj;

      boolean result = true;
      result = result && (hasSpec() == other.hasSpec());
      if (hasSpec()) {
        result = result && getSpec()
            .equals(other.getSpec());
      }
      result = result && (hasRegionState() == other.hasRegionState());
      if (hasRegionState()) {
        result = result && getRegionState()
            .equals(other.getRegionState());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasSpec()) {
        hash = (37 * hash) + SPEC_FIELD_NUMBER;
        hash = (53 * hash) + getSpec().hashCode();
      }
      if (hasRegionState()) {
        hash = (37 * hash) + REGION_STATE_FIELD_NUMBER;
        hash = (53 * hash) + getRegionState().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code RegionInTransition}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionInTransition_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionInTransition_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getSpecFieldBuilder();
          getRegionStateFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (specBuilder_ == null) {
          spec_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
        } else {
          specBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (regionStateBuilder_ == null) {
          regionState_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDefaultInstance();
        } else {
          regionStateBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionInTransition_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (specBuilder_ == null) {
          result.spec_ = spec_;
        } else {
          result.spec_ = specBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (regionStateBuilder_ == null) {
          result.regionState_ = regionState_;
        } else {
          result.regionState_ = regionStateBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.getDefaultInstance()) return this;
        if (other.hasSpec()) {
          mergeSpec(other.getSpec());
        }
        if (other.hasRegionState()) {
          mergeRegionState(other.getRegionState());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasSpec()) {
          
          return false;
        }
        if (!hasRegionState()) {
          
          return false;
        }
        if (!getSpec().isInitialized()) {
          
          return false;
        }
        if (!getRegionState().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .RegionSpecifier spec = 1;
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier spec_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> specBuilder_;
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public boolean hasSpec() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getSpec() {
        if (specBuilder_ == null) {
          return spec_;
        } else {
          return specBuilder_.getMessage();
        }
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public Builder setSpec(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
        if (specBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          spec_ = value;
          onChanged();
        } else {
          specBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public Builder setSpec(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder builderForValue) {
        if (specBuilder_ == null) {
          spec_ = builderForValue.build();
          onChanged();
        } else {
          specBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public Builder mergeSpec(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
        if (specBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              spec_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance()) {
            spec_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder(spec_).mergeFrom(value).buildPartial();
          } else {
            spec_ = value;
          }
          onChanged();
        } else {
          specBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public Builder clearSpec() {
        if (specBuilder_ == null) {
          spec_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
          onChanged();
        } else {
          specBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder getSpecBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getSpecFieldBuilder().getBuilder();
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getSpecOrBuilder() {
        if (specBuilder_ != null) {
          return specBuilder_.getMessageOrBuilder();
        } else {
          return spec_;
        }
      }
      /**
       * <code>required .RegionSpecifier spec = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> 
          getSpecFieldBuilder() {
        if (specBuilder_ == null) {
          specBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder>(
                  spec_,
                  getParentForChildren(),
                  isClean());
          spec_ = null;
        }
        return specBuilder_;
      }

      // required .RegionState region_state = 2;
      private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState regionState_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionStateOrBuilder> regionStateBuilder_;
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public boolean hasRegionState() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState getRegionState() {
        if (regionStateBuilder_ == null) {
          return regionState_;
        } else {
          return regionStateBuilder_.getMessage();
        }
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public Builder setRegionState(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState value) {
        if (regionStateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          regionState_ = value;
          onChanged();
        } else {
          regionStateBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public Builder setRegionState(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder builderForValue) {
        if (regionStateBuilder_ == null) {
          regionState_ = builderForValue.build();
          onChanged();
        } else {
          regionStateBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public Builder mergeRegionState(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState value) {
        if (regionStateBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              regionState_ != org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDefaultInstance()) {
            regionState_ =
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.newBuilder(regionState_).mergeFrom(value).buildPartial();
          } else {
            regionState_ = value;
          }
          onChanged();
        } else {
          regionStateBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public Builder clearRegionState() {
        if (regionStateBuilder_ == null) {
          regionState_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.getDefaultInstance();
          onChanged();
        } else {
          regionStateBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder getRegionStateBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getRegionStateFieldBuilder().getBuilder();
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionStateOrBuilder getRegionStateOrBuilder() {
        if (regionStateBuilder_ != null) {
          return regionStateBuilder_.getMessageOrBuilder();
        } else {
          return regionState_;
        }
      }
      /**
       * <code>required .RegionState region_state = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionStateOrBuilder> 
          getRegionStateFieldBuilder() {
        if (regionStateBuilder_ == null) {
          regionStateBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionState.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionStateOrBuilder>(
                  regionState_,
                  getParentForChildren(),
                  isClean());
          regionState_ = null;
        }
        return regionStateBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:RegionInTransition)
    }

    static {
      defaultInstance = new RegionInTransition(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:RegionInTransition)
  }

  public interface FamilyInfoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string familyname = 1;
    /**
     * <code>required string familyname = 1;</code>
     *
     * <pre>
     ** family name 
     * </pre>
     */
    boolean hasFamilyname();
    /**
     * <code>required string familyname = 1;</code>
     *
     * <pre>
     ** family name 
     * </pre>
     */
    java.lang.String getFamilyname();
    /**
     * <code>required string familyname = 1;</code>
     *
     * <pre>
     ** family name 
     * </pre>
     */
    com.google.protobuf.ByteString
        getFamilynameBytes();

    // required uint64 row_count = 2;
    /**
     * <code>required uint64 row_count = 2;</code>
     *
     * <pre>
     ** total row count 
     * </pre>
     */
    boolean hasRowCount();
    /**
     * <code>required uint64 row_count = 2;</code>
     *
     * <pre>
     ** total row count 
     * </pre>
     */
    long getRowCount();

    // required uint64 kv_count = 3;
    /**
     * <code>required uint64 kv_count = 3;</code>
     *
     * <pre>
     ** total kv count 
     * </pre>
     */
    boolean hasKvCount();
    /**
     * <code>required uint64 kv_count = 3;</code>
     *
     * <pre>
     ** total kv count 
     * </pre>
     */
    long getKvCount();

    // required uint64 del_kv_count = 4;
    /**
     * <code>required uint64 del_kv_count = 4;</code>
     *
     * <pre>
     ** total delete kv count 
     * </pre>
     */
    boolean hasDelKvCount();
    /**
     * <code>required uint64 del_kv_count = 4;</code>
     *
     * <pre>
     ** total delete kv count 
     * </pre>
     */
    long getDelKvCount();

    // required uint64 del_family_count = 5;
    /**
     * <code>required uint64 del_family_count = 5;</code>
     *
     * <pre>
     ** total del family count 
     * </pre>
     */
    boolean hasDelFamilyCount();
    /**
     * <code>required uint64 del_family_count = 5;</code>
     *
     * <pre>
     ** total del family count 
     * </pre>
     */
    long getDelFamilyCount();
  }
  /**
   * Protobuf type {@code FamilyInfo}
   */
  public static final class FamilyInfo extends
      com.google.protobuf.GeneratedMessage
      implements FamilyInfoOrBuilder {
    // Use FamilyInfo.newBuilder() to construct.
    private FamilyInfo(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FamilyInfo(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FamilyInfo defaultInstance;
    public static FamilyInfo getDefaultInstance() {
      return defaultInstance;
    }

    public FamilyInfo getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FamilyInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              familyname_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              rowCount_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              kvCount_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              delKvCount_ = input.readUInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              delFamilyCount_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_FamilyInfo_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_FamilyInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder.class);
    }

    public static com.google.protobuf.Parser<FamilyInfo> PARSER =
        new com.google.protobuf.AbstractParser<FamilyInfo>() {
      public FamilyInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FamilyInfo(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FamilyInfo> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string familyname = 1;
    public static final int FAMILYNAME_FIELD_NUMBER = 1;
    private java.lang.Object familyname_;
    /**
     * <code>required string familyname = 1;</code>
     *
     * <pre>
     ** family name 
     * </pre>
     */
    public boolean hasFamilyname() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string familyname = 1;</code>
     *
     * <pre>
     ** family name 
     * </pre>
     */
    public java.lang.String getFamilyname() {
      java.lang.Object ref = familyname_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          familyname_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string familyname = 1;</code>
     *
     * <pre>
     ** family name 
     * </pre>
     */
    public com.google.protobuf.ByteString
        getFamilynameBytes() {
      java.lang.Object ref = familyname_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        familyname_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required uint64 row_count = 2;
    public static final int ROW_COUNT_FIELD_NUMBER = 2;
    private long rowCount_;
    /**
     * <code>required uint64 row_count = 2;</code>
     *
     * <pre>
     ** total row count 
     * </pre>
     */
    public boolean hasRowCount() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required uint64 row_count = 2;</code>
     *
     * <pre>
     ** total row count 
     * </pre>
     */
    public long getRowCount() {
      return rowCount_;
    }

    // required uint64 kv_count = 3;
    public static final int KV_COUNT_FIELD_NUMBER = 3;
    private long kvCount_;
    /**
     * <code>required uint64 kv_count = 3;</code>
     *
     * <pre>
     ** total kv count 
     * </pre>
     */
    public boolean hasKvCount() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required uint64 kv_count = 3;</code>
     *
     * <pre>
     ** total kv count 
     * </pre>
     */
    public long getKvCount() {
      return kvCount_;
    }

    // required uint64 del_kv_count = 4;
    public static final int DEL_KV_COUNT_FIELD_NUMBER = 4;
    private long delKvCount_;
    /**
     * <code>required uint64 del_kv_count = 4;</code>
     *
     * <pre>
     ** total delete kv count 
     * </pre>
     */
    public boolean hasDelKvCount() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required uint64 del_kv_count = 4;</code>
     *
     * <pre>
     ** total delete kv count 
     * </pre>
     */
    public long getDelKvCount() {
      return delKvCount_;
    }

    // required uint64 del_family_count = 5;
    public static final int DEL_FAMILY_COUNT_FIELD_NUMBER = 5;
    private long delFamilyCount_;
    /**
     * <code>required uint64 del_family_count = 5;</code>
     *
     * <pre>
     ** total del family count 
     * </pre>
     */
    public boolean hasDelFamilyCount() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>required uint64 del_family_count = 5;</code>
     *
     * <pre>
     ** total del family count 
     * </pre>
     */
    public long getDelFamilyCount() {
      return delFamilyCount_;
    }

    private void initFields() {
      familyname_ = "";
      rowCount_ = 0L;
      kvCount_ = 0L;
      delKvCount_ = 0L;
      delFamilyCount_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasFamilyname()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasRowCount()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasKvCount()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasDelKvCount()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasDelFamilyCount()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getFamilynameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, rowCount_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, kvCount_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, delKvCount_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeUInt64(5, delFamilyCount_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getFamilynameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, rowCount_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, kvCount_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, delKvCount_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(5, delFamilyCount_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo) obj;

      boolean result = true;
      result = result && (hasFamilyname() == other.hasFamilyname());
      if (hasFamilyname()) {
        result = result && getFamilyname()
            .equals(other.getFamilyname());
      }
      result = result && (hasRowCount() == other.hasRowCount());
      if (hasRowCount()) {
        result = result && (getRowCount()
            == other.getRowCount());
      }
      result = result && (hasKvCount() == other.hasKvCount());
      if (hasKvCount()) {
        result = result && (getKvCount()
            == other.getKvCount());
      }
      result = result && (hasDelKvCount() == other.hasDelKvCount());
      if (hasDelKvCount()) {
        result = result && (getDelKvCount()
            == other.getDelKvCount());
      }
      result = result && (hasDelFamilyCount() == other.hasDelFamilyCount());
      if (hasDelFamilyCount()) {
        result = result && (getDelFamilyCount()
            == other.getDelFamilyCount());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasFamilyname()) {
        hash = (37 * hash) + FAMILYNAME_FIELD_NUMBER;
        hash = (53 * hash) + getFamilyname().hashCode();
      }
      if (hasRowCount()) {
        hash = (37 * hash) + ROW_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getRowCount());
      }
      if (hasKvCount()) {
        hash = (37 * hash) + KV_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getKvCount());
      }
      if (hasDelKvCount()) {
        hash = (37 * hash) + DEL_KV_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDelKvCount());
      }
      if (hasDelFamilyCount()) {
        hash = (37 * hash) + DEL_FAMILY_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDelFamilyCount());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code FamilyInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_FamilyInfo_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_FamilyInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        familyname_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        rowCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        kvCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        delKvCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        delFamilyCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_FamilyInfo_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.familyname_ = familyname_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.rowCount_ = rowCount_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.kvCount_ = kvCount_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.delKvCount_ = delKvCount_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.delFamilyCount_ = delFamilyCount_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.getDefaultInstance()) return this;
        if (other.hasFamilyname()) {
          bitField0_ |= 0x00000001;
          familyname_ = other.familyname_;
          onChanged();
        }
        if (other.hasRowCount()) {
          setRowCount(other.getRowCount());
        }
        if (other.hasKvCount()) {
          setKvCount(other.getKvCount());
        }
        if (other.hasDelKvCount()) {
          setDelKvCount(other.getDelKvCount());
        }
        if (other.hasDelFamilyCount()) {
          setDelFamilyCount(other.getDelFamilyCount());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasFamilyname()) {
          
          return false;
        }
        if (!hasRowCount()) {
          
          return false;
        }
        if (!hasKvCount()) {
          
          return false;
        }
        if (!hasDelKvCount()) {
          
          return false;
        }
        if (!hasDelFamilyCount()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string familyname = 1;
      private java.lang.Object familyname_ = "";
      /**
       * <code>required string familyname = 1;</code>
       *
       * <pre>
       ** family name 
       * </pre>
       */
      public boolean hasFamilyname() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string familyname = 1;</code>
       *
       * <pre>
       ** family name 
       * </pre>
       */
      public java.lang.String getFamilyname() {
        java.lang.Object ref = familyname_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          familyname_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string familyname = 1;</code>
       *
       * <pre>
       ** family name 
       * </pre>
       */
      public com.google.protobuf.ByteString
          getFamilynameBytes() {
        java.lang.Object ref = familyname_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          familyname_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string familyname = 1;</code>
       *
       * <pre>
       ** family name 
       * </pre>
       */
      public Builder setFamilyname(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        familyname_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string familyname = 1;</code>
       *
       * <pre>
       ** family name 
       * </pre>
       */
      public Builder clearFamilyname() {
        bitField0_ = (bitField0_ & ~0x00000001);
        familyname_ = getDefaultInstance().getFamilyname();
        onChanged();
        return this;
      }
      /**
       * <code>required string familyname = 1;</code>
       *
       * <pre>
       ** family name 
       * </pre>
       */
      public Builder setFamilynameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        familyname_ = value;
        onChanged();
        return this;
      }

      // required uint64 row_count = 2;
      private long rowCount_ ;
      /**
       * <code>required uint64 row_count = 2;</code>
       *
       * <pre>
       ** total row count 
       * </pre>
       */
      public boolean hasRowCount() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint64 row_count = 2;</code>
       *
       * <pre>
       ** total row count 
       * </pre>
       */
      public long getRowCount() {
        return rowCount_;
      }
      /**
       * <code>required uint64 row_count = 2;</code>
       *
       * <pre>
       ** total row count 
       * </pre>
       */
      public Builder setRowCount(long value) {
        bitField0_ |= 0x00000002;
        rowCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 row_count = 2;</code>
       *
       * <pre>
       ** total row count 
       * </pre>
       */
      public Builder clearRowCount() {
        bitField0_ = (bitField0_ & ~0x00000002);
        rowCount_ = 0L;
        onChanged();
        return this;
      }

      // required uint64 kv_count = 3;
      private long kvCount_ ;
      /**
       * <code>required uint64 kv_count = 3;</code>
       *
       * <pre>
       ** total kv count 
       * </pre>
       */
      public boolean hasKvCount() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required uint64 kv_count = 3;</code>
       *
       * <pre>
       ** total kv count 
       * </pre>
       */
      public long getKvCount() {
        return kvCount_;
      }
      /**
       * <code>required uint64 kv_count = 3;</code>
       *
       * <pre>
       ** total kv count 
       * </pre>
       */
      public Builder setKvCount(long value) {
        bitField0_ |= 0x00000004;
        kvCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 kv_count = 3;</code>
       *
       * <pre>
       ** total kv count 
       * </pre>
       */
      public Builder clearKvCount() {
        bitField0_ = (bitField0_ & ~0x00000004);
        kvCount_ = 0L;
        onChanged();
        return this;
      }

      // required uint64 del_kv_count = 4;
      private long delKvCount_ ;
      /**
       * <code>required uint64 del_kv_count = 4;</code>
       *
       * <pre>
       ** total delete kv count 
       * </pre>
       */
      public boolean hasDelKvCount() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required uint64 del_kv_count = 4;</code>
       *
       * <pre>
       ** total delete kv count 
       * </pre>
       */
      public long getDelKvCount() {
        return delKvCount_;
      }
      /**
       * <code>required uint64 del_kv_count = 4;</code>
       *
       * <pre>
       ** total delete kv count 
       * </pre>
       */
      public Builder setDelKvCount(long value) {
        bitField0_ |= 0x00000008;
        delKvCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 del_kv_count = 4;</code>
       *
       * <pre>
       ** total delete kv count 
       * </pre>
       */
      public Builder clearDelKvCount() {
        bitField0_ = (bitField0_ & ~0x00000008);
        delKvCount_ = 0L;
        onChanged();
        return this;
      }

      // required uint64 del_family_count = 5;
      private long delFamilyCount_ ;
      /**
       * <code>required uint64 del_family_count = 5;</code>
       *
       * <pre>
       ** total del family count 
       * </pre>
       */
      public boolean hasDelFamilyCount() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>required uint64 del_family_count = 5;</code>
       *
       * <pre>
       ** total del family count 
       * </pre>
       */
      public long getDelFamilyCount() {
        return delFamilyCount_;
      }
      /**
       * <code>required uint64 del_family_count = 5;</code>
       *
       * <pre>
       ** total del family count 
       * </pre>
       */
      public Builder setDelFamilyCount(long value) {
        bitField0_ |= 0x00000010;
        delFamilyCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 del_family_count = 5;</code>
       *
       * <pre>
       ** total del family count 
       * </pre>
       */
      public Builder clearDelFamilyCount() {
        bitField0_ = (bitField0_ & ~0x00000010);
        delFamilyCount_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:FamilyInfo)
    }

    static {
      defaultInstance = new FamilyInfo(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:FamilyInfo)
  }

  public interface RegionLoadOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .RegionSpecifier region_specifier = 1;
    /**
     * <code>required .RegionSpecifier region_specifier = 1;</code>
     *
     * <pre>
     ** the region specifier 
     * </pre>
     */
    boolean hasRegionSpecifier();
    /**
     * <code>required .RegionSpecifier region_specifier = 1;</code>
     *
     * <pre>
     ** the region specifier 
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegionSpecifier();
    /**
     * <code>required .RegionSpecifier region_specifier = 1;</code>
     *
     * <pre>
     ** the region specifier 
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionSpecifierOrBuilder();

    // optional uint32 stores = 2;
    /**
     * <code>optional uint32 stores = 2;</code>
     *
     * <pre>
     ** the number of stores for the region 
     * </pre>
     */
    boolean hasStores();
    /**
     * <code>optional uint32 stores = 2;</code>
     *
     * <pre>
     ** the number of stores for the region 
     * </pre>
     */
    int getStores();

    // optional uint32 storefiles = 3;
    /**
     * <code>optional uint32 storefiles = 3;</code>
     *
     * <pre>
     ** the number of storefiles for the region 
     * </pre>
     */
    boolean hasStorefiles();
    /**
     * <code>optional uint32 storefiles = 3;</code>
     *
     * <pre>
     ** the number of storefiles for the region 
     * </pre>
     */
    int getStorefiles();

    // optional uint32 store_uncompressed_size_MB = 4;
    /**
     * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
     *
     * <pre>
     ** the total size of the store files for the region, uncompressed, in MB 
     * </pre>
     */
    boolean hasStoreUncompressedSizeMB();
    /**
     * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
     *
     * <pre>
     ** the total size of the store files for the region, uncompressed, in MB 
     * </pre>
     */
    int getStoreUncompressedSizeMB();

    // optional uint32 storefile_size_MB = 5;
    /**
     * <code>optional uint32 storefile_size_MB = 5;</code>
     *
     * <pre>
     ** the current total size of the store files for the region, in MB 
     * </pre>
     */
    boolean hasStorefileSizeMB();
    /**
     * <code>optional uint32 storefile_size_MB = 5;</code>
     *
     * <pre>
     ** the current total size of the store files for the region, in MB 
     * </pre>
     */
    int getStorefileSizeMB();

    // optional uint32 memstore_size_MB = 6;
    /**
     * <code>optional uint32 memstore_size_MB = 6;</code>
     *
     * <pre>
     ** the current size of the memstore for the region, in MB 
     * </pre>
     */
    boolean hasMemstoreSizeMB();
    /**
     * <code>optional uint32 memstore_size_MB = 6;</code>
     *
     * <pre>
     ** the current size of the memstore for the region, in MB 
     * </pre>
     */
    int getMemstoreSizeMB();

    // optional uint32 storefile_index_size_MB = 7;
    /**
     * <code>optional uint32 storefile_index_size_MB = 7;</code>
     *
     * <pre>
     **
     * The current total size of root-level store file indexes for the region,
     * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
     * </pre>
     */
    boolean hasStorefileIndexSizeMB();
    /**
     * <code>optional uint32 storefile_index_size_MB = 7;</code>
     *
     * <pre>
     **
     * The current total size of root-level store file indexes for the region,
     * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
     * </pre>
     */
    int getStorefileIndexSizeMB();

    // optional uint64 read_requests_count = 8;
    /**
     * <code>optional uint64 read_requests_count = 8;</code>
     *
     * <pre>
     ** the current total read requests made to region 
     * </pre>
     */
    boolean hasReadRequestsCount();
    /**
     * <code>optional uint64 read_requests_count = 8;</code>
     *
     * <pre>
     ** the current total read requests made to region 
     * </pre>
     */
    long getReadRequestsCount();

    // optional uint64 write_requests_count = 9;
    /**
     * <code>optional uint64 write_requests_count = 9;</code>
     *
     * <pre>
     ** the current total write requests made to region 
     * </pre>
     */
    boolean hasWriteRequestsCount();
    /**
     * <code>optional uint64 write_requests_count = 9;</code>
     *
     * <pre>
     ** the current total write requests made to region 
     * </pre>
     */
    long getWriteRequestsCount();

    // optional uint64 total_compacting_KVs = 10;
    /**
     * <code>optional uint64 total_compacting_KVs = 10;</code>
     *
     * <pre>
     ** the total compacting key values in currently running compaction 
     * </pre>
     */
    boolean hasTotalCompactingKVs();
    /**
     * <code>optional uint64 total_compacting_KVs = 10;</code>
     *
     * <pre>
     ** the total compacting key values in currently running compaction 
     * </pre>
     */
    long getTotalCompactingKVs();

    // optional uint64 current_compacted_KVs = 11;
    /**
     * <code>optional uint64 current_compacted_KVs = 11;</code>
     *
     * <pre>
     ** the completed count of key values in currently running compaction 
     * </pre>
     */
    boolean hasCurrentCompactedKVs();
    /**
     * <code>optional uint64 current_compacted_KVs = 11;</code>
     *
     * <pre>
     ** the completed count of key values in currently running compaction 
     * </pre>
     */
    long getCurrentCompactedKVs();

    // optional uint32 root_index_size_KB = 12;
    /**
     * <code>optional uint32 root_index_size_KB = 12;</code>
     *
     * <pre>
     ** The current total size of root-level indexes for the region, in KB. 
     * </pre>
     */
    boolean hasRootIndexSizeKB();
    /**
     * <code>optional uint32 root_index_size_KB = 12;</code>
     *
     * <pre>
     ** The current total size of root-level indexes for the region, in KB. 
     * </pre>
     */
    int getRootIndexSizeKB();

    // optional uint32 total_static_index_size_KB = 13;
    /**
     * <code>optional uint32 total_static_index_size_KB = 13;</code>
     *
     * <pre>
     ** The total size of all index blocks, not just the root level, in KB. 
     * </pre>
     */
    boolean hasTotalStaticIndexSizeKB();
    /**
     * <code>optional uint32 total_static_index_size_KB = 13;</code>
     *
     * <pre>
     ** The total size of all index blocks, not just the root level, in KB. 
     * </pre>
     */
    int getTotalStaticIndexSizeKB();

    // optional uint32 total_static_bloom_size_KB = 14;
    /**
     * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
     *
     * <pre>
     **
     * The total size of all Bloom filter blocks, not just loaded into the
     * block cache, in KB.
     * </pre>
     */
    boolean hasTotalStaticBloomSizeKB();
    /**
     * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
     *
     * <pre>
     **
     * The total size of all Bloom filter blocks, not just loaded into the
     * block cache, in KB.
     * </pre>
     */
    int getTotalStaticBloomSizeKB();

    // optional uint64 complete_sequence_id = 15;
    /**
     * <code>optional uint64 complete_sequence_id = 15;</code>
     *
     * <pre>
     ** the most recent sequence Id from cache flush 
     * </pre>
     */
    boolean hasCompleteSequenceId();
    /**
     * <code>optional uint64 complete_sequence_id = 15;</code>
     *
     * <pre>
     ** the most recent sequence Id from cache flush 
     * </pre>
     */
    long getCompleteSequenceId();

    // optional float data_locality = 16;
    /**
     * <code>optional float data_locality = 16;</code>
     *
     * <pre>
     ** The current data locality for region in the regionserver 
     * </pre>
     */
    boolean hasDataLocality();
    /**
     * <code>optional float data_locality = 16;</code>
     *
     * <pre>
     ** The current data locality for region in the regionserver 
     * </pre>
     */
    float getDataLocality();

    // optional uint64 read_requests_per_second = 17;
    /**
     * <code>optional uint64 read_requests_per_second = 17;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    boolean hasReadRequestsPerSecond();
    /**
     * <code>optional uint64 read_requests_per_second = 17;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    long getReadRequestsPerSecond();

    // optional uint64 write_requests_per_second = 18;
    /**
     * <code>optional uint64 write_requests_per_second = 18;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    boolean hasWriteRequestsPerSecond();
    /**
     * <code>optional uint64 write_requests_per_second = 18;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    long getWriteRequestsPerSecond();

    // optional uint64 read_requests_by_capacity_unit_per_second = 19;
    /**
     * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region 
     * </pre>
     */
    boolean hasReadRequestsByCapacityUnitPerSecond();
    /**
     * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region 
     * </pre>
     */
    long getReadRequestsByCapacityUnitPerSecond();

    // optional uint64 write_requests_by_capacity_unit_per_second = 20;
    /**
     * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region 
     * </pre>
     */
    boolean hasWriteRequestsByCapacityUnitPerSecond();
    /**
     * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region 
     * </pre>
     */
    long getWriteRequestsByCapacityUnitPerSecond();

    // optional uint64 throttled_read_requests_count = 21;
    /**
     * <code>optional uint64 throttled_read_requests_count = 21;</code>
     *
     * <pre>
     ** throttled read requests made to region 
     * </pre>
     */
    boolean hasThrottledReadRequestsCount();
    /**
     * <code>optional uint64 throttled_read_requests_count = 21;</code>
     *
     * <pre>
     ** throttled read requests made to region 
     * </pre>
     */
    long getThrottledReadRequestsCount();

    // optional uint64 throttled_write_requests_count = 22;
    /**
     * <code>optional uint64 throttled_write_requests_count = 22;</code>
     *
     * <pre>
     ** throttled write requests made to region 
     * </pre>
     */
    boolean hasThrottledWriteRequestsCount();
    /**
     * <code>optional uint64 throttled_write_requests_count = 22;</code>
     *
     * <pre>
     ** throttled write requests made to region 
     * </pre>
     */
    long getThrottledWriteRequestsCount();

    // optional uint64 get_requests_count = 23;
    /**
     * <code>optional uint64 get_requests_count = 23;</code>
     *
     * <pre>
     ** the current total get requests made to region 
     * </pre>
     */
    boolean hasGetRequestsCount();
    /**
     * <code>optional uint64 get_requests_count = 23;</code>
     *
     * <pre>
     ** the current total get requests made to region 
     * </pre>
     */
    long getGetRequestsCount();

    // optional uint64 read_cell_count_per_second = 24;
    /**
     * <code>optional uint64 read_cell_count_per_second = 24;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    boolean hasReadCellCountPerSecond();
    /**
     * <code>optional uint64 read_cell_count_per_second = 24;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    long getReadCellCountPerSecond();

    // optional uint64 read_raw_cell_count_per_second = 25;
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    boolean hasReadRawCellCountPerSecond();
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    long getReadRawCellCountPerSecond();

    // optional uint64 scan_count_per_second = 26;
    /**
     * <code>optional uint64 scan_count_per_second = 26;</code>
     *
     * <pre>
     ** scan count per second made to region 
     * </pre>
     */
    boolean hasScanCountPerSecond();
    /**
     * <code>optional uint64 scan_count_per_second = 26;</code>
     *
     * <pre>
     ** scan count per second made to region 
     * </pre>
     */
    long getScanCountPerSecond();

    // optional uint64 scan_rows_per_second = 27;
    /**
     * <code>optional uint64 scan_rows_per_second = 27;</code>
     *
     * <pre>
     ** scan rows per second made to region 
     * </pre>
     */
    boolean hasScanRowsPerSecond();
    /**
     * <code>optional uint64 scan_rows_per_second = 27;</code>
     *
     * <pre>
     ** scan rows per second made to region 
     * </pre>
     */
    long getScanRowsPerSecond();

    // repeated .FamilyInfo family_info = 28;
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo> 
        getFamilyInfoList();
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo getFamilyInfo(int index);
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    int getFamilyInfoCount();
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder> 
        getFamilyInfoOrBuilderList();
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder getFamilyInfoOrBuilder(
        int index);

    // optional uint64 user_read_requests_per_second = 29;
    /**
     * <code>optional uint64 user_read_requests_per_second = 29;</code>
     *
     * <pre>
     ** read requests per second made to region by user 
     * </pre>
     */
    boolean hasUserReadRequestsPerSecond();
    /**
     * <code>optional uint64 user_read_requests_per_second = 29;</code>
     *
     * <pre>
     ** read requests per second made to region by user 
     * </pre>
     */
    long getUserReadRequestsPerSecond();

    // optional uint64 user_write_requests_per_second = 30;
    /**
     * <code>optional uint64 user_write_requests_per_second = 30;</code>
     *
     * <pre>
     ** write requests per second made to region by user 
     * </pre>
     */
    boolean hasUserWriteRequestsPerSecond();
    /**
     * <code>optional uint64 user_write_requests_per_second = 30;</code>
     *
     * <pre>
     ** write requests per second made to region by user 
     * </pre>
     */
    long getUserWriteRequestsPerSecond();

    // optional uint64 user_read_requests_by_capacity_unit_per_second = 31;
    /**
     * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region by user 
     * </pre>
     */
    boolean hasUserReadRequestsByCapacityUnitPerSecond();
    /**
     * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region by user 
     * </pre>
     */
    long getUserReadRequestsByCapacityUnitPerSecond();

    // optional uint64 user_write_requests_by_capacity_unit_per_second = 32;
    /**
     * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region by user 
     * </pre>
     */
    boolean hasUserWriteRequestsByCapacityUnitPerSecond();
    /**
     * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region by user 
     * </pre>
     */
    long getUserWriteRequestsByCapacityUnitPerSecond();
  }
  /**
   * Protobuf type {@code RegionLoad}
   */
  public static final class RegionLoad extends
      com.google.protobuf.GeneratedMessage
      implements RegionLoadOrBuilder {
    // Use RegionLoad.newBuilder() to construct.
    private RegionLoad(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegionLoad(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegionLoad defaultInstance;
    public static RegionLoad getDefaultInstance() {
      return defaultInstance;
    }

    public RegionLoad getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegionLoad(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = regionSpecifier_.toBuilder();
              }
              regionSpecifier_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(regionSpecifier_);
                regionSpecifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              stores_ = input.readUInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              storefiles_ = input.readUInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              storeUncompressedSizeMB_ = input.readUInt32();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              storefileSizeMB_ = input.readUInt32();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              memstoreSizeMB_ = input.readUInt32();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              storefileIndexSizeMB_ = input.readUInt32();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000080;
              readRequestsCount_ = input.readUInt64();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000100;
              writeRequestsCount_ = input.readUInt64();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000200;
              totalCompactingKVs_ = input.readUInt64();
              break;
            }
            case 88: {
              bitField0_ |= 0x00000400;
              currentCompactedKVs_ = input.readUInt64();
              break;
            }
            case 96: {
              bitField0_ |= 0x00000800;
              rootIndexSizeKB_ = input.readUInt32();
              break;
            }
            case 104: {
              bitField0_ |= 0x00001000;
              totalStaticIndexSizeKB_ = input.readUInt32();
              break;
            }
            case 112: {
              bitField0_ |= 0x00002000;
              totalStaticBloomSizeKB_ = input.readUInt32();
              break;
            }
            case 120: {
              bitField0_ |= 0x00004000;
              completeSequenceId_ = input.readUInt64();
              break;
            }
            case 133: {
              bitField0_ |= 0x00008000;
              dataLocality_ = input.readFloat();
              break;
            }
            case 136: {
              bitField0_ |= 0x00010000;
              readRequestsPerSecond_ = input.readUInt64();
              break;
            }
            case 144: {
              bitField0_ |= 0x00020000;
              writeRequestsPerSecond_ = input.readUInt64();
              break;
            }
            case 152: {
              bitField0_ |= 0x00040000;
              readRequestsByCapacityUnitPerSecond_ = input.readUInt64();
              break;
            }
            case 160: {
              bitField0_ |= 0x00080000;
              writeRequestsByCapacityUnitPerSecond_ = input.readUInt64();
              break;
            }
            case 168: {
              bitField0_ |= 0x00100000;
              throttledReadRequestsCount_ = input.readUInt64();
              break;
            }
            case 176: {
              bitField0_ |= 0x00200000;
              throttledWriteRequestsCount_ = input.readUInt64();
              break;
            }
            case 184: {
              bitField0_ |= 0x00400000;
              getRequestsCount_ = input.readUInt64();
              break;
            }
            case 192: {
              bitField0_ |= 0x00800000;
              readCellCountPerSecond_ = input.readUInt64();
              break;
            }
            case 200: {
              bitField0_ |= 0x01000000;
              readRawCellCountPerSecond_ = input.readUInt64();
              break;
            }
            case 208: {
              bitField0_ |= 0x02000000;
              scanCountPerSecond_ = input.readUInt64();
              break;
            }
            case 216: {
              bitField0_ |= 0x04000000;
              scanRowsPerSecond_ = input.readUInt64();
              break;
            }
            case 226: {
              if (!((mutable_bitField0_ & 0x08000000) == 0x08000000)) {
                familyInfo_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo>();
                mutable_bitField0_ |= 0x08000000;
              }
              familyInfo_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.PARSER, extensionRegistry));
              break;
            }
            case 232: {
              bitField0_ |= 0x08000000;
              userReadRequestsPerSecond_ = input.readUInt64();
              break;
            }
            case 240: {
              bitField0_ |= 0x10000000;
              userWriteRequestsPerSecond_ = input.readUInt64();
              break;
            }
            case 248: {
              bitField0_ |= 0x20000000;
              userReadRequestsByCapacityUnitPerSecond_ = input.readUInt64();
              break;
            }
            case 256: {
              bitField0_ |= 0x40000000;
              userWriteRequestsByCapacityUnitPerSecond_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x08000000) == 0x08000000)) {
          familyInfo_ = java.util.Collections.unmodifiableList(familyInfo_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionLoad_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionLoad_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder.class);
    }

    public static com.google.protobuf.Parser<RegionLoad> PARSER =
        new com.google.protobuf.AbstractParser<RegionLoad>() {
      public RegionLoad parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegionLoad(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegionLoad> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .RegionSpecifier region_specifier = 1;
    public static final int REGION_SPECIFIER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier regionSpecifier_;
    /**
     * <code>required .RegionSpecifier region_specifier = 1;</code>
     *
     * <pre>
     ** the region specifier 
     * </pre>
     */
    public boolean hasRegionSpecifier() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .RegionSpecifier region_specifier = 1;</code>
     *
     * <pre>
     ** the region specifier 
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegionSpecifier() {
      return regionSpecifier_;
    }
    /**
     * <code>required .RegionSpecifier region_specifier = 1;</code>
     *
     * <pre>
     ** the region specifier 
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionSpecifierOrBuilder() {
      return regionSpecifier_;
    }

    // optional uint32 stores = 2;
    public static final int STORES_FIELD_NUMBER = 2;
    private int stores_;
    /**
     * <code>optional uint32 stores = 2;</code>
     *
     * <pre>
     ** the number of stores for the region 
     * </pre>
     */
    public boolean hasStores() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint32 stores = 2;</code>
     *
     * <pre>
     ** the number of stores for the region 
     * </pre>
     */
    public int getStores() {
      return stores_;
    }

    // optional uint32 storefiles = 3;
    public static final int STOREFILES_FIELD_NUMBER = 3;
    private int storefiles_;
    /**
     * <code>optional uint32 storefiles = 3;</code>
     *
     * <pre>
     ** the number of storefiles for the region 
     * </pre>
     */
    public boolean hasStorefiles() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint32 storefiles = 3;</code>
     *
     * <pre>
     ** the number of storefiles for the region 
     * </pre>
     */
    public int getStorefiles() {
      return storefiles_;
    }

    // optional uint32 store_uncompressed_size_MB = 4;
    public static final int STORE_UNCOMPRESSED_SIZE_MB_FIELD_NUMBER = 4;
    private int storeUncompressedSizeMB_;
    /**
     * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
     *
     * <pre>
     ** the total size of the store files for the region, uncompressed, in MB 
     * </pre>
     */
    public boolean hasStoreUncompressedSizeMB() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
     *
     * <pre>
     ** the total size of the store files for the region, uncompressed, in MB 
     * </pre>
     */
    public int getStoreUncompressedSizeMB() {
      return storeUncompressedSizeMB_;
    }

    // optional uint32 storefile_size_MB = 5;
    public static final int STOREFILE_SIZE_MB_FIELD_NUMBER = 5;
    private int storefileSizeMB_;
    /**
     * <code>optional uint32 storefile_size_MB = 5;</code>
     *
     * <pre>
     ** the current total size of the store files for the region, in MB 
     * </pre>
     */
    public boolean hasStorefileSizeMB() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional uint32 storefile_size_MB = 5;</code>
     *
     * <pre>
     ** the current total size of the store files for the region, in MB 
     * </pre>
     */
    public int getStorefileSizeMB() {
      return storefileSizeMB_;
    }

    // optional uint32 memstore_size_MB = 6;
    public static final int MEMSTORE_SIZE_MB_FIELD_NUMBER = 6;
    private int memstoreSizeMB_;
    /**
     * <code>optional uint32 memstore_size_MB = 6;</code>
     *
     * <pre>
     ** the current size of the memstore for the region, in MB 
     * </pre>
     */
    public boolean hasMemstoreSizeMB() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional uint32 memstore_size_MB = 6;</code>
     *
     * <pre>
     ** the current size of the memstore for the region, in MB 
     * </pre>
     */
    public int getMemstoreSizeMB() {
      return memstoreSizeMB_;
    }

    // optional uint32 storefile_index_size_MB = 7;
    public static final int STOREFILE_INDEX_SIZE_MB_FIELD_NUMBER = 7;
    private int storefileIndexSizeMB_;
    /**
     * <code>optional uint32 storefile_index_size_MB = 7;</code>
     *
     * <pre>
     **
     * The current total size of root-level store file indexes for the region,
     * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
     * </pre>
     */
    public boolean hasStorefileIndexSizeMB() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional uint32 storefile_index_size_MB = 7;</code>
     *
     * <pre>
     **
     * The current total size of root-level store file indexes for the region,
     * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
     * </pre>
     */
    public int getStorefileIndexSizeMB() {
      return storefileIndexSizeMB_;
    }

    // optional uint64 read_requests_count = 8;
    public static final int READ_REQUESTS_COUNT_FIELD_NUMBER = 8;
    private long readRequestsCount_;
    /**
     * <code>optional uint64 read_requests_count = 8;</code>
     *
     * <pre>
     ** the current total read requests made to region 
     * </pre>
     */
    public boolean hasReadRequestsCount() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional uint64 read_requests_count = 8;</code>
     *
     * <pre>
     ** the current total read requests made to region 
     * </pre>
     */
    public long getReadRequestsCount() {
      return readRequestsCount_;
    }

    // optional uint64 write_requests_count = 9;
    public static final int WRITE_REQUESTS_COUNT_FIELD_NUMBER = 9;
    private long writeRequestsCount_;
    /**
     * <code>optional uint64 write_requests_count = 9;</code>
     *
     * <pre>
     ** the current total write requests made to region 
     * </pre>
     */
    public boolean hasWriteRequestsCount() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional uint64 write_requests_count = 9;</code>
     *
     * <pre>
     ** the current total write requests made to region 
     * </pre>
     */
    public long getWriteRequestsCount() {
      return writeRequestsCount_;
    }

    // optional uint64 total_compacting_KVs = 10;
    public static final int TOTAL_COMPACTING_KVS_FIELD_NUMBER = 10;
    private long totalCompactingKVs_;
    /**
     * <code>optional uint64 total_compacting_KVs = 10;</code>
     *
     * <pre>
     ** the total compacting key values in currently running compaction 
     * </pre>
     */
    public boolean hasTotalCompactingKVs() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    /**
     * <code>optional uint64 total_compacting_KVs = 10;</code>
     *
     * <pre>
     ** the total compacting key values in currently running compaction 
     * </pre>
     */
    public long getTotalCompactingKVs() {
      return totalCompactingKVs_;
    }

    // optional uint64 current_compacted_KVs = 11;
    public static final int CURRENT_COMPACTED_KVS_FIELD_NUMBER = 11;
    private long currentCompactedKVs_;
    /**
     * <code>optional uint64 current_compacted_KVs = 11;</code>
     *
     * <pre>
     ** the completed count of key values in currently running compaction 
     * </pre>
     */
    public boolean hasCurrentCompactedKVs() {
      return ((bitField0_ & 0x00000400) == 0x00000400);
    }
    /**
     * <code>optional uint64 current_compacted_KVs = 11;</code>
     *
     * <pre>
     ** the completed count of key values in currently running compaction 
     * </pre>
     */
    public long getCurrentCompactedKVs() {
      return currentCompactedKVs_;
    }

    // optional uint32 root_index_size_KB = 12;
    public static final int ROOT_INDEX_SIZE_KB_FIELD_NUMBER = 12;
    private int rootIndexSizeKB_;
    /**
     * <code>optional uint32 root_index_size_KB = 12;</code>
     *
     * <pre>
     ** The current total size of root-level indexes for the region, in KB. 
     * </pre>
     */
    public boolean hasRootIndexSizeKB() {
      return ((bitField0_ & 0x00000800) == 0x00000800);
    }
    /**
     * <code>optional uint32 root_index_size_KB = 12;</code>
     *
     * <pre>
     ** The current total size of root-level indexes for the region, in KB. 
     * </pre>
     */
    public int getRootIndexSizeKB() {
      return rootIndexSizeKB_;
    }

    // optional uint32 total_static_index_size_KB = 13;
    public static final int TOTAL_STATIC_INDEX_SIZE_KB_FIELD_NUMBER = 13;
    private int totalStaticIndexSizeKB_;
    /**
     * <code>optional uint32 total_static_index_size_KB = 13;</code>
     *
     * <pre>
     ** The total size of all index blocks, not just the root level, in KB. 
     * </pre>
     */
    public boolean hasTotalStaticIndexSizeKB() {
      return ((bitField0_ & 0x00001000) == 0x00001000);
    }
    /**
     * <code>optional uint32 total_static_index_size_KB = 13;</code>
     *
     * <pre>
     ** The total size of all index blocks, not just the root level, in KB. 
     * </pre>
     */
    public int getTotalStaticIndexSizeKB() {
      return totalStaticIndexSizeKB_;
    }

    // optional uint32 total_static_bloom_size_KB = 14;
    public static final int TOTAL_STATIC_BLOOM_SIZE_KB_FIELD_NUMBER = 14;
    private int totalStaticBloomSizeKB_;
    /**
     * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
     *
     * <pre>
     **
     * The total size of all Bloom filter blocks, not just loaded into the
     * block cache, in KB.
     * </pre>
     */
    public boolean hasTotalStaticBloomSizeKB() {
      return ((bitField0_ & 0x00002000) == 0x00002000);
    }
    /**
     * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
     *
     * <pre>
     **
     * The total size of all Bloom filter blocks, not just loaded into the
     * block cache, in KB.
     * </pre>
     */
    public int getTotalStaticBloomSizeKB() {
      return totalStaticBloomSizeKB_;
    }

    // optional uint64 complete_sequence_id = 15;
    public static final int COMPLETE_SEQUENCE_ID_FIELD_NUMBER = 15;
    private long completeSequenceId_;
    /**
     * <code>optional uint64 complete_sequence_id = 15;</code>
     *
     * <pre>
     ** the most recent sequence Id from cache flush 
     * </pre>
     */
    public boolean hasCompleteSequenceId() {
      return ((bitField0_ & 0x00004000) == 0x00004000);
    }
    /**
     * <code>optional uint64 complete_sequence_id = 15;</code>
     *
     * <pre>
     ** the most recent sequence Id from cache flush 
     * </pre>
     */
    public long getCompleteSequenceId() {
      return completeSequenceId_;
    }

    // optional float data_locality = 16;
    public static final int DATA_LOCALITY_FIELD_NUMBER = 16;
    private float dataLocality_;
    /**
     * <code>optional float data_locality = 16;</code>
     *
     * <pre>
     ** The current data locality for region in the regionserver 
     * </pre>
     */
    public boolean hasDataLocality() {
      return ((bitField0_ & 0x00008000) == 0x00008000);
    }
    /**
     * <code>optional float data_locality = 16;</code>
     *
     * <pre>
     ** The current data locality for region in the regionserver 
     * </pre>
     */
    public float getDataLocality() {
      return dataLocality_;
    }

    // optional uint64 read_requests_per_second = 17;
    public static final int READ_REQUESTS_PER_SECOND_FIELD_NUMBER = 17;
    private long readRequestsPerSecond_;
    /**
     * <code>optional uint64 read_requests_per_second = 17;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    public boolean hasReadRequestsPerSecond() {
      return ((bitField0_ & 0x00010000) == 0x00010000);
    }
    /**
     * <code>optional uint64 read_requests_per_second = 17;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    public long getReadRequestsPerSecond() {
      return readRequestsPerSecond_;
    }

    // optional uint64 write_requests_per_second = 18;
    public static final int WRITE_REQUESTS_PER_SECOND_FIELD_NUMBER = 18;
    private long writeRequestsPerSecond_;
    /**
     * <code>optional uint64 write_requests_per_second = 18;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    public boolean hasWriteRequestsPerSecond() {
      return ((bitField0_ & 0x00020000) == 0x00020000);
    }
    /**
     * <code>optional uint64 write_requests_per_second = 18;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    public long getWriteRequestsPerSecond() {
      return writeRequestsPerSecond_;
    }

    // optional uint64 read_requests_by_capacity_unit_per_second = 19;
    public static final int READ_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER = 19;
    private long readRequestsByCapacityUnitPerSecond_;
    /**
     * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region 
     * </pre>
     */
    public boolean hasReadRequestsByCapacityUnitPerSecond() {
      return ((bitField0_ & 0x00040000) == 0x00040000);
    }
    /**
     * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region 
     * </pre>
     */
    public long getReadRequestsByCapacityUnitPerSecond() {
      return readRequestsByCapacityUnitPerSecond_;
    }

    // optional uint64 write_requests_by_capacity_unit_per_second = 20;
    public static final int WRITE_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER = 20;
    private long writeRequestsByCapacityUnitPerSecond_;
    /**
     * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region 
     * </pre>
     */
    public boolean hasWriteRequestsByCapacityUnitPerSecond() {
      return ((bitField0_ & 0x00080000) == 0x00080000);
    }
    /**
     * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region 
     * </pre>
     */
    public long getWriteRequestsByCapacityUnitPerSecond() {
      return writeRequestsByCapacityUnitPerSecond_;
    }

    // optional uint64 throttled_read_requests_count = 21;
    public static final int THROTTLED_READ_REQUESTS_COUNT_FIELD_NUMBER = 21;
    private long throttledReadRequestsCount_;
    /**
     * <code>optional uint64 throttled_read_requests_count = 21;</code>
     *
     * <pre>
     ** throttled read requests made to region 
     * </pre>
     */
    public boolean hasThrottledReadRequestsCount() {
      return ((bitField0_ & 0x00100000) == 0x00100000);
    }
    /**
     * <code>optional uint64 throttled_read_requests_count = 21;</code>
     *
     * <pre>
     ** throttled read requests made to region 
     * </pre>
     */
    public long getThrottledReadRequestsCount() {
      return throttledReadRequestsCount_;
    }

    // optional uint64 throttled_write_requests_count = 22;
    public static final int THROTTLED_WRITE_REQUESTS_COUNT_FIELD_NUMBER = 22;
    private long throttledWriteRequestsCount_;
    /**
     * <code>optional uint64 throttled_write_requests_count = 22;</code>
     *
     * <pre>
     ** throttled write requests made to region 
     * </pre>
     */
    public boolean hasThrottledWriteRequestsCount() {
      return ((bitField0_ & 0x00200000) == 0x00200000);
    }
    /**
     * <code>optional uint64 throttled_write_requests_count = 22;</code>
     *
     * <pre>
     ** throttled write requests made to region 
     * </pre>
     */
    public long getThrottledWriteRequestsCount() {
      return throttledWriteRequestsCount_;
    }

    // optional uint64 get_requests_count = 23;
    public static final int GET_REQUESTS_COUNT_FIELD_NUMBER = 23;
    private long getRequestsCount_;
    /**
     * <code>optional uint64 get_requests_count = 23;</code>
     *
     * <pre>
     ** the current total get requests made to region 
     * </pre>
     */
    public boolean hasGetRequestsCount() {
      return ((bitField0_ & 0x00400000) == 0x00400000);
    }
    /**
     * <code>optional uint64 get_requests_count = 23;</code>
     *
     * <pre>
     ** the current total get requests made to region 
     * </pre>
     */
    public long getGetRequestsCount() {
      return getRequestsCount_;
    }

    // optional uint64 read_cell_count_per_second = 24;
    public static final int READ_CELL_COUNT_PER_SECOND_FIELD_NUMBER = 24;
    private long readCellCountPerSecond_;
    /**
     * <code>optional uint64 read_cell_count_per_second = 24;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    public boolean hasReadCellCountPerSecond() {
      return ((bitField0_ & 0x00800000) == 0x00800000);
    }
    /**
     * <code>optional uint64 read_cell_count_per_second = 24;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    public long getReadCellCountPerSecond() {
      return readCellCountPerSecond_;
    }

    // optional uint64 read_raw_cell_count_per_second = 25;
    public static final int READ_RAW_CELL_COUNT_PER_SECOND_FIELD_NUMBER = 25;
    private long readRawCellCountPerSecond_;
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    public boolean hasReadRawCellCountPerSecond() {
      return ((bitField0_ & 0x01000000) == 0x01000000);
    }
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    public long getReadRawCellCountPerSecond() {
      return readRawCellCountPerSecond_;
    }

    // optional uint64 scan_count_per_second = 26;
    public static final int SCAN_COUNT_PER_SECOND_FIELD_NUMBER = 26;
    private long scanCountPerSecond_;
    /**
     * <code>optional uint64 scan_count_per_second = 26;</code>
     *
     * <pre>
     ** scan count per second made to region 
     * </pre>
     */
    public boolean hasScanCountPerSecond() {
      return ((bitField0_ & 0x02000000) == 0x02000000);
    }
    /**
     * <code>optional uint64 scan_count_per_second = 26;</code>
     *
     * <pre>
     ** scan count per second made to region 
     * </pre>
     */
    public long getScanCountPerSecond() {
      return scanCountPerSecond_;
    }

    // optional uint64 scan_rows_per_second = 27;
    public static final int SCAN_ROWS_PER_SECOND_FIELD_NUMBER = 27;
    private long scanRowsPerSecond_;
    /**
     * <code>optional uint64 scan_rows_per_second = 27;</code>
     *
     * <pre>
     ** scan rows per second made to region 
     * </pre>
     */
    public boolean hasScanRowsPerSecond() {
      return ((bitField0_ & 0x04000000) == 0x04000000);
    }
    /**
     * <code>optional uint64 scan_rows_per_second = 27;</code>
     *
     * <pre>
     ** scan rows per second made to region 
     * </pre>
     */
    public long getScanRowsPerSecond() {
      return scanRowsPerSecond_;
    }

    // repeated .FamilyInfo family_info = 28;
    public static final int FAMILY_INFO_FIELD_NUMBER = 28;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo> familyInfo_;
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo> getFamilyInfoList() {
      return familyInfo_;
    }
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder> 
        getFamilyInfoOrBuilderList() {
      return familyInfo_;
    }
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    public int getFamilyInfoCount() {
      return familyInfo_.size();
    }
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo getFamilyInfo(int index) {
      return familyInfo_.get(index);
    }
    /**
     * <code>repeated .FamilyInfo family_info = 28;</code>
     *
     * <pre>
     ** family info 
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder getFamilyInfoOrBuilder(
        int index) {
      return familyInfo_.get(index);
    }

    // optional uint64 user_read_requests_per_second = 29;
    public static final int USER_READ_REQUESTS_PER_SECOND_FIELD_NUMBER = 29;
    private long userReadRequestsPerSecond_;
    /**
     * <code>optional uint64 user_read_requests_per_second = 29;</code>
     *
     * <pre>
     ** read requests per second made to region by user 
     * </pre>
     */
    public boolean hasUserReadRequestsPerSecond() {
      return ((bitField0_ & 0x08000000) == 0x08000000);
    }
    /**
     * <code>optional uint64 user_read_requests_per_second = 29;</code>
     *
     * <pre>
     ** read requests per second made to region by user 
     * </pre>
     */
    public long getUserReadRequestsPerSecond() {
      return userReadRequestsPerSecond_;
    }

    // optional uint64 user_write_requests_per_second = 30;
    public static final int USER_WRITE_REQUESTS_PER_SECOND_FIELD_NUMBER = 30;
    private long userWriteRequestsPerSecond_;
    /**
     * <code>optional uint64 user_write_requests_per_second = 30;</code>
     *
     * <pre>
     ** write requests per second made to region by user 
     * </pre>
     */
    public boolean hasUserWriteRequestsPerSecond() {
      return ((bitField0_ & 0x10000000) == 0x10000000);
    }
    /**
     * <code>optional uint64 user_write_requests_per_second = 30;</code>
     *
     * <pre>
     ** write requests per second made to region by user 
     * </pre>
     */
    public long getUserWriteRequestsPerSecond() {
      return userWriteRequestsPerSecond_;
    }

    // optional uint64 user_read_requests_by_capacity_unit_per_second = 31;
    public static final int USER_READ_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER = 31;
    private long userReadRequestsByCapacityUnitPerSecond_;
    /**
     * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region by user 
     * </pre>
     */
    public boolean hasUserReadRequestsByCapacityUnitPerSecond() {
      return ((bitField0_ & 0x20000000) == 0x20000000);
    }
    /**
     * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
     *
     * <pre>
     ** the current read capacity unit count per second made to region by user 
     * </pre>
     */
    public long getUserReadRequestsByCapacityUnitPerSecond() {
      return userReadRequestsByCapacityUnitPerSecond_;
    }

    // optional uint64 user_write_requests_by_capacity_unit_per_second = 32;
    public static final int USER_WRITE_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER = 32;
    private long userWriteRequestsByCapacityUnitPerSecond_;
    /**
     * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region by user 
     * </pre>
     */
    public boolean hasUserWriteRequestsByCapacityUnitPerSecond() {
      return ((bitField0_ & 0x40000000) == 0x40000000);
    }
    /**
     * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
     *
     * <pre>
     ** the current write capacity unit count per second made to region by user 
     * </pre>
     */
    public long getUserWriteRequestsByCapacityUnitPerSecond() {
      return userWriteRequestsByCapacityUnitPerSecond_;
    }

    private void initFields() {
      regionSpecifier_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
      stores_ = 0;
      storefiles_ = 0;
      storeUncompressedSizeMB_ = 0;
      storefileSizeMB_ = 0;
      memstoreSizeMB_ = 0;
      storefileIndexSizeMB_ = 0;
      readRequestsCount_ = 0L;
      writeRequestsCount_ = 0L;
      totalCompactingKVs_ = 0L;
      currentCompactedKVs_ = 0L;
      rootIndexSizeKB_ = 0;
      totalStaticIndexSizeKB_ = 0;
      totalStaticBloomSizeKB_ = 0;
      completeSequenceId_ = 0L;
      dataLocality_ = 0F;
      readRequestsPerSecond_ = 0L;
      writeRequestsPerSecond_ = 0L;
      readRequestsByCapacityUnitPerSecond_ = 0L;
      writeRequestsByCapacityUnitPerSecond_ = 0L;
      throttledReadRequestsCount_ = 0L;
      throttledWriteRequestsCount_ = 0L;
      getRequestsCount_ = 0L;
      readCellCountPerSecond_ = 0L;
      readRawCellCountPerSecond_ = 0L;
      scanCountPerSecond_ = 0L;
      scanRowsPerSecond_ = 0L;
      familyInfo_ = java.util.Collections.emptyList();
      userReadRequestsPerSecond_ = 0L;
      userWriteRequestsPerSecond_ = 0L;
      userReadRequestsByCapacityUnitPerSecond_ = 0L;
      userWriteRequestsByCapacityUnitPerSecond_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasRegionSpecifier()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getRegionSpecifier().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getFamilyInfoCount(); i++) {
        if (!getFamilyInfo(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, regionSpecifier_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, stores_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt32(3, storefiles_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt32(4, storeUncompressedSizeMB_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeUInt32(5, storefileSizeMB_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt32(6, memstoreSizeMB_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt32(7, storefileIndexSizeMB_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeUInt64(8, readRequestsCount_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeUInt64(9, writeRequestsCount_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeUInt64(10, totalCompactingKVs_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        output.writeUInt64(11, currentCompactedKVs_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        output.writeUInt32(12, rootIndexSizeKB_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        output.writeUInt32(13, totalStaticIndexSizeKB_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        output.writeUInt32(14, totalStaticBloomSizeKB_);
      }
      if (((bitField0_ & 0x00004000) == 0x00004000)) {
        output.writeUInt64(15, completeSequenceId_);
      }
      if (((bitField0_ & 0x00008000) == 0x00008000)) {
        output.writeFloat(16, dataLocality_);
      }
      if (((bitField0_ & 0x00010000) == 0x00010000)) {
        output.writeUInt64(17, readRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00020000) == 0x00020000)) {
        output.writeUInt64(18, writeRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00040000) == 0x00040000)) {
        output.writeUInt64(19, readRequestsByCapacityUnitPerSecond_);
      }
      if (((bitField0_ & 0x00080000) == 0x00080000)) {
        output.writeUInt64(20, writeRequestsByCapacityUnitPerSecond_);
      }
      if (((bitField0_ & 0x00100000) == 0x00100000)) {
        output.writeUInt64(21, throttledReadRequestsCount_);
      }
      if (((bitField0_ & 0x00200000) == 0x00200000)) {
        output.writeUInt64(22, throttledWriteRequestsCount_);
      }
      if (((bitField0_ & 0x00400000) == 0x00400000)) {
        output.writeUInt64(23, getRequestsCount_);
      }
      if (((bitField0_ & 0x00800000) == 0x00800000)) {
        output.writeUInt64(24, readCellCountPerSecond_);
      }
      if (((bitField0_ & 0x01000000) == 0x01000000)) {
        output.writeUInt64(25, readRawCellCountPerSecond_);
      }
      if (((bitField0_ & 0x02000000) == 0x02000000)) {
        output.writeUInt64(26, scanCountPerSecond_);
      }
      if (((bitField0_ & 0x04000000) == 0x04000000)) {
        output.writeUInt64(27, scanRowsPerSecond_);
      }
      for (int i = 0; i < familyInfo_.size(); i++) {
        output.writeMessage(28, familyInfo_.get(i));
      }
      if (((bitField0_ & 0x08000000) == 0x08000000)) {
        output.writeUInt64(29, userReadRequestsPerSecond_);
      }
      if (((bitField0_ & 0x10000000) == 0x10000000)) {
        output.writeUInt64(30, userWriteRequestsPerSecond_);
      }
      if (((bitField0_ & 0x20000000) == 0x20000000)) {
        output.writeUInt64(31, userReadRequestsByCapacityUnitPerSecond_);
      }
      if (((bitField0_ & 0x40000000) == 0x40000000)) {
        output.writeUInt64(32, userWriteRequestsByCapacityUnitPerSecond_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, regionSpecifier_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, stores_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, storefiles_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(4, storeUncompressedSizeMB_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(5, storefileSizeMB_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(6, memstoreSizeMB_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(7, storefileIndexSizeMB_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(8, readRequestsCount_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(9, writeRequestsCount_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(10, totalCompactingKVs_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(11, currentCompactedKVs_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(12, rootIndexSizeKB_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(13, totalStaticIndexSizeKB_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(14, totalStaticBloomSizeKB_);
      }
      if (((bitField0_ & 0x00004000) == 0x00004000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(15, completeSequenceId_);
      }
      if (((bitField0_ & 0x00008000) == 0x00008000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeFloatSize(16, dataLocality_);
      }
      if (((bitField0_ & 0x00010000) == 0x00010000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(17, readRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00020000) == 0x00020000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(18, writeRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00040000) == 0x00040000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(19, readRequestsByCapacityUnitPerSecond_);
      }
      if (((bitField0_ & 0x00080000) == 0x00080000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(20, writeRequestsByCapacityUnitPerSecond_);
      }
      if (((bitField0_ & 0x00100000) == 0x00100000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(21, throttledReadRequestsCount_);
      }
      if (((bitField0_ & 0x00200000) == 0x00200000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(22, throttledWriteRequestsCount_);
      }
      if (((bitField0_ & 0x00400000) == 0x00400000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(23, getRequestsCount_);
      }
      if (((bitField0_ & 0x00800000) == 0x00800000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(24, readCellCountPerSecond_);
      }
      if (((bitField0_ & 0x01000000) == 0x01000000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(25, readRawCellCountPerSecond_);
      }
      if (((bitField0_ & 0x02000000) == 0x02000000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(26, scanCountPerSecond_);
      }
      if (((bitField0_ & 0x04000000) == 0x04000000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(27, scanRowsPerSecond_);
      }
      for (int i = 0; i < familyInfo_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(28, familyInfo_.get(i));
      }
      if (((bitField0_ & 0x08000000) == 0x08000000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(29, userReadRequestsPerSecond_);
      }
      if (((bitField0_ & 0x10000000) == 0x10000000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(30, userWriteRequestsPerSecond_);
      }
      if (((bitField0_ & 0x20000000) == 0x20000000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(31, userReadRequestsByCapacityUnitPerSecond_);
      }
      if (((bitField0_ & 0x40000000) == 0x40000000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(32, userWriteRequestsByCapacityUnitPerSecond_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad) obj;

      boolean result = true;
      result = result && (hasRegionSpecifier() == other.hasRegionSpecifier());
      if (hasRegionSpecifier()) {
        result = result && getRegionSpecifier()
            .equals(other.getRegionSpecifier());
      }
      result = result && (hasStores() == other.hasStores());
      if (hasStores()) {
        result = result && (getStores()
            == other.getStores());
      }
      result = result && (hasStorefiles() == other.hasStorefiles());
      if (hasStorefiles()) {
        result = result && (getStorefiles()
            == other.getStorefiles());
      }
      result = result && (hasStoreUncompressedSizeMB() == other.hasStoreUncompressedSizeMB());
      if (hasStoreUncompressedSizeMB()) {
        result = result && (getStoreUncompressedSizeMB()
            == other.getStoreUncompressedSizeMB());
      }
      result = result && (hasStorefileSizeMB() == other.hasStorefileSizeMB());
      if (hasStorefileSizeMB()) {
        result = result && (getStorefileSizeMB()
            == other.getStorefileSizeMB());
      }
      result = result && (hasMemstoreSizeMB() == other.hasMemstoreSizeMB());
      if (hasMemstoreSizeMB()) {
        result = result && (getMemstoreSizeMB()
            == other.getMemstoreSizeMB());
      }
      result = result && (hasStorefileIndexSizeMB() == other.hasStorefileIndexSizeMB());
      if (hasStorefileIndexSizeMB()) {
        result = result && (getStorefileIndexSizeMB()
            == other.getStorefileIndexSizeMB());
      }
      result = result && (hasReadRequestsCount() == other.hasReadRequestsCount());
      if (hasReadRequestsCount()) {
        result = result && (getReadRequestsCount()
            == other.getReadRequestsCount());
      }
      result = result && (hasWriteRequestsCount() == other.hasWriteRequestsCount());
      if (hasWriteRequestsCount()) {
        result = result && (getWriteRequestsCount()
            == other.getWriteRequestsCount());
      }
      result = result && (hasTotalCompactingKVs() == other.hasTotalCompactingKVs());
      if (hasTotalCompactingKVs()) {
        result = result && (getTotalCompactingKVs()
            == other.getTotalCompactingKVs());
      }
      result = result && (hasCurrentCompactedKVs() == other.hasCurrentCompactedKVs());
      if (hasCurrentCompactedKVs()) {
        result = result && (getCurrentCompactedKVs()
            == other.getCurrentCompactedKVs());
      }
      result = result && (hasRootIndexSizeKB() == other.hasRootIndexSizeKB());
      if (hasRootIndexSizeKB()) {
        result = result && (getRootIndexSizeKB()
            == other.getRootIndexSizeKB());
      }
      result = result && (hasTotalStaticIndexSizeKB() == other.hasTotalStaticIndexSizeKB());
      if (hasTotalStaticIndexSizeKB()) {
        result = result && (getTotalStaticIndexSizeKB()
            == other.getTotalStaticIndexSizeKB());
      }
      result = result && (hasTotalStaticBloomSizeKB() == other.hasTotalStaticBloomSizeKB());
      if (hasTotalStaticBloomSizeKB()) {
        result = result && (getTotalStaticBloomSizeKB()
            == other.getTotalStaticBloomSizeKB());
      }
      result = result && (hasCompleteSequenceId() == other.hasCompleteSequenceId());
      if (hasCompleteSequenceId()) {
        result = result && (getCompleteSequenceId()
            == other.getCompleteSequenceId());
      }
      result = result && (hasDataLocality() == other.hasDataLocality());
      if (hasDataLocality()) {
        result = result && (Float.floatToIntBits(getDataLocality())    == Float.floatToIntBits(other.getDataLocality()));
      }
      result = result && (hasReadRequestsPerSecond() == other.hasReadRequestsPerSecond());
      if (hasReadRequestsPerSecond()) {
        result = result && (getReadRequestsPerSecond()
            == other.getReadRequestsPerSecond());
      }
      result = result && (hasWriteRequestsPerSecond() == other.hasWriteRequestsPerSecond());
      if (hasWriteRequestsPerSecond()) {
        result = result && (getWriteRequestsPerSecond()
            == other.getWriteRequestsPerSecond());
      }
      result = result && (hasReadRequestsByCapacityUnitPerSecond() == other.hasReadRequestsByCapacityUnitPerSecond());
      if (hasReadRequestsByCapacityUnitPerSecond()) {
        result = result && (getReadRequestsByCapacityUnitPerSecond()
            == other.getReadRequestsByCapacityUnitPerSecond());
      }
      result = result && (hasWriteRequestsByCapacityUnitPerSecond() == other.hasWriteRequestsByCapacityUnitPerSecond());
      if (hasWriteRequestsByCapacityUnitPerSecond()) {
        result = result && (getWriteRequestsByCapacityUnitPerSecond()
            == other.getWriteRequestsByCapacityUnitPerSecond());
      }
      result = result && (hasThrottledReadRequestsCount() == other.hasThrottledReadRequestsCount());
      if (hasThrottledReadRequestsCount()) {
        result = result && (getThrottledReadRequestsCount()
            == other.getThrottledReadRequestsCount());
      }
      result = result && (hasThrottledWriteRequestsCount() == other.hasThrottledWriteRequestsCount());
      if (hasThrottledWriteRequestsCount()) {
        result = result && (getThrottledWriteRequestsCount()
            == other.getThrottledWriteRequestsCount());
      }
      result = result && (hasGetRequestsCount() == other.hasGetRequestsCount());
      if (hasGetRequestsCount()) {
        result = result && (getGetRequestsCount()
            == other.getGetRequestsCount());
      }
      result = result && (hasReadCellCountPerSecond() == other.hasReadCellCountPerSecond());
      if (hasReadCellCountPerSecond()) {
        result = result && (getReadCellCountPerSecond()
            == other.getReadCellCountPerSecond());
      }
      result = result && (hasReadRawCellCountPerSecond() == other.hasReadRawCellCountPerSecond());
      if (hasReadRawCellCountPerSecond()) {
        result = result && (getReadRawCellCountPerSecond()
            == other.getReadRawCellCountPerSecond());
      }
      result = result && (hasScanCountPerSecond() == other.hasScanCountPerSecond());
      if (hasScanCountPerSecond()) {
        result = result && (getScanCountPerSecond()
            == other.getScanCountPerSecond());
      }
      result = result && (hasScanRowsPerSecond() == other.hasScanRowsPerSecond());
      if (hasScanRowsPerSecond()) {
        result = result && (getScanRowsPerSecond()
            == other.getScanRowsPerSecond());
      }
      result = result && getFamilyInfoList()
          .equals(other.getFamilyInfoList());
      result = result && (hasUserReadRequestsPerSecond() == other.hasUserReadRequestsPerSecond());
      if (hasUserReadRequestsPerSecond()) {
        result = result && (getUserReadRequestsPerSecond()
            == other.getUserReadRequestsPerSecond());
      }
      result = result && (hasUserWriteRequestsPerSecond() == other.hasUserWriteRequestsPerSecond());
      if (hasUserWriteRequestsPerSecond()) {
        result = result && (getUserWriteRequestsPerSecond()
            == other.getUserWriteRequestsPerSecond());
      }
      result = result && (hasUserReadRequestsByCapacityUnitPerSecond() == other.hasUserReadRequestsByCapacityUnitPerSecond());
      if (hasUserReadRequestsByCapacityUnitPerSecond()) {
        result = result && (getUserReadRequestsByCapacityUnitPerSecond()
            == other.getUserReadRequestsByCapacityUnitPerSecond());
      }
      result = result && (hasUserWriteRequestsByCapacityUnitPerSecond() == other.hasUserWriteRequestsByCapacityUnitPerSecond());
      if (hasUserWriteRequestsByCapacityUnitPerSecond()) {
        result = result && (getUserWriteRequestsByCapacityUnitPerSecond()
            == other.getUserWriteRequestsByCapacityUnitPerSecond());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasRegionSpecifier()) {
        hash = (37 * hash) + REGION_SPECIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getRegionSpecifier().hashCode();
      }
      if (hasStores()) {
        hash = (37 * hash) + STORES_FIELD_NUMBER;
        hash = (53 * hash) + getStores();
      }
      if (hasStorefiles()) {
        hash = (37 * hash) + STOREFILES_FIELD_NUMBER;
        hash = (53 * hash) + getStorefiles();
      }
      if (hasStoreUncompressedSizeMB()) {
        hash = (37 * hash) + STORE_UNCOMPRESSED_SIZE_MB_FIELD_NUMBER;
        hash = (53 * hash) + getStoreUncompressedSizeMB();
      }
      if (hasStorefileSizeMB()) {
        hash = (37 * hash) + STOREFILE_SIZE_MB_FIELD_NUMBER;
        hash = (53 * hash) + getStorefileSizeMB();
      }
      if (hasMemstoreSizeMB()) {
        hash = (37 * hash) + MEMSTORE_SIZE_MB_FIELD_NUMBER;
        hash = (53 * hash) + getMemstoreSizeMB();
      }
      if (hasStorefileIndexSizeMB()) {
        hash = (37 * hash) + STOREFILE_INDEX_SIZE_MB_FIELD_NUMBER;
        hash = (53 * hash) + getStorefileIndexSizeMB();
      }
      if (hasReadRequestsCount()) {
        hash = (37 * hash) + READ_REQUESTS_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadRequestsCount());
      }
      if (hasWriteRequestsCount()) {
        hash = (37 * hash) + WRITE_REQUESTS_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getWriteRequestsCount());
      }
      if (hasTotalCompactingKVs()) {
        hash = (37 * hash) + TOTAL_COMPACTING_KVS_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTotalCompactingKVs());
      }
      if (hasCurrentCompactedKVs()) {
        hash = (37 * hash) + CURRENT_COMPACTED_KVS_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getCurrentCompactedKVs());
      }
      if (hasRootIndexSizeKB()) {
        hash = (37 * hash) + ROOT_INDEX_SIZE_KB_FIELD_NUMBER;
        hash = (53 * hash) + getRootIndexSizeKB();
      }
      if (hasTotalStaticIndexSizeKB()) {
        hash = (37 * hash) + TOTAL_STATIC_INDEX_SIZE_KB_FIELD_NUMBER;
        hash = (53 * hash) + getTotalStaticIndexSizeKB();
      }
      if (hasTotalStaticBloomSizeKB()) {
        hash = (37 * hash) + TOTAL_STATIC_BLOOM_SIZE_KB_FIELD_NUMBER;
        hash = (53 * hash) + getTotalStaticBloomSizeKB();
      }
      if (hasCompleteSequenceId()) {
        hash = (37 * hash) + COMPLETE_SEQUENCE_ID_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getCompleteSequenceId());
      }
      if (hasDataLocality()) {
        hash = (37 * hash) + DATA_LOCALITY_FIELD_NUMBER;
        hash = (53 * hash) + Float.floatToIntBits(
            getDataLocality());
      }
      if (hasReadRequestsPerSecond()) {
        hash = (37 * hash) + READ_REQUESTS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadRequestsPerSecond());
      }
      if (hasWriteRequestsPerSecond()) {
        hash = (37 * hash) + WRITE_REQUESTS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getWriteRequestsPerSecond());
      }
      if (hasReadRequestsByCapacityUnitPerSecond()) {
        hash = (37 * hash) + READ_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadRequestsByCapacityUnitPerSecond());
      }
      if (hasWriteRequestsByCapacityUnitPerSecond()) {
        hash = (37 * hash) + WRITE_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getWriteRequestsByCapacityUnitPerSecond());
      }
      if (hasThrottledReadRequestsCount()) {
        hash = (37 * hash) + THROTTLED_READ_REQUESTS_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getThrottledReadRequestsCount());
      }
      if (hasThrottledWriteRequestsCount()) {
        hash = (37 * hash) + THROTTLED_WRITE_REQUESTS_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getThrottledWriteRequestsCount());
      }
      if (hasGetRequestsCount()) {
        hash = (37 * hash) + GET_REQUESTS_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getGetRequestsCount());
      }
      if (hasReadCellCountPerSecond()) {
        hash = (37 * hash) + READ_CELL_COUNT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadCellCountPerSecond());
      }
      if (hasReadRawCellCountPerSecond()) {
        hash = (37 * hash) + READ_RAW_CELL_COUNT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadRawCellCountPerSecond());
      }
      if (hasScanCountPerSecond()) {
        hash = (37 * hash) + SCAN_COUNT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getScanCountPerSecond());
      }
      if (hasScanRowsPerSecond()) {
        hash = (37 * hash) + SCAN_ROWS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getScanRowsPerSecond());
      }
      if (getFamilyInfoCount() > 0) {
        hash = (37 * hash) + FAMILY_INFO_FIELD_NUMBER;
        hash = (53 * hash) + getFamilyInfoList().hashCode();
      }
      if (hasUserReadRequestsPerSecond()) {
        hash = (37 * hash) + USER_READ_REQUESTS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getUserReadRequestsPerSecond());
      }
      if (hasUserWriteRequestsPerSecond()) {
        hash = (37 * hash) + USER_WRITE_REQUESTS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getUserWriteRequestsPerSecond());
      }
      if (hasUserReadRequestsByCapacityUnitPerSecond()) {
        hash = (37 * hash) + USER_READ_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getUserReadRequestsByCapacityUnitPerSecond());
      }
      if (hasUserWriteRequestsByCapacityUnitPerSecond()) {
        hash = (37 * hash) + USER_WRITE_REQUESTS_BY_CAPACITY_UNIT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getUserWriteRequestsByCapacityUnitPerSecond());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code RegionLoad}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionLoad_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionLoad_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRegionSpecifierFieldBuilder();
          getFamilyInfoFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (regionSpecifierBuilder_ == null) {
          regionSpecifier_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
        } else {
          regionSpecifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        stores_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        storefiles_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        storeUncompressedSizeMB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        storefileSizeMB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        memstoreSizeMB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000020);
        storefileIndexSizeMB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        readRequestsCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        writeRequestsCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000100);
        totalCompactingKVs_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        currentCompactedKVs_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000400);
        rootIndexSizeKB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000800);
        totalStaticIndexSizeKB_ = 0;
        bitField0_ = (bitField0_ & ~0x00001000);
        totalStaticBloomSizeKB_ = 0;
        bitField0_ = (bitField0_ & ~0x00002000);
        completeSequenceId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00004000);
        dataLocality_ = 0F;
        bitField0_ = (bitField0_ & ~0x00008000);
        readRequestsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00010000);
        writeRequestsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00020000);
        readRequestsByCapacityUnitPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00040000);
        writeRequestsByCapacityUnitPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00080000);
        throttledReadRequestsCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00100000);
        throttledWriteRequestsCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00200000);
        getRequestsCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00400000);
        readCellCountPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00800000);
        readRawCellCountPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x01000000);
        scanCountPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x02000000);
        scanRowsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x04000000);
        if (familyInfoBuilder_ == null) {
          familyInfo_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x08000000);
        } else {
          familyInfoBuilder_.clear();
        }
        userReadRequestsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x10000000);
        userWriteRequestsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x20000000);
        userReadRequestsByCapacityUnitPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x40000000);
        userWriteRequestsByCapacityUnitPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x80000000);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionLoad_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (regionSpecifierBuilder_ == null) {
          result.regionSpecifier_ = regionSpecifier_;
        } else {
          result.regionSpecifier_ = regionSpecifierBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.stores_ = stores_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.storefiles_ = storefiles_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.storeUncompressedSizeMB_ = storeUncompressedSizeMB_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.storefileSizeMB_ = storefileSizeMB_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.memstoreSizeMB_ = memstoreSizeMB_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.storefileIndexSizeMB_ = storefileIndexSizeMB_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        result.readRequestsCount_ = readRequestsCount_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000100;
        }
        result.writeRequestsCount_ = writeRequestsCount_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000200;
        }
        result.totalCompactingKVs_ = totalCompactingKVs_;
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000400;
        }
        result.currentCompactedKVs_ = currentCompactedKVs_;
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000800;
        }
        result.rootIndexSizeKB_ = rootIndexSizeKB_;
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00001000;
        }
        result.totalStaticIndexSizeKB_ = totalStaticIndexSizeKB_;
        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
          to_bitField0_ |= 0x00002000;
        }
        result.totalStaticBloomSizeKB_ = totalStaticBloomSizeKB_;
        if (((from_bitField0_ & 0x00004000) == 0x00004000)) {
          to_bitField0_ |= 0x00004000;
        }
        result.completeSequenceId_ = completeSequenceId_;
        if (((from_bitField0_ & 0x00008000) == 0x00008000)) {
          to_bitField0_ |= 0x00008000;
        }
        result.dataLocality_ = dataLocality_;
        if (((from_bitField0_ & 0x00010000) == 0x00010000)) {
          to_bitField0_ |= 0x00010000;
        }
        result.readRequestsPerSecond_ = readRequestsPerSecond_;
        if (((from_bitField0_ & 0x00020000) == 0x00020000)) {
          to_bitField0_ |= 0x00020000;
        }
        result.writeRequestsPerSecond_ = writeRequestsPerSecond_;
        if (((from_bitField0_ & 0x00040000) == 0x00040000)) {
          to_bitField0_ |= 0x00040000;
        }
        result.readRequestsByCapacityUnitPerSecond_ = readRequestsByCapacityUnitPerSecond_;
        if (((from_bitField0_ & 0x00080000) == 0x00080000)) {
          to_bitField0_ |= 0x00080000;
        }
        result.writeRequestsByCapacityUnitPerSecond_ = writeRequestsByCapacityUnitPerSecond_;
        if (((from_bitField0_ & 0x00100000) == 0x00100000)) {
          to_bitField0_ |= 0x00100000;
        }
        result.throttledReadRequestsCount_ = throttledReadRequestsCount_;
        if (((from_bitField0_ & 0x00200000) == 0x00200000)) {
          to_bitField0_ |= 0x00200000;
        }
        result.throttledWriteRequestsCount_ = throttledWriteRequestsCount_;
        if (((from_bitField0_ & 0x00400000) == 0x00400000)) {
          to_bitField0_ |= 0x00400000;
        }
        result.getRequestsCount_ = getRequestsCount_;
        if (((from_bitField0_ & 0x00800000) == 0x00800000)) {
          to_bitField0_ |= 0x00800000;
        }
        result.readCellCountPerSecond_ = readCellCountPerSecond_;
        if (((from_bitField0_ & 0x01000000) == 0x01000000)) {
          to_bitField0_ |= 0x01000000;
        }
        result.readRawCellCountPerSecond_ = readRawCellCountPerSecond_;
        if (((from_bitField0_ & 0x02000000) == 0x02000000)) {
          to_bitField0_ |= 0x02000000;
        }
        result.scanCountPerSecond_ = scanCountPerSecond_;
        if (((from_bitField0_ & 0x04000000) == 0x04000000)) {
          to_bitField0_ |= 0x04000000;
        }
        result.scanRowsPerSecond_ = scanRowsPerSecond_;
        if (familyInfoBuilder_ == null) {
          if (((bitField0_ & 0x08000000) == 0x08000000)) {
            familyInfo_ = java.util.Collections.unmodifiableList(familyInfo_);
            bitField0_ = (bitField0_ & ~0x08000000);
          }
          result.familyInfo_ = familyInfo_;
        } else {
          result.familyInfo_ = familyInfoBuilder_.build();
        }
        if (((from_bitField0_ & 0x10000000) == 0x10000000)) {
          to_bitField0_ |= 0x08000000;
        }
        result.userReadRequestsPerSecond_ = userReadRequestsPerSecond_;
        if (((from_bitField0_ & 0x20000000) == 0x20000000)) {
          to_bitField0_ |= 0x10000000;
        }
        result.userWriteRequestsPerSecond_ = userWriteRequestsPerSecond_;
        if (((from_bitField0_ & 0x40000000) == 0x40000000)) {
          to_bitField0_ |= 0x20000000;
        }
        result.userReadRequestsByCapacityUnitPerSecond_ = userReadRequestsByCapacityUnitPerSecond_;
        if (((from_bitField0_ & 0x80000000) == 0x80000000)) {
          to_bitField0_ |= 0x40000000;
        }
        result.userWriteRequestsByCapacityUnitPerSecond_ = userWriteRequestsByCapacityUnitPerSecond_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance()) return this;
        if (other.hasRegionSpecifier()) {
          mergeRegionSpecifier(other.getRegionSpecifier());
        }
        if (other.hasStores()) {
          setStores(other.getStores());
        }
        if (other.hasStorefiles()) {
          setStorefiles(other.getStorefiles());
        }
        if (other.hasStoreUncompressedSizeMB()) {
          setStoreUncompressedSizeMB(other.getStoreUncompressedSizeMB());
        }
        if (other.hasStorefileSizeMB()) {
          setStorefileSizeMB(other.getStorefileSizeMB());
        }
        if (other.hasMemstoreSizeMB()) {
          setMemstoreSizeMB(other.getMemstoreSizeMB());
        }
        if (other.hasStorefileIndexSizeMB()) {
          setStorefileIndexSizeMB(other.getStorefileIndexSizeMB());
        }
        if (other.hasReadRequestsCount()) {
          setReadRequestsCount(other.getReadRequestsCount());
        }
        if (other.hasWriteRequestsCount()) {
          setWriteRequestsCount(other.getWriteRequestsCount());
        }
        if (other.hasTotalCompactingKVs()) {
          setTotalCompactingKVs(other.getTotalCompactingKVs());
        }
        if (other.hasCurrentCompactedKVs()) {
          setCurrentCompactedKVs(other.getCurrentCompactedKVs());
        }
        if (other.hasRootIndexSizeKB()) {
          setRootIndexSizeKB(other.getRootIndexSizeKB());
        }
        if (other.hasTotalStaticIndexSizeKB()) {
          setTotalStaticIndexSizeKB(other.getTotalStaticIndexSizeKB());
        }
        if (other.hasTotalStaticBloomSizeKB()) {
          setTotalStaticBloomSizeKB(other.getTotalStaticBloomSizeKB());
        }
        if (other.hasCompleteSequenceId()) {
          setCompleteSequenceId(other.getCompleteSequenceId());
        }
        if (other.hasDataLocality()) {
          setDataLocality(other.getDataLocality());
        }
        if (other.hasReadRequestsPerSecond()) {
          setReadRequestsPerSecond(other.getReadRequestsPerSecond());
        }
        if (other.hasWriteRequestsPerSecond()) {
          setWriteRequestsPerSecond(other.getWriteRequestsPerSecond());
        }
        if (other.hasReadRequestsByCapacityUnitPerSecond()) {
          setReadRequestsByCapacityUnitPerSecond(other.getReadRequestsByCapacityUnitPerSecond());
        }
        if (other.hasWriteRequestsByCapacityUnitPerSecond()) {
          setWriteRequestsByCapacityUnitPerSecond(other.getWriteRequestsByCapacityUnitPerSecond());
        }
        if (other.hasThrottledReadRequestsCount()) {
          setThrottledReadRequestsCount(other.getThrottledReadRequestsCount());
        }
        if (other.hasThrottledWriteRequestsCount()) {
          setThrottledWriteRequestsCount(other.getThrottledWriteRequestsCount());
        }
        if (other.hasGetRequestsCount()) {
          setGetRequestsCount(other.getGetRequestsCount());
        }
        if (other.hasReadCellCountPerSecond()) {
          setReadCellCountPerSecond(other.getReadCellCountPerSecond());
        }
        if (other.hasReadRawCellCountPerSecond()) {
          setReadRawCellCountPerSecond(other.getReadRawCellCountPerSecond());
        }
        if (other.hasScanCountPerSecond()) {
          setScanCountPerSecond(other.getScanCountPerSecond());
        }
        if (other.hasScanRowsPerSecond()) {
          setScanRowsPerSecond(other.getScanRowsPerSecond());
        }
        if (familyInfoBuilder_ == null) {
          if (!other.familyInfo_.isEmpty()) {
            if (familyInfo_.isEmpty()) {
              familyInfo_ = other.familyInfo_;
              bitField0_ = (bitField0_ & ~0x08000000);
            } else {
              ensureFamilyInfoIsMutable();
              familyInfo_.addAll(other.familyInfo_);
            }
            onChanged();
          }
        } else {
          if (!other.familyInfo_.isEmpty()) {
            if (familyInfoBuilder_.isEmpty()) {
              familyInfoBuilder_.dispose();
              familyInfoBuilder_ = null;
              familyInfo_ = other.familyInfo_;
              bitField0_ = (bitField0_ & ~0x08000000);
              familyInfoBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFamilyInfoFieldBuilder() : null;
            } else {
              familyInfoBuilder_.addAllMessages(other.familyInfo_);
            }
          }
        }
        if (other.hasUserReadRequestsPerSecond()) {
          setUserReadRequestsPerSecond(other.getUserReadRequestsPerSecond());
        }
        if (other.hasUserWriteRequestsPerSecond()) {
          setUserWriteRequestsPerSecond(other.getUserWriteRequestsPerSecond());
        }
        if (other.hasUserReadRequestsByCapacityUnitPerSecond()) {
          setUserReadRequestsByCapacityUnitPerSecond(other.getUserReadRequestsByCapacityUnitPerSecond());
        }
        if (other.hasUserWriteRequestsByCapacityUnitPerSecond()) {
          setUserWriteRequestsByCapacityUnitPerSecond(other.getUserWriteRequestsByCapacityUnitPerSecond());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasRegionSpecifier()) {
          
          return false;
        }
        if (!getRegionSpecifier().isInitialized()) {
          
          return false;
        }
        for (int i = 0; i < getFamilyInfoCount(); i++) {
          if (!getFamilyInfo(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .RegionSpecifier region_specifier = 1;
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier regionSpecifier_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> regionSpecifierBuilder_;
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public boolean hasRegionSpecifier() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegionSpecifier() {
        if (regionSpecifierBuilder_ == null) {
          return regionSpecifier_;
        } else {
          return regionSpecifierBuilder_.getMessage();
        }
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public Builder setRegionSpecifier(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
        if (regionSpecifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          regionSpecifier_ = value;
          onChanged();
        } else {
          regionSpecifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public Builder setRegionSpecifier(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder builderForValue) {
        if (regionSpecifierBuilder_ == null) {
          regionSpecifier_ = builderForValue.build();
          onChanged();
        } else {
          regionSpecifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public Builder mergeRegionSpecifier(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
        if (regionSpecifierBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              regionSpecifier_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance()) {
            regionSpecifier_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder(regionSpecifier_).mergeFrom(value).buildPartial();
          } else {
            regionSpecifier_ = value;
          }
          onChanged();
        } else {
          regionSpecifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public Builder clearRegionSpecifier() {
        if (regionSpecifierBuilder_ == null) {
          regionSpecifier_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
          onChanged();
        } else {
          regionSpecifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder getRegionSpecifierBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegionSpecifierFieldBuilder().getBuilder();
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionSpecifierOrBuilder() {
        if (regionSpecifierBuilder_ != null) {
          return regionSpecifierBuilder_.getMessageOrBuilder();
        } else {
          return regionSpecifier_;
        }
      }
      /**
       * <code>required .RegionSpecifier region_specifier = 1;</code>
       *
       * <pre>
       ** the region specifier 
       * </pre>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> 
          getRegionSpecifierFieldBuilder() {
        if (regionSpecifierBuilder_ == null) {
          regionSpecifierBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder>(
                  regionSpecifier_,
                  getParentForChildren(),
                  isClean());
          regionSpecifier_ = null;
        }
        return regionSpecifierBuilder_;
      }

      // optional uint32 stores = 2;
      private int stores_ ;
      /**
       * <code>optional uint32 stores = 2;</code>
       *
       * <pre>
       ** the number of stores for the region 
       * </pre>
       */
      public boolean hasStores() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint32 stores = 2;</code>
       *
       * <pre>
       ** the number of stores for the region 
       * </pre>
       */
      public int getStores() {
        return stores_;
      }
      /**
       * <code>optional uint32 stores = 2;</code>
       *
       * <pre>
       ** the number of stores for the region 
       * </pre>
       */
      public Builder setStores(int value) {
        bitField0_ |= 0x00000002;
        stores_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 stores = 2;</code>
       *
       * <pre>
       ** the number of stores for the region 
       * </pre>
       */
      public Builder clearStores() {
        bitField0_ = (bitField0_ & ~0x00000002);
        stores_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 storefiles = 3;
      private int storefiles_ ;
      /**
       * <code>optional uint32 storefiles = 3;</code>
       *
       * <pre>
       ** the number of storefiles for the region 
       * </pre>
       */
      public boolean hasStorefiles() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint32 storefiles = 3;</code>
       *
       * <pre>
       ** the number of storefiles for the region 
       * </pre>
       */
      public int getStorefiles() {
        return storefiles_;
      }
      /**
       * <code>optional uint32 storefiles = 3;</code>
       *
       * <pre>
       ** the number of storefiles for the region 
       * </pre>
       */
      public Builder setStorefiles(int value) {
        bitField0_ |= 0x00000004;
        storefiles_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 storefiles = 3;</code>
       *
       * <pre>
       ** the number of storefiles for the region 
       * </pre>
       */
      public Builder clearStorefiles() {
        bitField0_ = (bitField0_ & ~0x00000004);
        storefiles_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 store_uncompressed_size_MB = 4;
      private int storeUncompressedSizeMB_ ;
      /**
       * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
       *
       * <pre>
       ** the total size of the store files for the region, uncompressed, in MB 
       * </pre>
       */
      public boolean hasStoreUncompressedSizeMB() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
       *
       * <pre>
       ** the total size of the store files for the region, uncompressed, in MB 
       * </pre>
       */
      public int getStoreUncompressedSizeMB() {
        return storeUncompressedSizeMB_;
      }
      /**
       * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
       *
       * <pre>
       ** the total size of the store files for the region, uncompressed, in MB 
       * </pre>
       */
      public Builder setStoreUncompressedSizeMB(int value) {
        bitField0_ |= 0x00000008;
        storeUncompressedSizeMB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 store_uncompressed_size_MB = 4;</code>
       *
       * <pre>
       ** the total size of the store files for the region, uncompressed, in MB 
       * </pre>
       */
      public Builder clearStoreUncompressedSizeMB() {
        bitField0_ = (bitField0_ & ~0x00000008);
        storeUncompressedSizeMB_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 storefile_size_MB = 5;
      private int storefileSizeMB_ ;
      /**
       * <code>optional uint32 storefile_size_MB = 5;</code>
       *
       * <pre>
       ** the current total size of the store files for the region, in MB 
       * </pre>
       */
      public boolean hasStorefileSizeMB() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional uint32 storefile_size_MB = 5;</code>
       *
       * <pre>
       ** the current total size of the store files for the region, in MB 
       * </pre>
       */
      public int getStorefileSizeMB() {
        return storefileSizeMB_;
      }
      /**
       * <code>optional uint32 storefile_size_MB = 5;</code>
       *
       * <pre>
       ** the current total size of the store files for the region, in MB 
       * </pre>
       */
      public Builder setStorefileSizeMB(int value) {
        bitField0_ |= 0x00000010;
        storefileSizeMB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 storefile_size_MB = 5;</code>
       *
       * <pre>
       ** the current total size of the store files for the region, in MB 
       * </pre>
       */
      public Builder clearStorefileSizeMB() {
        bitField0_ = (bitField0_ & ~0x00000010);
        storefileSizeMB_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 memstore_size_MB = 6;
      private int memstoreSizeMB_ ;
      /**
       * <code>optional uint32 memstore_size_MB = 6;</code>
       *
       * <pre>
       ** the current size of the memstore for the region, in MB 
       * </pre>
       */
      public boolean hasMemstoreSizeMB() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional uint32 memstore_size_MB = 6;</code>
       *
       * <pre>
       ** the current size of the memstore for the region, in MB 
       * </pre>
       */
      public int getMemstoreSizeMB() {
        return memstoreSizeMB_;
      }
      /**
       * <code>optional uint32 memstore_size_MB = 6;</code>
       *
       * <pre>
       ** the current size of the memstore for the region, in MB 
       * </pre>
       */
      public Builder setMemstoreSizeMB(int value) {
        bitField0_ |= 0x00000020;
        memstoreSizeMB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 memstore_size_MB = 6;</code>
       *
       * <pre>
       ** the current size of the memstore for the region, in MB 
       * </pre>
       */
      public Builder clearMemstoreSizeMB() {
        bitField0_ = (bitField0_ & ~0x00000020);
        memstoreSizeMB_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 storefile_index_size_MB = 7;
      private int storefileIndexSizeMB_ ;
      /**
       * <code>optional uint32 storefile_index_size_MB = 7;</code>
       *
       * <pre>
       **
       * The current total size of root-level store file indexes for the region,
       * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
       * </pre>
       */
      public boolean hasStorefileIndexSizeMB() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint32 storefile_index_size_MB = 7;</code>
       *
       * <pre>
       **
       * The current total size of root-level store file indexes for the region,
       * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
       * </pre>
       */
      public int getStorefileIndexSizeMB() {
        return storefileIndexSizeMB_;
      }
      /**
       * <code>optional uint32 storefile_index_size_MB = 7;</code>
       *
       * <pre>
       **
       * The current total size of root-level store file indexes for the region,
       * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
       * </pre>
       */
      public Builder setStorefileIndexSizeMB(int value) {
        bitField0_ |= 0x00000040;
        storefileIndexSizeMB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 storefile_index_size_MB = 7;</code>
       *
       * <pre>
       **
       * The current total size of root-level store file indexes for the region,
       * in MB. The same as {&#64;link #rootIndexSizeKB} but in MB.
       * </pre>
       */
      public Builder clearStorefileIndexSizeMB() {
        bitField0_ = (bitField0_ & ~0x00000040);
        storefileIndexSizeMB_ = 0;
        onChanged();
        return this;
      }

      // optional uint64 read_requests_count = 8;
      private long readRequestsCount_ ;
      /**
       * <code>optional uint64 read_requests_count = 8;</code>
       *
       * <pre>
       ** the current total read requests made to region 
       * </pre>
       */
      public boolean hasReadRequestsCount() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional uint64 read_requests_count = 8;</code>
       *
       * <pre>
       ** the current total read requests made to region 
       * </pre>
       */
      public long getReadRequestsCount() {
        return readRequestsCount_;
      }
      /**
       * <code>optional uint64 read_requests_count = 8;</code>
       *
       * <pre>
       ** the current total read requests made to region 
       * </pre>
       */
      public Builder setReadRequestsCount(long value) {
        bitField0_ |= 0x00000080;
        readRequestsCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_requests_count = 8;</code>
       *
       * <pre>
       ** the current total read requests made to region 
       * </pre>
       */
      public Builder clearReadRequestsCount() {
        bitField0_ = (bitField0_ & ~0x00000080);
        readRequestsCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 write_requests_count = 9;
      private long writeRequestsCount_ ;
      /**
       * <code>optional uint64 write_requests_count = 9;</code>
       *
       * <pre>
       ** the current total write requests made to region 
       * </pre>
       */
      public boolean hasWriteRequestsCount() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint64 write_requests_count = 9;</code>
       *
       * <pre>
       ** the current total write requests made to region 
       * </pre>
       */
      public long getWriteRequestsCount() {
        return writeRequestsCount_;
      }
      /**
       * <code>optional uint64 write_requests_count = 9;</code>
       *
       * <pre>
       ** the current total write requests made to region 
       * </pre>
       */
      public Builder setWriteRequestsCount(long value) {
        bitField0_ |= 0x00000100;
        writeRequestsCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 write_requests_count = 9;</code>
       *
       * <pre>
       ** the current total write requests made to region 
       * </pre>
       */
      public Builder clearWriteRequestsCount() {
        bitField0_ = (bitField0_ & ~0x00000100);
        writeRequestsCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 total_compacting_KVs = 10;
      private long totalCompactingKVs_ ;
      /**
       * <code>optional uint64 total_compacting_KVs = 10;</code>
       *
       * <pre>
       ** the total compacting key values in currently running compaction 
       * </pre>
       */
      public boolean hasTotalCompactingKVs() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional uint64 total_compacting_KVs = 10;</code>
       *
       * <pre>
       ** the total compacting key values in currently running compaction 
       * </pre>
       */
      public long getTotalCompactingKVs() {
        return totalCompactingKVs_;
      }
      /**
       * <code>optional uint64 total_compacting_KVs = 10;</code>
       *
       * <pre>
       ** the total compacting key values in currently running compaction 
       * </pre>
       */
      public Builder setTotalCompactingKVs(long value) {
        bitField0_ |= 0x00000200;
        totalCompactingKVs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 total_compacting_KVs = 10;</code>
       *
       * <pre>
       ** the total compacting key values in currently running compaction 
       * </pre>
       */
      public Builder clearTotalCompactingKVs() {
        bitField0_ = (bitField0_ & ~0x00000200);
        totalCompactingKVs_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 current_compacted_KVs = 11;
      private long currentCompactedKVs_ ;
      /**
       * <code>optional uint64 current_compacted_KVs = 11;</code>
       *
       * <pre>
       ** the completed count of key values in currently running compaction 
       * </pre>
       */
      public boolean hasCurrentCompactedKVs() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional uint64 current_compacted_KVs = 11;</code>
       *
       * <pre>
       ** the completed count of key values in currently running compaction 
       * </pre>
       */
      public long getCurrentCompactedKVs() {
        return currentCompactedKVs_;
      }
      /**
       * <code>optional uint64 current_compacted_KVs = 11;</code>
       *
       * <pre>
       ** the completed count of key values in currently running compaction 
       * </pre>
       */
      public Builder setCurrentCompactedKVs(long value) {
        bitField0_ |= 0x00000400;
        currentCompactedKVs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 current_compacted_KVs = 11;</code>
       *
       * <pre>
       ** the completed count of key values in currently running compaction 
       * </pre>
       */
      public Builder clearCurrentCompactedKVs() {
        bitField0_ = (bitField0_ & ~0x00000400);
        currentCompactedKVs_ = 0L;
        onChanged();
        return this;
      }

      // optional uint32 root_index_size_KB = 12;
      private int rootIndexSizeKB_ ;
      /**
       * <code>optional uint32 root_index_size_KB = 12;</code>
       *
       * <pre>
       ** The current total size of root-level indexes for the region, in KB. 
       * </pre>
       */
      public boolean hasRootIndexSizeKB() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      /**
       * <code>optional uint32 root_index_size_KB = 12;</code>
       *
       * <pre>
       ** The current total size of root-level indexes for the region, in KB. 
       * </pre>
       */
      public int getRootIndexSizeKB() {
        return rootIndexSizeKB_;
      }
      /**
       * <code>optional uint32 root_index_size_KB = 12;</code>
       *
       * <pre>
       ** The current total size of root-level indexes for the region, in KB. 
       * </pre>
       */
      public Builder setRootIndexSizeKB(int value) {
        bitField0_ |= 0x00000800;
        rootIndexSizeKB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 root_index_size_KB = 12;</code>
       *
       * <pre>
       ** The current total size of root-level indexes for the region, in KB. 
       * </pre>
       */
      public Builder clearRootIndexSizeKB() {
        bitField0_ = (bitField0_ & ~0x00000800);
        rootIndexSizeKB_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 total_static_index_size_KB = 13;
      private int totalStaticIndexSizeKB_ ;
      /**
       * <code>optional uint32 total_static_index_size_KB = 13;</code>
       *
       * <pre>
       ** The total size of all index blocks, not just the root level, in KB. 
       * </pre>
       */
      public boolean hasTotalStaticIndexSizeKB() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      /**
       * <code>optional uint32 total_static_index_size_KB = 13;</code>
       *
       * <pre>
       ** The total size of all index blocks, not just the root level, in KB. 
       * </pre>
       */
      public int getTotalStaticIndexSizeKB() {
        return totalStaticIndexSizeKB_;
      }
      /**
       * <code>optional uint32 total_static_index_size_KB = 13;</code>
       *
       * <pre>
       ** The total size of all index blocks, not just the root level, in KB. 
       * </pre>
       */
      public Builder setTotalStaticIndexSizeKB(int value) {
        bitField0_ |= 0x00001000;
        totalStaticIndexSizeKB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 total_static_index_size_KB = 13;</code>
       *
       * <pre>
       ** The total size of all index blocks, not just the root level, in KB. 
       * </pre>
       */
      public Builder clearTotalStaticIndexSizeKB() {
        bitField0_ = (bitField0_ & ~0x00001000);
        totalStaticIndexSizeKB_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 total_static_bloom_size_KB = 14;
      private int totalStaticBloomSizeKB_ ;
      /**
       * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
       *
       * <pre>
       **
       * The total size of all Bloom filter blocks, not just loaded into the
       * block cache, in KB.
       * </pre>
       */
      public boolean hasTotalStaticBloomSizeKB() {
        return ((bitField0_ & 0x00002000) == 0x00002000);
      }
      /**
       * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
       *
       * <pre>
       **
       * The total size of all Bloom filter blocks, not just loaded into the
       * block cache, in KB.
       * </pre>
       */
      public int getTotalStaticBloomSizeKB() {
        return totalStaticBloomSizeKB_;
      }
      /**
       * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
       *
       * <pre>
       **
       * The total size of all Bloom filter blocks, not just loaded into the
       * block cache, in KB.
       * </pre>
       */
      public Builder setTotalStaticBloomSizeKB(int value) {
        bitField0_ |= 0x00002000;
        totalStaticBloomSizeKB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 total_static_bloom_size_KB = 14;</code>
       *
       * <pre>
       **
       * The total size of all Bloom filter blocks, not just loaded into the
       * block cache, in KB.
       * </pre>
       */
      public Builder clearTotalStaticBloomSizeKB() {
        bitField0_ = (bitField0_ & ~0x00002000);
        totalStaticBloomSizeKB_ = 0;
        onChanged();
        return this;
      }

      // optional uint64 complete_sequence_id = 15;
      private long completeSequenceId_ ;
      /**
       * <code>optional uint64 complete_sequence_id = 15;</code>
       *
       * <pre>
       ** the most recent sequence Id from cache flush 
       * </pre>
       */
      public boolean hasCompleteSequenceId() {
        return ((bitField0_ & 0x00004000) == 0x00004000);
      }
      /**
       * <code>optional uint64 complete_sequence_id = 15;</code>
       *
       * <pre>
       ** the most recent sequence Id from cache flush 
       * </pre>
       */
      public long getCompleteSequenceId() {
        return completeSequenceId_;
      }
      /**
       * <code>optional uint64 complete_sequence_id = 15;</code>
       *
       * <pre>
       ** the most recent sequence Id from cache flush 
       * </pre>
       */
      public Builder setCompleteSequenceId(long value) {
        bitField0_ |= 0x00004000;
        completeSequenceId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 complete_sequence_id = 15;</code>
       *
       * <pre>
       ** the most recent sequence Id from cache flush 
       * </pre>
       */
      public Builder clearCompleteSequenceId() {
        bitField0_ = (bitField0_ & ~0x00004000);
        completeSequenceId_ = 0L;
        onChanged();
        return this;
      }

      // optional float data_locality = 16;
      private float dataLocality_ ;
      /**
       * <code>optional float data_locality = 16;</code>
       *
       * <pre>
       ** The current data locality for region in the regionserver 
       * </pre>
       */
      public boolean hasDataLocality() {
        return ((bitField0_ & 0x00008000) == 0x00008000);
      }
      /**
       * <code>optional float data_locality = 16;</code>
       *
       * <pre>
       ** The current data locality for region in the regionserver 
       * </pre>
       */
      public float getDataLocality() {
        return dataLocality_;
      }
      /**
       * <code>optional float data_locality = 16;</code>
       *
       * <pre>
       ** The current data locality for region in the regionserver 
       * </pre>
       */
      public Builder setDataLocality(float value) {
        bitField0_ |= 0x00008000;
        dataLocality_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float data_locality = 16;</code>
       *
       * <pre>
       ** The current data locality for region in the regionserver 
       * </pre>
       */
      public Builder clearDataLocality() {
        bitField0_ = (bitField0_ & ~0x00008000);
        dataLocality_ = 0F;
        onChanged();
        return this;
      }

      // optional uint64 read_requests_per_second = 17;
      private long readRequestsPerSecond_ ;
      /**
       * <code>optional uint64 read_requests_per_second = 17;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public boolean hasReadRequestsPerSecond() {
        return ((bitField0_ & 0x00010000) == 0x00010000);
      }
      /**
       * <code>optional uint64 read_requests_per_second = 17;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public long getReadRequestsPerSecond() {
        return readRequestsPerSecond_;
      }
      /**
       * <code>optional uint64 read_requests_per_second = 17;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public Builder setReadRequestsPerSecond(long value) {
        bitField0_ |= 0x00010000;
        readRequestsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_requests_per_second = 17;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public Builder clearReadRequestsPerSecond() {
        bitField0_ = (bitField0_ & ~0x00010000);
        readRequestsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 write_requests_per_second = 18;
      private long writeRequestsPerSecond_ ;
      /**
       * <code>optional uint64 write_requests_per_second = 18;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public boolean hasWriteRequestsPerSecond() {
        return ((bitField0_ & 0x00020000) == 0x00020000);
      }
      /**
       * <code>optional uint64 write_requests_per_second = 18;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public long getWriteRequestsPerSecond() {
        return writeRequestsPerSecond_;
      }
      /**
       * <code>optional uint64 write_requests_per_second = 18;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public Builder setWriteRequestsPerSecond(long value) {
        bitField0_ |= 0x00020000;
        writeRequestsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 write_requests_per_second = 18;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public Builder clearWriteRequestsPerSecond() {
        bitField0_ = (bitField0_ & ~0x00020000);
        writeRequestsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 read_requests_by_capacity_unit_per_second = 19;
      private long readRequestsByCapacityUnitPerSecond_ ;
      /**
       * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region 
       * </pre>
       */
      public boolean hasReadRequestsByCapacityUnitPerSecond() {
        return ((bitField0_ & 0x00040000) == 0x00040000);
      }
      /**
       * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region 
       * </pre>
       */
      public long getReadRequestsByCapacityUnitPerSecond() {
        return readRequestsByCapacityUnitPerSecond_;
      }
      /**
       * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region 
       * </pre>
       */
      public Builder setReadRequestsByCapacityUnitPerSecond(long value) {
        bitField0_ |= 0x00040000;
        readRequestsByCapacityUnitPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_requests_by_capacity_unit_per_second = 19;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region 
       * </pre>
       */
      public Builder clearReadRequestsByCapacityUnitPerSecond() {
        bitField0_ = (bitField0_ & ~0x00040000);
        readRequestsByCapacityUnitPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 write_requests_by_capacity_unit_per_second = 20;
      private long writeRequestsByCapacityUnitPerSecond_ ;
      /**
       * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region 
       * </pre>
       */
      public boolean hasWriteRequestsByCapacityUnitPerSecond() {
        return ((bitField0_ & 0x00080000) == 0x00080000);
      }
      /**
       * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region 
       * </pre>
       */
      public long getWriteRequestsByCapacityUnitPerSecond() {
        return writeRequestsByCapacityUnitPerSecond_;
      }
      /**
       * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region 
       * </pre>
       */
      public Builder setWriteRequestsByCapacityUnitPerSecond(long value) {
        bitField0_ |= 0x00080000;
        writeRequestsByCapacityUnitPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 write_requests_by_capacity_unit_per_second = 20;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region 
       * </pre>
       */
      public Builder clearWriteRequestsByCapacityUnitPerSecond() {
        bitField0_ = (bitField0_ & ~0x00080000);
        writeRequestsByCapacityUnitPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 throttled_read_requests_count = 21;
      private long throttledReadRequestsCount_ ;
      /**
       * <code>optional uint64 throttled_read_requests_count = 21;</code>
       *
       * <pre>
       ** throttled read requests made to region 
       * </pre>
       */
      public boolean hasThrottledReadRequestsCount() {
        return ((bitField0_ & 0x00100000) == 0x00100000);
      }
      /**
       * <code>optional uint64 throttled_read_requests_count = 21;</code>
       *
       * <pre>
       ** throttled read requests made to region 
       * </pre>
       */
      public long getThrottledReadRequestsCount() {
        return throttledReadRequestsCount_;
      }
      /**
       * <code>optional uint64 throttled_read_requests_count = 21;</code>
       *
       * <pre>
       ** throttled read requests made to region 
       * </pre>
       */
      public Builder setThrottledReadRequestsCount(long value) {
        bitField0_ |= 0x00100000;
        throttledReadRequestsCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 throttled_read_requests_count = 21;</code>
       *
       * <pre>
       ** throttled read requests made to region 
       * </pre>
       */
      public Builder clearThrottledReadRequestsCount() {
        bitField0_ = (bitField0_ & ~0x00100000);
        throttledReadRequestsCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 throttled_write_requests_count = 22;
      private long throttledWriteRequestsCount_ ;
      /**
       * <code>optional uint64 throttled_write_requests_count = 22;</code>
       *
       * <pre>
       ** throttled write requests made to region 
       * </pre>
       */
      public boolean hasThrottledWriteRequestsCount() {
        return ((bitField0_ & 0x00200000) == 0x00200000);
      }
      /**
       * <code>optional uint64 throttled_write_requests_count = 22;</code>
       *
       * <pre>
       ** throttled write requests made to region 
       * </pre>
       */
      public long getThrottledWriteRequestsCount() {
        return throttledWriteRequestsCount_;
      }
      /**
       * <code>optional uint64 throttled_write_requests_count = 22;</code>
       *
       * <pre>
       ** throttled write requests made to region 
       * </pre>
       */
      public Builder setThrottledWriteRequestsCount(long value) {
        bitField0_ |= 0x00200000;
        throttledWriteRequestsCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 throttled_write_requests_count = 22;</code>
       *
       * <pre>
       ** throttled write requests made to region 
       * </pre>
       */
      public Builder clearThrottledWriteRequestsCount() {
        bitField0_ = (bitField0_ & ~0x00200000);
        throttledWriteRequestsCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 get_requests_count = 23;
      private long getRequestsCount_ ;
      /**
       * <code>optional uint64 get_requests_count = 23;</code>
       *
       * <pre>
       ** the current total get requests made to region 
       * </pre>
       */
      public boolean hasGetRequestsCount() {
        return ((bitField0_ & 0x00400000) == 0x00400000);
      }
      /**
       * <code>optional uint64 get_requests_count = 23;</code>
       *
       * <pre>
       ** the current total get requests made to region 
       * </pre>
       */
      public long getGetRequestsCount() {
        return getRequestsCount_;
      }
      /**
       * <code>optional uint64 get_requests_count = 23;</code>
       *
       * <pre>
       ** the current total get requests made to region 
       * </pre>
       */
      public Builder setGetRequestsCount(long value) {
        bitField0_ |= 0x00400000;
        getRequestsCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 get_requests_count = 23;</code>
       *
       * <pre>
       ** the current total get requests made to region 
       * </pre>
       */
      public Builder clearGetRequestsCount() {
        bitField0_ = (bitField0_ & ~0x00400000);
        getRequestsCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 read_cell_count_per_second = 24;
      private long readCellCountPerSecond_ ;
      /**
       * <code>optional uint64 read_cell_count_per_second = 24;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public boolean hasReadCellCountPerSecond() {
        return ((bitField0_ & 0x00800000) == 0x00800000);
      }
      /**
       * <code>optional uint64 read_cell_count_per_second = 24;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public long getReadCellCountPerSecond() {
        return readCellCountPerSecond_;
      }
      /**
       * <code>optional uint64 read_cell_count_per_second = 24;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public Builder setReadCellCountPerSecond(long value) {
        bitField0_ |= 0x00800000;
        readCellCountPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_cell_count_per_second = 24;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public Builder clearReadCellCountPerSecond() {
        bitField0_ = (bitField0_ & ~0x00800000);
        readCellCountPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 read_raw_cell_count_per_second = 25;
      private long readRawCellCountPerSecond_ ;
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public boolean hasReadRawCellCountPerSecond() {
        return ((bitField0_ & 0x01000000) == 0x01000000);
      }
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public long getReadRawCellCountPerSecond() {
        return readRawCellCountPerSecond_;
      }
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public Builder setReadRawCellCountPerSecond(long value) {
        bitField0_ |= 0x01000000;
        readRawCellCountPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 25;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public Builder clearReadRawCellCountPerSecond() {
        bitField0_ = (bitField0_ & ~0x01000000);
        readRawCellCountPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 scan_count_per_second = 26;
      private long scanCountPerSecond_ ;
      /**
       * <code>optional uint64 scan_count_per_second = 26;</code>
       *
       * <pre>
       ** scan count per second made to region 
       * </pre>
       */
      public boolean hasScanCountPerSecond() {
        return ((bitField0_ & 0x02000000) == 0x02000000);
      }
      /**
       * <code>optional uint64 scan_count_per_second = 26;</code>
       *
       * <pre>
       ** scan count per second made to region 
       * </pre>
       */
      public long getScanCountPerSecond() {
        return scanCountPerSecond_;
      }
      /**
       * <code>optional uint64 scan_count_per_second = 26;</code>
       *
       * <pre>
       ** scan count per second made to region 
       * </pre>
       */
      public Builder setScanCountPerSecond(long value) {
        bitField0_ |= 0x02000000;
        scanCountPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 scan_count_per_second = 26;</code>
       *
       * <pre>
       ** scan count per second made to region 
       * </pre>
       */
      public Builder clearScanCountPerSecond() {
        bitField0_ = (bitField0_ & ~0x02000000);
        scanCountPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 scan_rows_per_second = 27;
      private long scanRowsPerSecond_ ;
      /**
       * <code>optional uint64 scan_rows_per_second = 27;</code>
       *
       * <pre>
       ** scan rows per second made to region 
       * </pre>
       */
      public boolean hasScanRowsPerSecond() {
        return ((bitField0_ & 0x04000000) == 0x04000000);
      }
      /**
       * <code>optional uint64 scan_rows_per_second = 27;</code>
       *
       * <pre>
       ** scan rows per second made to region 
       * </pre>
       */
      public long getScanRowsPerSecond() {
        return scanRowsPerSecond_;
      }
      /**
       * <code>optional uint64 scan_rows_per_second = 27;</code>
       *
       * <pre>
       ** scan rows per second made to region 
       * </pre>
       */
      public Builder setScanRowsPerSecond(long value) {
        bitField0_ |= 0x04000000;
        scanRowsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 scan_rows_per_second = 27;</code>
       *
       * <pre>
       ** scan rows per second made to region 
       * </pre>
       */
      public Builder clearScanRowsPerSecond() {
        bitField0_ = (bitField0_ & ~0x04000000);
        scanRowsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // repeated .FamilyInfo family_info = 28;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo> familyInfo_ =
        java.util.Collections.emptyList();
      private void ensureFamilyInfoIsMutable() {
        if (!((bitField0_ & 0x08000000) == 0x08000000)) {
          familyInfo_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo>(familyInfo_);
          bitField0_ |= 0x08000000;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder> familyInfoBuilder_;

      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo> getFamilyInfoList() {
        if (familyInfoBuilder_ == null) {
          return java.util.Collections.unmodifiableList(familyInfo_);
        } else {
          return familyInfoBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public int getFamilyInfoCount() {
        if (familyInfoBuilder_ == null) {
          return familyInfo_.size();
        } else {
          return familyInfoBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo getFamilyInfo(int index) {
        if (familyInfoBuilder_ == null) {
          return familyInfo_.get(index);
        } else {
          return familyInfoBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder setFamilyInfo(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo value) {
        if (familyInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFamilyInfoIsMutable();
          familyInfo_.set(index, value);
          onChanged();
        } else {
          familyInfoBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder setFamilyInfo(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder builderForValue) {
        if (familyInfoBuilder_ == null) {
          ensureFamilyInfoIsMutable();
          familyInfo_.set(index, builderForValue.build());
          onChanged();
        } else {
          familyInfoBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder addFamilyInfo(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo value) {
        if (familyInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFamilyInfoIsMutable();
          familyInfo_.add(value);
          onChanged();
        } else {
          familyInfoBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder addFamilyInfo(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo value) {
        if (familyInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFamilyInfoIsMutable();
          familyInfo_.add(index, value);
          onChanged();
        } else {
          familyInfoBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder addFamilyInfo(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder builderForValue) {
        if (familyInfoBuilder_ == null) {
          ensureFamilyInfoIsMutable();
          familyInfo_.add(builderForValue.build());
          onChanged();
        } else {
          familyInfoBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder addFamilyInfo(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder builderForValue) {
        if (familyInfoBuilder_ == null) {
          ensureFamilyInfoIsMutable();
          familyInfo_.add(index, builderForValue.build());
          onChanged();
        } else {
          familyInfoBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder addAllFamilyInfo(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo> values) {
        if (familyInfoBuilder_ == null) {
          ensureFamilyInfoIsMutable();
          super.addAll(values, familyInfo_);
          onChanged();
        } else {
          familyInfoBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder clearFamilyInfo() {
        if (familyInfoBuilder_ == null) {
          familyInfo_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x08000000);
          onChanged();
        } else {
          familyInfoBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public Builder removeFamilyInfo(int index) {
        if (familyInfoBuilder_ == null) {
          ensureFamilyInfoIsMutable();
          familyInfo_.remove(index);
          onChanged();
        } else {
          familyInfoBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder getFamilyInfoBuilder(
          int index) {
        return getFamilyInfoFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder getFamilyInfoOrBuilder(
          int index) {
        if (familyInfoBuilder_ == null) {
          return familyInfo_.get(index);  } else {
          return familyInfoBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder> 
           getFamilyInfoOrBuilderList() {
        if (familyInfoBuilder_ != null) {
          return familyInfoBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(familyInfo_);
        }
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder addFamilyInfoBuilder() {
        return getFamilyInfoFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder addFamilyInfoBuilder(
          int index) {
        return getFamilyInfoFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .FamilyInfo family_info = 28;</code>
       *
       * <pre>
       ** family info 
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder> 
           getFamilyInfoBuilderList() {
        return getFamilyInfoFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder> 
          getFamilyInfoFieldBuilder() {
        if (familyInfoBuilder_ == null) {
          familyInfoBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.FamilyInfoOrBuilder>(
                  familyInfo_,
                  ((bitField0_ & 0x08000000) == 0x08000000),
                  getParentForChildren(),
                  isClean());
          familyInfo_ = null;
        }
        return familyInfoBuilder_;
      }

      // optional uint64 user_read_requests_per_second = 29;
      private long userReadRequestsPerSecond_ ;
      /**
       * <code>optional uint64 user_read_requests_per_second = 29;</code>
       *
       * <pre>
       ** read requests per second made to region by user 
       * </pre>
       */
      public boolean hasUserReadRequestsPerSecond() {
        return ((bitField0_ & 0x10000000) == 0x10000000);
      }
      /**
       * <code>optional uint64 user_read_requests_per_second = 29;</code>
       *
       * <pre>
       ** read requests per second made to region by user 
       * </pre>
       */
      public long getUserReadRequestsPerSecond() {
        return userReadRequestsPerSecond_;
      }
      /**
       * <code>optional uint64 user_read_requests_per_second = 29;</code>
       *
       * <pre>
       ** read requests per second made to region by user 
       * </pre>
       */
      public Builder setUserReadRequestsPerSecond(long value) {
        bitField0_ |= 0x10000000;
        userReadRequestsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 user_read_requests_per_second = 29;</code>
       *
       * <pre>
       ** read requests per second made to region by user 
       * </pre>
       */
      public Builder clearUserReadRequestsPerSecond() {
        bitField0_ = (bitField0_ & ~0x10000000);
        userReadRequestsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 user_write_requests_per_second = 30;
      private long userWriteRequestsPerSecond_ ;
      /**
       * <code>optional uint64 user_write_requests_per_second = 30;</code>
       *
       * <pre>
       ** write requests per second made to region by user 
       * </pre>
       */
      public boolean hasUserWriteRequestsPerSecond() {
        return ((bitField0_ & 0x20000000) == 0x20000000);
      }
      /**
       * <code>optional uint64 user_write_requests_per_second = 30;</code>
       *
       * <pre>
       ** write requests per second made to region by user 
       * </pre>
       */
      public long getUserWriteRequestsPerSecond() {
        return userWriteRequestsPerSecond_;
      }
      /**
       * <code>optional uint64 user_write_requests_per_second = 30;</code>
       *
       * <pre>
       ** write requests per second made to region by user 
       * </pre>
       */
      public Builder setUserWriteRequestsPerSecond(long value) {
        bitField0_ |= 0x20000000;
        userWriteRequestsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 user_write_requests_per_second = 30;</code>
       *
       * <pre>
       ** write requests per second made to region by user 
       * </pre>
       */
      public Builder clearUserWriteRequestsPerSecond() {
        bitField0_ = (bitField0_ & ~0x20000000);
        userWriteRequestsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 user_read_requests_by_capacity_unit_per_second = 31;
      private long userReadRequestsByCapacityUnitPerSecond_ ;
      /**
       * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region by user 
       * </pre>
       */
      public boolean hasUserReadRequestsByCapacityUnitPerSecond() {
        return ((bitField0_ & 0x40000000) == 0x40000000);
      }
      /**
       * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region by user 
       * </pre>
       */
      public long getUserReadRequestsByCapacityUnitPerSecond() {
        return userReadRequestsByCapacityUnitPerSecond_;
      }
      /**
       * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region by user 
       * </pre>
       */
      public Builder setUserReadRequestsByCapacityUnitPerSecond(long value) {
        bitField0_ |= 0x40000000;
        userReadRequestsByCapacityUnitPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 user_read_requests_by_capacity_unit_per_second = 31;</code>
       *
       * <pre>
       ** the current read capacity unit count per second made to region by user 
       * </pre>
       */
      public Builder clearUserReadRequestsByCapacityUnitPerSecond() {
        bitField0_ = (bitField0_ & ~0x40000000);
        userReadRequestsByCapacityUnitPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 user_write_requests_by_capacity_unit_per_second = 32;
      private long userWriteRequestsByCapacityUnitPerSecond_ ;
      /**
       * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region by user 
       * </pre>
       */
      public boolean hasUserWriteRequestsByCapacityUnitPerSecond() {
        return ((bitField0_ & 0x80000000) == 0x80000000);
      }
      /**
       * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region by user 
       * </pre>
       */
      public long getUserWriteRequestsByCapacityUnitPerSecond() {
        return userWriteRequestsByCapacityUnitPerSecond_;
      }
      /**
       * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region by user 
       * </pre>
       */
      public Builder setUserWriteRequestsByCapacityUnitPerSecond(long value) {
        bitField0_ |= 0x80000000;
        userWriteRequestsByCapacityUnitPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 user_write_requests_by_capacity_unit_per_second = 32;</code>
       *
       * <pre>
       ** the current write capacity unit count per second made to region by user 
       * </pre>
       */
      public Builder clearUserWriteRequestsByCapacityUnitPerSecond() {
        bitField0_ = (bitField0_ & ~0x80000000);
        userWriteRequestsByCapacityUnitPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:RegionLoad)
    }

    static {
      defaultInstance = new RegionLoad(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:RegionLoad)
  }

  public interface ReplicationLoadSinkOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required uint64 ageOfLastAppliedOp = 1;
    /**
     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
     */
    boolean hasAgeOfLastAppliedOp();
    /**
     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
     */
    long getAgeOfLastAppliedOp();

    // required uint64 timeStampsOfLastAppliedOp = 2;
    /**
     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
     */
    boolean hasTimeStampsOfLastAppliedOp();
    /**
     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
     */
    long getTimeStampsOfLastAppliedOp();
  }
  /**
   * Protobuf type {@code ReplicationLoadSink}
   */
  public static final class ReplicationLoadSink extends
      com.google.protobuf.GeneratedMessage
      implements ReplicationLoadSinkOrBuilder {
    // Use ReplicationLoadSink.newBuilder() to construct.
    private ReplicationLoadSink(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReplicationLoadSink(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReplicationLoadSink defaultInstance;
    public static ReplicationLoadSink getDefaultInstance() {
      return defaultInstance;
    }

    public ReplicationLoadSink getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReplicationLoadSink(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              ageOfLastAppliedOp_ = input.readUInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              timeStampsOfLastAppliedOp_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder.class);
    }

    public static com.google.protobuf.Parser<ReplicationLoadSink> PARSER =
        new com.google.protobuf.AbstractParser<ReplicationLoadSink>() {
      public ReplicationLoadSink parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReplicationLoadSink(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReplicationLoadSink> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required uint64 ageOfLastAppliedOp = 1;
    public static final int AGEOFLASTAPPLIEDOP_FIELD_NUMBER = 1;
    private long ageOfLastAppliedOp_;
    /**
     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
     */
    public boolean hasAgeOfLastAppliedOp() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
     */
    public long getAgeOfLastAppliedOp() {
      return ageOfLastAppliedOp_;
    }

    // required uint64 timeStampsOfLastAppliedOp = 2;
    public static final int TIMESTAMPSOFLASTAPPLIEDOP_FIELD_NUMBER = 2;
    private long timeStampsOfLastAppliedOp_;
    /**
     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
     */
    public boolean hasTimeStampsOfLastAppliedOp() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
     */
    public long getTimeStampsOfLastAppliedOp() {
      return timeStampsOfLastAppliedOp_;
    }

    private void initFields() {
      ageOfLastAppliedOp_ = 0L;
      timeStampsOfLastAppliedOp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasAgeOfLastAppliedOp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTimeStampsOfLastAppliedOp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, ageOfLastAppliedOp_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, timeStampsOfLastAppliedOp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, ageOfLastAppliedOp_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, timeStampsOfLastAppliedOp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink) obj;

      boolean result = true;
      result = result && (hasAgeOfLastAppliedOp() == other.hasAgeOfLastAppliedOp());
      if (hasAgeOfLastAppliedOp()) {
        result = result && (getAgeOfLastAppliedOp()
            == other.getAgeOfLastAppliedOp());
      }
      result = result && (hasTimeStampsOfLastAppliedOp() == other.hasTimeStampsOfLastAppliedOp());
      if (hasTimeStampsOfLastAppliedOp()) {
        result = result && (getTimeStampsOfLastAppliedOp()
            == other.getTimeStampsOfLastAppliedOp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasAgeOfLastAppliedOp()) {
        hash = (37 * hash) + AGEOFLASTAPPLIEDOP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getAgeOfLastAppliedOp());
      }
      if (hasTimeStampsOfLastAppliedOp()) {
        hash = (37 * hash) + TIMESTAMPSOFLASTAPPLIEDOP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimeStampsOfLastAppliedOp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code ReplicationLoadSink}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        ageOfLastAppliedOp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        timeStampsOfLastAppliedOp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.ageOfLastAppliedOp_ = ageOfLastAppliedOp_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.timeStampsOfLastAppliedOp_ = timeStampsOfLastAppliedOp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance()) return this;
        if (other.hasAgeOfLastAppliedOp()) {
          setAgeOfLastAppliedOp(other.getAgeOfLastAppliedOp());
        }
        if (other.hasTimeStampsOfLastAppliedOp()) {
          setTimeStampsOfLastAppliedOp(other.getTimeStampsOfLastAppliedOp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasAgeOfLastAppliedOp()) {
          
          return false;
        }
        if (!hasTimeStampsOfLastAppliedOp()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required uint64 ageOfLastAppliedOp = 1;
      private long ageOfLastAppliedOp_ ;
      /**
       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
       */
      public boolean hasAgeOfLastAppliedOp() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
       */
      public long getAgeOfLastAppliedOp() {
        return ageOfLastAppliedOp_;
      }
      /**
       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
       */
      public Builder setAgeOfLastAppliedOp(long value) {
        bitField0_ |= 0x00000001;
        ageOfLastAppliedOp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
       */
      public Builder clearAgeOfLastAppliedOp() {
        bitField0_ = (bitField0_ & ~0x00000001);
        ageOfLastAppliedOp_ = 0L;
        onChanged();
        return this;
      }

      // required uint64 timeStampsOfLastAppliedOp = 2;
      private long timeStampsOfLastAppliedOp_ ;
      /**
       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
       */
      public boolean hasTimeStampsOfLastAppliedOp() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
       */
      public long getTimeStampsOfLastAppliedOp() {
        return timeStampsOfLastAppliedOp_;
      }
      /**
       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
       */
      public Builder setTimeStampsOfLastAppliedOp(long value) {
        bitField0_ |= 0x00000002;
        timeStampsOfLastAppliedOp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
       */
      public Builder clearTimeStampsOfLastAppliedOp() {
        bitField0_ = (bitField0_ & ~0x00000002);
        timeStampsOfLastAppliedOp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:ReplicationLoadSink)
    }

    static {
      defaultInstance = new ReplicationLoadSink(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:ReplicationLoadSink)
  }

  public interface ReplicationLoadSourceOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string peerID = 1;
    /**
     * <code>required string peerID = 1;</code>
     */
    boolean hasPeerID();
    /**
     * <code>required string peerID = 1;</code>
     */
    java.lang.String getPeerID();
    /**
     * <code>required string peerID = 1;</code>
     */
    com.google.protobuf.ByteString
        getPeerIDBytes();

    // required uint64 ageOfLastShippedOp = 2;
    /**
     * <code>required uint64 ageOfLastShippedOp = 2;</code>
     */
    boolean hasAgeOfLastShippedOp();
    /**
     * <code>required uint64 ageOfLastShippedOp = 2;</code>
     */
    long getAgeOfLastShippedOp();

    // required uint32 sizeOfLogQueue = 3;
    /**
     * <code>required uint32 sizeOfLogQueue = 3;</code>
     */
    boolean hasSizeOfLogQueue();
    /**
     * <code>required uint32 sizeOfLogQueue = 3;</code>
     */
    int getSizeOfLogQueue();

    // required uint64 timeStampOfLastShippedOp = 4;
    /**
     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
     */
    boolean hasTimeStampOfLastShippedOp();
    /**
     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
     */
    long getTimeStampOfLastShippedOp();

    // required uint64 replicationLag = 5;
    /**
     * <code>required uint64 replicationLag = 5;</code>
     */
    boolean hasReplicationLag();
    /**
     * <code>required uint64 replicationLag = 5;</code>
     */
    long getReplicationLag();
  }
  /**
   * Protobuf type {@code ReplicationLoadSource}
   */
  public static final class ReplicationLoadSource extends
      com.google.protobuf.GeneratedMessage
      implements ReplicationLoadSourceOrBuilder {
    // Use ReplicationLoadSource.newBuilder() to construct.
    private ReplicationLoadSource(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReplicationLoadSource(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReplicationLoadSource defaultInstance;
    public static ReplicationLoadSource getDefaultInstance() {
      return defaultInstance;
    }

    public ReplicationLoadSource getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReplicationLoadSource(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              peerID_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              ageOfLastShippedOp_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              sizeOfLogQueue_ = input.readUInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              timeStampOfLastShippedOp_ = input.readUInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              replicationLag_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder.class);
    }

    public static com.google.protobuf.Parser<ReplicationLoadSource> PARSER =
        new com.google.protobuf.AbstractParser<ReplicationLoadSource>() {
      public ReplicationLoadSource parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReplicationLoadSource(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReplicationLoadSource> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string peerID = 1;
    public static final int PEERID_FIELD_NUMBER = 1;
    private java.lang.Object peerID_;
    /**
     * <code>required string peerID = 1;</code>
     */
    public boolean hasPeerID() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string peerID = 1;</code>
     */
    public java.lang.String getPeerID() {
      java.lang.Object ref = peerID_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          peerID_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string peerID = 1;</code>
     */
    public com.google.protobuf.ByteString
        getPeerIDBytes() {
      java.lang.Object ref = peerID_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        peerID_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required uint64 ageOfLastShippedOp = 2;
    public static final int AGEOFLASTSHIPPEDOP_FIELD_NUMBER = 2;
    private long ageOfLastShippedOp_;
    /**
     * <code>required uint64 ageOfLastShippedOp = 2;</code>
     */
    public boolean hasAgeOfLastShippedOp() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required uint64 ageOfLastShippedOp = 2;</code>
     */
    public long getAgeOfLastShippedOp() {
      return ageOfLastShippedOp_;
    }

    // required uint32 sizeOfLogQueue = 3;
    public static final int SIZEOFLOGQUEUE_FIELD_NUMBER = 3;
    private int sizeOfLogQueue_;
    /**
     * <code>required uint32 sizeOfLogQueue = 3;</code>
     */
    public boolean hasSizeOfLogQueue() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required uint32 sizeOfLogQueue = 3;</code>
     */
    public int getSizeOfLogQueue() {
      return sizeOfLogQueue_;
    }

    // required uint64 timeStampOfLastShippedOp = 4;
    public static final int TIMESTAMPOFLASTSHIPPEDOP_FIELD_NUMBER = 4;
    private long timeStampOfLastShippedOp_;
    /**
     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
     */
    public boolean hasTimeStampOfLastShippedOp() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
     */
    public long getTimeStampOfLastShippedOp() {
      return timeStampOfLastShippedOp_;
    }

    // required uint64 replicationLag = 5;
    public static final int REPLICATIONLAG_FIELD_NUMBER = 5;
    private long replicationLag_;
    /**
     * <code>required uint64 replicationLag = 5;</code>
     */
    public boolean hasReplicationLag() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>required uint64 replicationLag = 5;</code>
     */
    public long getReplicationLag() {
      return replicationLag_;
    }

    private void initFields() {
      peerID_ = "";
      ageOfLastShippedOp_ = 0L;
      sizeOfLogQueue_ = 0;
      timeStampOfLastShippedOp_ = 0L;
      replicationLag_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasPeerID()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasAgeOfLastShippedOp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSizeOfLogQueue()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTimeStampOfLastShippedOp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasReplicationLag()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getPeerIDBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, ageOfLastShippedOp_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt32(3, sizeOfLogQueue_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, timeStampOfLastShippedOp_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeUInt64(5, replicationLag_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getPeerIDBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, ageOfLastShippedOp_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, sizeOfLogQueue_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, timeStampOfLastShippedOp_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(5, replicationLag_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource) obj;

      boolean result = true;
      result = result && (hasPeerID() == other.hasPeerID());
      if (hasPeerID()) {
        result = result && getPeerID()
            .equals(other.getPeerID());
      }
      result = result && (hasAgeOfLastShippedOp() == other.hasAgeOfLastShippedOp());
      if (hasAgeOfLastShippedOp()) {
        result = result && (getAgeOfLastShippedOp()
            == other.getAgeOfLastShippedOp());
      }
      result = result && (hasSizeOfLogQueue() == other.hasSizeOfLogQueue());
      if (hasSizeOfLogQueue()) {
        result = result && (getSizeOfLogQueue()
            == other.getSizeOfLogQueue());
      }
      result = result && (hasTimeStampOfLastShippedOp() == other.hasTimeStampOfLastShippedOp());
      if (hasTimeStampOfLastShippedOp()) {
        result = result && (getTimeStampOfLastShippedOp()
            == other.getTimeStampOfLastShippedOp());
      }
      result = result && (hasReplicationLag() == other.hasReplicationLag());
      if (hasReplicationLag()) {
        result = result && (getReplicationLag()
            == other.getReplicationLag());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasPeerID()) {
        hash = (37 * hash) + PEERID_FIELD_NUMBER;
        hash = (53 * hash) + getPeerID().hashCode();
      }
      if (hasAgeOfLastShippedOp()) {
        hash = (37 * hash) + AGEOFLASTSHIPPEDOP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getAgeOfLastShippedOp());
      }
      if (hasSizeOfLogQueue()) {
        hash = (37 * hash) + SIZEOFLOGQUEUE_FIELD_NUMBER;
        hash = (53 * hash) + getSizeOfLogQueue();
      }
      if (hasTimeStampOfLastShippedOp()) {
        hash = (37 * hash) + TIMESTAMPOFLASTSHIPPEDOP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimeStampOfLastShippedOp());
      }
      if (hasReplicationLag()) {
        hash = (37 * hash) + REPLICATIONLAG_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReplicationLag());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code ReplicationLoadSource}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        peerID_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        ageOfLastShippedOp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        sizeOfLogQueue_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        timeStampOfLastShippedOp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        replicationLag_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.peerID_ = peerID_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.ageOfLastShippedOp_ = ageOfLastShippedOp_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.sizeOfLogQueue_ = sizeOfLogQueue_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.timeStampOfLastShippedOp_ = timeStampOfLastShippedOp_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.replicationLag_ = replicationLag_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance()) return this;
        if (other.hasPeerID()) {
          bitField0_ |= 0x00000001;
          peerID_ = other.peerID_;
          onChanged();
        }
        if (other.hasAgeOfLastShippedOp()) {
          setAgeOfLastShippedOp(other.getAgeOfLastShippedOp());
        }
        if (other.hasSizeOfLogQueue()) {
          setSizeOfLogQueue(other.getSizeOfLogQueue());
        }
        if (other.hasTimeStampOfLastShippedOp()) {
          setTimeStampOfLastShippedOp(other.getTimeStampOfLastShippedOp());
        }
        if (other.hasReplicationLag()) {
          setReplicationLag(other.getReplicationLag());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasPeerID()) {
          
          return false;
        }
        if (!hasAgeOfLastShippedOp()) {
          
          return false;
        }
        if (!hasSizeOfLogQueue()) {
          
          return false;
        }
        if (!hasTimeStampOfLastShippedOp()) {
          
          return false;
        }
        if (!hasReplicationLag()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string peerID = 1;
      private java.lang.Object peerID_ = "";
      /**
       * <code>required string peerID = 1;</code>
       */
      public boolean hasPeerID() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string peerID = 1;</code>
       */
      public java.lang.String getPeerID() {
        java.lang.Object ref = peerID_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          peerID_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string peerID = 1;</code>
       */
      public com.google.protobuf.ByteString
          getPeerIDBytes() {
        java.lang.Object ref = peerID_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          peerID_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string peerID = 1;</code>
       */
      public Builder setPeerID(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        peerID_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string peerID = 1;</code>
       */
      public Builder clearPeerID() {
        bitField0_ = (bitField0_ & ~0x00000001);
        peerID_ = getDefaultInstance().getPeerID();
        onChanged();
        return this;
      }
      /**
       * <code>required string peerID = 1;</code>
       */
      public Builder setPeerIDBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        peerID_ = value;
        onChanged();
        return this;
      }

      // required uint64 ageOfLastShippedOp = 2;
      private long ageOfLastShippedOp_ ;
      /**
       * <code>required uint64 ageOfLastShippedOp = 2;</code>
       */
      public boolean hasAgeOfLastShippedOp() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required uint64 ageOfLastShippedOp = 2;</code>
       */
      public long getAgeOfLastShippedOp() {
        return ageOfLastShippedOp_;
      }
      /**
       * <code>required uint64 ageOfLastShippedOp = 2;</code>
       */
      public Builder setAgeOfLastShippedOp(long value) {
        bitField0_ |= 0x00000002;
        ageOfLastShippedOp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 ageOfLastShippedOp = 2;</code>
       */
      public Builder clearAgeOfLastShippedOp() {
        bitField0_ = (bitField0_ & ~0x00000002);
        ageOfLastShippedOp_ = 0L;
        onChanged();
        return this;
      }

      // required uint32 sizeOfLogQueue = 3;
      private int sizeOfLogQueue_ ;
      /**
       * <code>required uint32 sizeOfLogQueue = 3;</code>
       */
      public boolean hasSizeOfLogQueue() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required uint32 sizeOfLogQueue = 3;</code>
       */
      public int getSizeOfLogQueue() {
        return sizeOfLogQueue_;
      }
      /**
       * <code>required uint32 sizeOfLogQueue = 3;</code>
       */
      public Builder setSizeOfLogQueue(int value) {
        bitField0_ |= 0x00000004;
        sizeOfLogQueue_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint32 sizeOfLogQueue = 3;</code>
       */
      public Builder clearSizeOfLogQueue() {
        bitField0_ = (bitField0_ & ~0x00000004);
        sizeOfLogQueue_ = 0;
        onChanged();
        return this;
      }

      // required uint64 timeStampOfLastShippedOp = 4;
      private long timeStampOfLastShippedOp_ ;
      /**
       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
       */
      public boolean hasTimeStampOfLastShippedOp() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
       */
      public long getTimeStampOfLastShippedOp() {
        return timeStampOfLastShippedOp_;
      }
      /**
       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
       */
      public Builder setTimeStampOfLastShippedOp(long value) {
        bitField0_ |= 0x00000008;
        timeStampOfLastShippedOp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
       */
      public Builder clearTimeStampOfLastShippedOp() {
        bitField0_ = (bitField0_ & ~0x00000008);
        timeStampOfLastShippedOp_ = 0L;
        onChanged();
        return this;
      }

      // required uint64 replicationLag = 5;
      private long replicationLag_ ;
      /**
       * <code>required uint64 replicationLag = 5;</code>
       */
      public boolean hasReplicationLag() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>required uint64 replicationLag = 5;</code>
       */
      public long getReplicationLag() {
        return replicationLag_;
      }
      /**
       * <code>required uint64 replicationLag = 5;</code>
       */
      public Builder setReplicationLag(long value) {
        bitField0_ |= 0x00000010;
        replicationLag_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 replicationLag = 5;</code>
       */
      public Builder clearReplicationLag() {
        bitField0_ = (bitField0_ & ~0x00000010);
        replicationLag_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:ReplicationLoadSource)
    }

    static {
      defaultInstance = new ReplicationLoadSource(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:ReplicationLoadSource)
  }

  public interface ServerLoadOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional uint32 number_of_requests = 1;
    /**
     * <code>optional uint32 number_of_requests = 1;</code>
     *
     * <pre>
     ** Number of requests since last report. 
     * </pre>
     */
    boolean hasNumberOfRequests();
    /**
     * <code>optional uint32 number_of_requests = 1;</code>
     *
     * <pre>
     ** Number of requests since last report. 
     * </pre>
     */
    int getNumberOfRequests();

    // optional uint32 total_number_of_requests = 2;
    /**
     * <code>optional uint32 total_number_of_requests = 2;</code>
     *
     * <pre>
     ** Total Number of requests from the start of the region server. 
     * </pre>
     */
    boolean hasTotalNumberOfRequests();
    /**
     * <code>optional uint32 total_number_of_requests = 2;</code>
     *
     * <pre>
     ** Total Number of requests from the start of the region server. 
     * </pre>
     */
    int getTotalNumberOfRequests();

    // optional uint32 used_heap_MB = 3;
    /**
     * <code>optional uint32 used_heap_MB = 3;</code>
     *
     * <pre>
     ** the amount of used heap, in MB. 
     * </pre>
     */
    boolean hasUsedHeapMB();
    /**
     * <code>optional uint32 used_heap_MB = 3;</code>
     *
     * <pre>
     ** the amount of used heap, in MB. 
     * </pre>
     */
    int getUsedHeapMB();

    // optional uint32 max_heap_MB = 4;
    /**
     * <code>optional uint32 max_heap_MB = 4;</code>
     *
     * <pre>
     ** the maximum allowable size of the heap, in MB. 
     * </pre>
     */
    boolean hasMaxHeapMB();
    /**
     * <code>optional uint32 max_heap_MB = 4;</code>
     *
     * <pre>
     ** the maximum allowable size of the heap, in MB. 
     * </pre>
     */
    int getMaxHeapMB();

    // repeated .RegionLoad region_loads = 5;
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> 
        getRegionLoadsList();
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index);
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    int getRegionLoadsCount();
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
        getRegionLoadsOrBuilderList();
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
        int index);

    // repeated .Coprocessor coprocessors = 6;
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> 
        getCoprocessorsList();
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index);
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    int getCoprocessorsCount();
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
        getCoprocessorsOrBuilderList();
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
        int index);

    // optional uint64 report_start_time = 7;
    /**
     * <code>optional uint64 report_start_time = 7;</code>
     *
     * <pre>
     **
     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    boolean hasReportStartTime();
    /**
     * <code>optional uint64 report_start_time = 7;</code>
     *
     * <pre>
     **
     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    long getReportStartTime();

    // optional uint64 report_end_time = 8;
    /**
     * <code>optional uint64 report_end_time = 8;</code>
     *
     * <pre>
     **
     * Time when report was generated.
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    boolean hasReportEndTime();
    /**
     * <code>optional uint64 report_end_time = 8;</code>
     *
     * <pre>
     **
     * Time when report was generated.
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    long getReportEndTime();

    // optional uint32 info_server_port = 9;
    /**
     * <code>optional uint32 info_server_port = 9;</code>
     *
     * <pre>
     **
     * The port number that this region server is hosing an info server on.
     * </pre>
     */
    boolean hasInfoServerPort();
    /**
     * <code>optional uint32 info_server_port = 9;</code>
     *
     * <pre>
     **
     * The port number that this region server is hosing an info server on.
     * </pre>
     */
    int getInfoServerPort();

    // repeated .ReplicationLoadSource replLoadSource = 10;
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> 
        getReplLoadSourceList();
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getReplLoadSource(int index);
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    int getReplLoadSourceCount();
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
        getReplLoadSourceOrBuilderList();
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder getReplLoadSourceOrBuilder(
        int index);

    // optional .ReplicationLoadSink replLoadSink = 11;
    /**
     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
     *
     * <pre>
     **
     * The replicationLoadSink for the replication Sink status of this region server.
     * </pre>
     */
    boolean hasReplLoadSink();
    /**
     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
     *
     * <pre>
     **
     * The replicationLoadSink for the replication Sink status of this region server.
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getReplLoadSink();
    /**
     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
     *
     * <pre>
     **
     * The replicationLoadSink for the replication Sink status of this region server.
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder getReplLoadSinkOrBuilder();

    // optional uint64 read_requests_per_second = 12;
    /**
     * <code>optional uint64 read_requests_per_second = 12;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    boolean hasReadRequestsPerSecond();
    /**
     * <code>optional uint64 read_requests_per_second = 12;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    long getReadRequestsPerSecond();

    // optional uint64 write_requests_per_second = 13;
    /**
     * <code>optional uint64 write_requests_per_second = 13;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    boolean hasWriteRequestsPerSecond();
    /**
     * <code>optional uint64 write_requests_per_second = 13;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    long getWriteRequestsPerSecond();

    // optional uint64 read_cell_count_per_second = 14;
    /**
     * <code>optional uint64 read_cell_count_per_second = 14;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    boolean hasReadCellCountPerSecond();
    /**
     * <code>optional uint64 read_cell_count_per_second = 14;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    long getReadCellCountPerSecond();

    // optional uint64 read_raw_cell_count_per_second = 15;
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    boolean hasReadRawCellCountPerSecond();
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    long getReadRawCellCountPerSecond();

    // optional uint64 scan_count_per_second = 16;
    /**
     * <code>optional uint64 scan_count_per_second = 16;</code>
     */
    boolean hasScanCountPerSecond();
    /**
     * <code>optional uint64 scan_count_per_second = 16;</code>
     */
    long getScanCountPerSecond();

    // optional uint64 scan_rows_per_second = 17;
    /**
     * <code>optional uint64 scan_rows_per_second = 17;</code>
     */
    boolean hasScanRowsPerSecond();
    /**
     * <code>optional uint64 scan_rows_per_second = 17;</code>
     */
    long getScanRowsPerSecond();

    // repeated .RegionServerTableLatency region_server_table_latency = 18;
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency> 
        getRegionServerTableLatencyList();
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency getRegionServerTableLatency(int index);
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    int getRegionServerTableLatencyCount();
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder> 
        getRegionServerTableLatencyOrBuilderList();
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder getRegionServerTableLatencyOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code ServerLoad}
   */
  public static final class ServerLoad extends
      com.google.protobuf.GeneratedMessage
      implements ServerLoadOrBuilder {
    // Use ServerLoad.newBuilder() to construct.
    private ServerLoad(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ServerLoad(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ServerLoad defaultInstance;
    public static ServerLoad getDefaultInstance() {
      return defaultInstance;
    }

    public ServerLoad getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ServerLoad(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              numberOfRequests_ = input.readUInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              totalNumberOfRequests_ = input.readUInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              usedHeapMB_ = input.readUInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              maxHeapMB_ = input.readUInt32();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                regionLoads_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad>();
                mutable_bitField0_ |= 0x00000010;
              }
              regionLoads_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.PARSER, extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                coprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>();
                mutable_bitField0_ |= 0x00000020;
              }
              coprocessors_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              bitField0_ |= 0x00000010;
              reportStartTime_ = input.readUInt64();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000020;
              reportEndTime_ = input.readUInt64();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000040;
              infoServerPort_ = input.readUInt32();
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
                replLoadSource_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource>();
                mutable_bitField0_ |= 0x00000200;
              }
              replLoadSource_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.PARSER, extensionRegistry));
              break;
            }
            case 90: {
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) == 0x00000080)) {
                subBuilder = replLoadSink_.toBuilder();
              }
              replLoadSink_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(replLoadSink_);
                replLoadSink_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            case 96: {
              bitField0_ |= 0x00000100;
              readRequestsPerSecond_ = input.readUInt64();
              break;
            }
            case 104: {
              bitField0_ |= 0x00000200;
              writeRequestsPerSecond_ = input.readUInt64();
              break;
            }
            case 112: {
              bitField0_ |= 0x00000400;
              readCellCountPerSecond_ = input.readUInt64();
              break;
            }
            case 120: {
              bitField0_ |= 0x00000800;
              readRawCellCountPerSecond_ = input.readUInt64();
              break;
            }
            case 128: {
              bitField0_ |= 0x00001000;
              scanCountPerSecond_ = input.readUInt64();
              break;
            }
            case 136: {
              bitField0_ |= 0x00002000;
              scanRowsPerSecond_ = input.readUInt64();
              break;
            }
            case 146: {
              if (!((mutable_bitField0_ & 0x00020000) == 0x00020000)) {
                regionServerTableLatency_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency>();
                mutable_bitField0_ |= 0x00020000;
              }
              regionServerTableLatency_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          regionLoads_ = java.util.Collections.unmodifiableList(regionLoads_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          coprocessors_ = java.util.Collections.unmodifiableList(coprocessors_);
        }
        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
          replLoadSource_ = java.util.Collections.unmodifiableList(replLoadSource_);
        }
        if (((mutable_bitField0_ & 0x00020000) == 0x00020000)) {
          regionServerTableLatency_ = java.util.Collections.unmodifiableList(regionServerTableLatency_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder.class);
    }

    public static com.google.protobuf.Parser<ServerLoad> PARSER =
        new com.google.protobuf.AbstractParser<ServerLoad>() {
      public ServerLoad parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ServerLoad(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ServerLoad> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional uint32 number_of_requests = 1;
    public static final int NUMBER_OF_REQUESTS_FIELD_NUMBER = 1;
    private int numberOfRequests_;
    /**
     * <code>optional uint32 number_of_requests = 1;</code>
     *
     * <pre>
     ** Number of requests since last report. 
     * </pre>
     */
    public boolean hasNumberOfRequests() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional uint32 number_of_requests = 1;</code>
     *
     * <pre>
     ** Number of requests since last report. 
     * </pre>
     */
    public int getNumberOfRequests() {
      return numberOfRequests_;
    }

    // optional uint32 total_number_of_requests = 2;
    public static final int TOTAL_NUMBER_OF_REQUESTS_FIELD_NUMBER = 2;
    private int totalNumberOfRequests_;
    /**
     * <code>optional uint32 total_number_of_requests = 2;</code>
     *
     * <pre>
     ** Total Number of requests from the start of the region server. 
     * </pre>
     */
    public boolean hasTotalNumberOfRequests() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint32 total_number_of_requests = 2;</code>
     *
     * <pre>
     ** Total Number of requests from the start of the region server. 
     * </pre>
     */
    public int getTotalNumberOfRequests() {
      return totalNumberOfRequests_;
    }

    // optional uint32 used_heap_MB = 3;
    public static final int USED_HEAP_MB_FIELD_NUMBER = 3;
    private int usedHeapMB_;
    /**
     * <code>optional uint32 used_heap_MB = 3;</code>
     *
     * <pre>
     ** the amount of used heap, in MB. 
     * </pre>
     */
    public boolean hasUsedHeapMB() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint32 used_heap_MB = 3;</code>
     *
     * <pre>
     ** the amount of used heap, in MB. 
     * </pre>
     */
    public int getUsedHeapMB() {
      return usedHeapMB_;
    }

    // optional uint32 max_heap_MB = 4;
    public static final int MAX_HEAP_MB_FIELD_NUMBER = 4;
    private int maxHeapMB_;
    /**
     * <code>optional uint32 max_heap_MB = 4;</code>
     *
     * <pre>
     ** the maximum allowable size of the heap, in MB. 
     * </pre>
     */
    public boolean hasMaxHeapMB() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint32 max_heap_MB = 4;</code>
     *
     * <pre>
     ** the maximum allowable size of the heap, in MB. 
     * </pre>
     */
    public int getMaxHeapMB() {
      return maxHeapMB_;
    }

    // repeated .RegionLoad region_loads = 5;
    public static final int REGION_LOADS_FIELD_NUMBER = 5;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> regionLoads_;
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> getRegionLoadsList() {
      return regionLoads_;
    }
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
        getRegionLoadsOrBuilderList() {
      return regionLoads_;
    }
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    public int getRegionLoadsCount() {
      return regionLoads_.size();
    }
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index) {
      return regionLoads_.get(index);
    }
    /**
     * <code>repeated .RegionLoad region_loads = 5;</code>
     *
     * <pre>
     ** Information on the load of individual regions. 
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
        int index) {
      return regionLoads_.get(index);
    }

    // repeated .Coprocessor coprocessors = 6;
    public static final int COPROCESSORS_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> coprocessors_;
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getCoprocessorsList() {
      return coprocessors_;
    }
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
        getCoprocessorsOrBuilderList() {
      return coprocessors_;
    }
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    public int getCoprocessorsCount() {
      return coprocessors_.size();
    }
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index) {
      return coprocessors_.get(index);
    }
    /**
     * <code>repeated .Coprocessor coprocessors = 6;</code>
     *
     * <pre>
     **
     * Regionserver-level coprocessors, e.g., WALObserver implementations.
     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
     * objects.
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
        int index) {
      return coprocessors_.get(index);
    }

    // optional uint64 report_start_time = 7;
    public static final int REPORT_START_TIME_FIELD_NUMBER = 7;
    private long reportStartTime_;
    /**
     * <code>optional uint64 report_start_time = 7;</code>
     *
     * <pre>
     **
     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    public boolean hasReportStartTime() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional uint64 report_start_time = 7;</code>
     *
     * <pre>
     **
     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    public long getReportStartTime() {
      return reportStartTime_;
    }

    // optional uint64 report_end_time = 8;
    public static final int REPORT_END_TIME_FIELD_NUMBER = 8;
    private long reportEndTime_;
    /**
     * <code>optional uint64 report_end_time = 8;</code>
     *
     * <pre>
     **
     * Time when report was generated.
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    public boolean hasReportEndTime() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional uint64 report_end_time = 8;</code>
     *
     * <pre>
     **
     * Time when report was generated.
     * time is measured as the difference, measured in milliseconds, between the current time
     * and midnight, January 1, 1970 UTC.
     * </pre>
     */
    public long getReportEndTime() {
      return reportEndTime_;
    }

    // optional uint32 info_server_port = 9;
    public static final int INFO_SERVER_PORT_FIELD_NUMBER = 9;
    private int infoServerPort_;
    /**
     * <code>optional uint32 info_server_port = 9;</code>
     *
     * <pre>
     **
     * The port number that this region server is hosing an info server on.
     * </pre>
     */
    public boolean hasInfoServerPort() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional uint32 info_server_port = 9;</code>
     *
     * <pre>
     **
     * The port number that this region server is hosing an info server on.
     * </pre>
     */
    public int getInfoServerPort() {
      return infoServerPort_;
    }

    // repeated .ReplicationLoadSource replLoadSource = 10;
    public static final int REPLLOADSOURCE_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> replLoadSource_;
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> getReplLoadSourceList() {
      return replLoadSource_;
    }
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
        getReplLoadSourceOrBuilderList() {
      return replLoadSource_;
    }
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    public int getReplLoadSourceCount() {
      return replLoadSource_.size();
    }
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getReplLoadSource(int index) {
      return replLoadSource_.get(index);
    }
    /**
     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
     *
     * <pre>
     **
     * The replicationLoadSource for the replication Source status of this region server.
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder getReplLoadSourceOrBuilder(
        int index) {
      return replLoadSource_.get(index);
    }

    // optional .ReplicationLoadSink replLoadSink = 11;
    public static final int REPLLOADSINK_FIELD_NUMBER = 11;
    private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink replLoadSink_;
    /**
     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
     *
     * <pre>
     **
     * The replicationLoadSink for the replication Sink status of this region server.
     * </pre>
     */
    public boolean hasReplLoadSink() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
     *
     * <pre>
     **
     * The replicationLoadSink for the replication Sink status of this region server.
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getReplLoadSink() {
      return replLoadSink_;
    }
    /**
     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
     *
     * <pre>
     **
     * The replicationLoadSink for the replication Sink status of this region server.
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder getReplLoadSinkOrBuilder() {
      return replLoadSink_;
    }

    // optional uint64 read_requests_per_second = 12;
    public static final int READ_REQUESTS_PER_SECOND_FIELD_NUMBER = 12;
    private long readRequestsPerSecond_;
    /**
     * <code>optional uint64 read_requests_per_second = 12;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    public boolean hasReadRequestsPerSecond() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional uint64 read_requests_per_second = 12;</code>
     *
     * <pre>
     ** read requests per second made to region 
     * </pre>
     */
    public long getReadRequestsPerSecond() {
      return readRequestsPerSecond_;
    }

    // optional uint64 write_requests_per_second = 13;
    public static final int WRITE_REQUESTS_PER_SECOND_FIELD_NUMBER = 13;
    private long writeRequestsPerSecond_;
    /**
     * <code>optional uint64 write_requests_per_second = 13;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    public boolean hasWriteRequestsPerSecond() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    /**
     * <code>optional uint64 write_requests_per_second = 13;</code>
     *
     * <pre>
     ** write requests per second made to region 
     * </pre>
     */
    public long getWriteRequestsPerSecond() {
      return writeRequestsPerSecond_;
    }

    // optional uint64 read_cell_count_per_second = 14;
    public static final int READ_CELL_COUNT_PER_SECOND_FIELD_NUMBER = 14;
    private long readCellCountPerSecond_;
    /**
     * <code>optional uint64 read_cell_count_per_second = 14;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    public boolean hasReadCellCountPerSecond() {
      return ((bitField0_ & 0x00000400) == 0x00000400);
    }
    /**
     * <code>optional uint64 read_cell_count_per_second = 14;</code>
     *
     * <pre>
     ** cell read per second made to region 
     * </pre>
     */
    public long getReadCellCountPerSecond() {
      return readCellCountPerSecond_;
    }

    // optional uint64 read_raw_cell_count_per_second = 15;
    public static final int READ_RAW_CELL_COUNT_PER_SECOND_FIELD_NUMBER = 15;
    private long readRawCellCountPerSecond_;
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    public boolean hasReadRawCellCountPerSecond() {
      return ((bitField0_ & 0x00000800) == 0x00000800);
    }
    /**
     * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
     *
     * <pre>
     ** raw cell read per second made to region 
     * </pre>
     */
    public long getReadRawCellCountPerSecond() {
      return readRawCellCountPerSecond_;
    }

    // optional uint64 scan_count_per_second = 16;
    public static final int SCAN_COUNT_PER_SECOND_FIELD_NUMBER = 16;
    private long scanCountPerSecond_;
    /**
     * <code>optional uint64 scan_count_per_second = 16;</code>
     */
    public boolean hasScanCountPerSecond() {
      return ((bitField0_ & 0x00001000) == 0x00001000);
    }
    /**
     * <code>optional uint64 scan_count_per_second = 16;</code>
     */
    public long getScanCountPerSecond() {
      return scanCountPerSecond_;
    }

    // optional uint64 scan_rows_per_second = 17;
    public static final int SCAN_ROWS_PER_SECOND_FIELD_NUMBER = 17;
    private long scanRowsPerSecond_;
    /**
     * <code>optional uint64 scan_rows_per_second = 17;</code>
     */
    public boolean hasScanRowsPerSecond() {
      return ((bitField0_ & 0x00002000) == 0x00002000);
    }
    /**
     * <code>optional uint64 scan_rows_per_second = 17;</code>
     */
    public long getScanRowsPerSecond() {
      return scanRowsPerSecond_;
    }

    // repeated .RegionServerTableLatency region_server_table_latency = 18;
    public static final int REGION_SERVER_TABLE_LATENCY_FIELD_NUMBER = 18;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency> regionServerTableLatency_;
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency> getRegionServerTableLatencyList() {
      return regionServerTableLatency_;
    }
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder> 
        getRegionServerTableLatencyOrBuilderList() {
      return regionServerTableLatency_;
    }
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    public int getRegionServerTableLatencyCount() {
      return regionServerTableLatency_.size();
    }
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency getRegionServerTableLatency(int index) {
      return regionServerTableLatency_.get(index);
    }
    /**
     * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder getRegionServerTableLatencyOrBuilder(
        int index) {
      return regionServerTableLatency_.get(index);
    }

    private void initFields() {
      numberOfRequests_ = 0;
      totalNumberOfRequests_ = 0;
      usedHeapMB_ = 0;
      maxHeapMB_ = 0;
      regionLoads_ = java.util.Collections.emptyList();
      coprocessors_ = java.util.Collections.emptyList();
      reportStartTime_ = 0L;
      reportEndTime_ = 0L;
      infoServerPort_ = 0;
      replLoadSource_ = java.util.Collections.emptyList();
      replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
      readRequestsPerSecond_ = 0L;
      writeRequestsPerSecond_ = 0L;
      readCellCountPerSecond_ = 0L;
      readRawCellCountPerSecond_ = 0L;
      scanCountPerSecond_ = 0L;
      scanRowsPerSecond_ = 0L;
      regionServerTableLatency_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getRegionLoadsCount(); i++) {
        if (!getRegionLoads(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getCoprocessorsCount(); i++) {
        if (!getCoprocessors(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getReplLoadSourceCount(); i++) {
        if (!getReplLoadSource(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasReplLoadSink()) {
        if (!getReplLoadSink().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getRegionServerTableLatencyCount(); i++) {
        if (!getRegionServerTableLatency(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt32(1, numberOfRequests_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, totalNumberOfRequests_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt32(3, usedHeapMB_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt32(4, maxHeapMB_);
      }
      for (int i = 0; i < regionLoads_.size(); i++) {
        output.writeMessage(5, regionLoads_.get(i));
      }
      for (int i = 0; i < coprocessors_.size(); i++) {
        output.writeMessage(6, coprocessors_.get(i));
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeUInt64(7, reportStartTime_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt64(8, reportEndTime_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt32(9, infoServerPort_);
      }
      for (int i = 0; i < replLoadSource_.size(); i++) {
        output.writeMessage(10, replLoadSource_.get(i));
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeMessage(11, replLoadSink_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeUInt64(12, readRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeUInt64(13, writeRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        output.writeUInt64(14, readCellCountPerSecond_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        output.writeUInt64(15, readRawCellCountPerSecond_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        output.writeUInt64(16, scanCountPerSecond_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        output.writeUInt64(17, scanRowsPerSecond_);
      }
      for (int i = 0; i < regionServerTableLatency_.size(); i++) {
        output.writeMessage(18, regionServerTableLatency_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, numberOfRequests_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, totalNumberOfRequests_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, usedHeapMB_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(4, maxHeapMB_);
      }
      for (int i = 0; i < regionLoads_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, regionLoads_.get(i));
      }
      for (int i = 0; i < coprocessors_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, coprocessors_.get(i));
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(7, reportStartTime_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(8, reportEndTime_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(9, infoServerPort_);
      }
      for (int i = 0; i < replLoadSource_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, replLoadSource_.get(i));
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, replLoadSink_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(12, readRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(13, writeRequestsPerSecond_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(14, readCellCountPerSecond_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(15, readRawCellCountPerSecond_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(16, scanCountPerSecond_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(17, scanRowsPerSecond_);
      }
      for (int i = 0; i < regionServerTableLatency_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(18, regionServerTableLatency_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) obj;

      boolean result = true;
      result = result && (hasNumberOfRequests() == other.hasNumberOfRequests());
      if (hasNumberOfRequests()) {
        result = result && (getNumberOfRequests()
            == other.getNumberOfRequests());
      }
      result = result && (hasTotalNumberOfRequests() == other.hasTotalNumberOfRequests());
      if (hasTotalNumberOfRequests()) {
        result = result && (getTotalNumberOfRequests()
            == other.getTotalNumberOfRequests());
      }
      result = result && (hasUsedHeapMB() == other.hasUsedHeapMB());
      if (hasUsedHeapMB()) {
        result = result && (getUsedHeapMB()
            == other.getUsedHeapMB());
      }
      result = result && (hasMaxHeapMB() == other.hasMaxHeapMB());
      if (hasMaxHeapMB()) {
        result = result && (getMaxHeapMB()
            == other.getMaxHeapMB());
      }
      result = result && getRegionLoadsList()
          .equals(other.getRegionLoadsList());
      result = result && getCoprocessorsList()
          .equals(other.getCoprocessorsList());
      result = result && (hasReportStartTime() == other.hasReportStartTime());
      if (hasReportStartTime()) {
        result = result && (getReportStartTime()
            == other.getReportStartTime());
      }
      result = result && (hasReportEndTime() == other.hasReportEndTime());
      if (hasReportEndTime()) {
        result = result && (getReportEndTime()
            == other.getReportEndTime());
      }
      result = result && (hasInfoServerPort() == other.hasInfoServerPort());
      if (hasInfoServerPort()) {
        result = result && (getInfoServerPort()
            == other.getInfoServerPort());
      }
      result = result && getReplLoadSourceList()
          .equals(other.getReplLoadSourceList());
      result = result && (hasReplLoadSink() == other.hasReplLoadSink());
      if (hasReplLoadSink()) {
        result = result && getReplLoadSink()
            .equals(other.getReplLoadSink());
      }
      result = result && (hasReadRequestsPerSecond() == other.hasReadRequestsPerSecond());
      if (hasReadRequestsPerSecond()) {
        result = result && (getReadRequestsPerSecond()
            == other.getReadRequestsPerSecond());
      }
      result = result && (hasWriteRequestsPerSecond() == other.hasWriteRequestsPerSecond());
      if (hasWriteRequestsPerSecond()) {
        result = result && (getWriteRequestsPerSecond()
            == other.getWriteRequestsPerSecond());
      }
      result = result && (hasReadCellCountPerSecond() == other.hasReadCellCountPerSecond());
      if (hasReadCellCountPerSecond()) {
        result = result && (getReadCellCountPerSecond()
            == other.getReadCellCountPerSecond());
      }
      result = result && (hasReadRawCellCountPerSecond() == other.hasReadRawCellCountPerSecond());
      if (hasReadRawCellCountPerSecond()) {
        result = result && (getReadRawCellCountPerSecond()
            == other.getReadRawCellCountPerSecond());
      }
      result = result && (hasScanCountPerSecond() == other.hasScanCountPerSecond());
      if (hasScanCountPerSecond()) {
        result = result && (getScanCountPerSecond()
            == other.getScanCountPerSecond());
      }
      result = result && (hasScanRowsPerSecond() == other.hasScanRowsPerSecond());
      if (hasScanRowsPerSecond()) {
        result = result && (getScanRowsPerSecond()
            == other.getScanRowsPerSecond());
      }
      result = result && getRegionServerTableLatencyList()
          .equals(other.getRegionServerTableLatencyList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNumberOfRequests()) {
        hash = (37 * hash) + NUMBER_OF_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getNumberOfRequests();
      }
      if (hasTotalNumberOfRequests()) {
        hash = (37 * hash) + TOTAL_NUMBER_OF_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getTotalNumberOfRequests();
      }
      if (hasUsedHeapMB()) {
        hash = (37 * hash) + USED_HEAP_MB_FIELD_NUMBER;
        hash = (53 * hash) + getUsedHeapMB();
      }
      if (hasMaxHeapMB()) {
        hash = (37 * hash) + MAX_HEAP_MB_FIELD_NUMBER;
        hash = (53 * hash) + getMaxHeapMB();
      }
      if (getRegionLoadsCount() > 0) {
        hash = (37 * hash) + REGION_LOADS_FIELD_NUMBER;
        hash = (53 * hash) + getRegionLoadsList().hashCode();
      }
      if (getCoprocessorsCount() > 0) {
        hash = (37 * hash) + COPROCESSORS_FIELD_NUMBER;
        hash = (53 * hash) + getCoprocessorsList().hashCode();
      }
      if (hasReportStartTime()) {
        hash = (37 * hash) + REPORT_START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReportStartTime());
      }
      if (hasReportEndTime()) {
        hash = (37 * hash) + REPORT_END_TIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReportEndTime());
      }
      if (hasInfoServerPort()) {
        hash = (37 * hash) + INFO_SERVER_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getInfoServerPort();
      }
      if (getReplLoadSourceCount() > 0) {
        hash = (37 * hash) + REPLLOADSOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getReplLoadSourceList().hashCode();
      }
      if (hasReplLoadSink()) {
        hash = (37 * hash) + REPLLOADSINK_FIELD_NUMBER;
        hash = (53 * hash) + getReplLoadSink().hashCode();
      }
      if (hasReadRequestsPerSecond()) {
        hash = (37 * hash) + READ_REQUESTS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadRequestsPerSecond());
      }
      if (hasWriteRequestsPerSecond()) {
        hash = (37 * hash) + WRITE_REQUESTS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getWriteRequestsPerSecond());
      }
      if (hasReadCellCountPerSecond()) {
        hash = (37 * hash) + READ_CELL_COUNT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadCellCountPerSecond());
      }
      if (hasReadRawCellCountPerSecond()) {
        hash = (37 * hash) + READ_RAW_CELL_COUNT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getReadRawCellCountPerSecond());
      }
      if (hasScanCountPerSecond()) {
        hash = (37 * hash) + SCAN_COUNT_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getScanCountPerSecond());
      }
      if (hasScanRowsPerSecond()) {
        hash = (37 * hash) + SCAN_ROWS_PER_SECOND_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getScanRowsPerSecond());
      }
      if (getRegionServerTableLatencyCount() > 0) {
        hash = (37 * hash) + REGION_SERVER_TABLE_LATENCY_FIELD_NUMBER;
        hash = (53 * hash) + getRegionServerTableLatencyList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code ServerLoad}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRegionLoadsFieldBuilder();
          getCoprocessorsFieldBuilder();
          getReplLoadSourceFieldBuilder();
          getReplLoadSinkFieldBuilder();
          getRegionServerTableLatencyFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        numberOfRequests_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        totalNumberOfRequests_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        usedHeapMB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        maxHeapMB_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (regionLoadsBuilder_ == null) {
          regionLoads_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          regionLoadsBuilder_.clear();
        }
        if (coprocessorsBuilder_ == null) {
          coprocessors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          coprocessorsBuilder_.clear();
        }
        reportStartTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        reportEndTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        infoServerPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00000100);
        if (replLoadSourceBuilder_ == null) {
          replLoadSource_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          replLoadSourceBuilder_.clear();
        }
        if (replLoadSinkBuilder_ == null) {
          replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
        } else {
          replLoadSinkBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000400);
        readRequestsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000800);
        writeRequestsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00001000);
        readCellCountPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00002000);
        readRawCellCountPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00004000);
        scanCountPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00008000);
        scanRowsPerSecond_ = 0L;
        bitField0_ = (bitField0_ & ~0x00010000);
        if (regionServerTableLatencyBuilder_ == null) {
          regionServerTableLatency_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00020000);
        } else {
          regionServerTableLatencyBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.numberOfRequests_ = numberOfRequests_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.totalNumberOfRequests_ = totalNumberOfRequests_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.usedHeapMB_ = usedHeapMB_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.maxHeapMB_ = maxHeapMB_;
        if (regionLoadsBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010)) {
            regionLoads_ = java.util.Collections.unmodifiableList(regionLoads_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.regionLoads_ = regionLoads_;
        } else {
          result.regionLoads_ = regionLoadsBuilder_.build();
        }
        if (coprocessorsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            coprocessors_ = java.util.Collections.unmodifiableList(coprocessors_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.coprocessors_ = coprocessors_;
        } else {
          result.coprocessors_ = coprocessorsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000010;
        }
        result.reportStartTime_ = reportStartTime_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000020;
        }
        result.reportEndTime_ = reportEndTime_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000040;
        }
        result.infoServerPort_ = infoServerPort_;
        if (replLoadSourceBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200)) {
            replLoadSource_ = java.util.Collections.unmodifiableList(replLoadSource_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.replLoadSource_ = replLoadSource_;
        } else {
          result.replLoadSource_ = replLoadSourceBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000080;
        }
        if (replLoadSinkBuilder_ == null) {
          result.replLoadSink_ = replLoadSink_;
        } else {
          result.replLoadSink_ = replLoadSinkBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000100;
        }
        result.readRequestsPerSecond_ = readRequestsPerSecond_;
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00000200;
        }
        result.writeRequestsPerSecond_ = writeRequestsPerSecond_;
        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
          to_bitField0_ |= 0x00000400;
        }
        result.readCellCountPerSecond_ = readCellCountPerSecond_;
        if (((from_bitField0_ & 0x00004000) == 0x00004000)) {
          to_bitField0_ |= 0x00000800;
        }
        result.readRawCellCountPerSecond_ = readRawCellCountPerSecond_;
        if (((from_bitField0_ & 0x00008000) == 0x00008000)) {
          to_bitField0_ |= 0x00001000;
        }
        result.scanCountPerSecond_ = scanCountPerSecond_;
        if (((from_bitField0_ & 0x00010000) == 0x00010000)) {
          to_bitField0_ |= 0x00002000;
        }
        result.scanRowsPerSecond_ = scanRowsPerSecond_;
        if (regionServerTableLatencyBuilder_ == null) {
          if (((bitField0_ & 0x00020000) == 0x00020000)) {
            regionServerTableLatency_ = java.util.Collections.unmodifiableList(regionServerTableLatency_);
            bitField0_ = (bitField0_ & ~0x00020000);
          }
          result.regionServerTableLatency_ = regionServerTableLatency_;
        } else {
          result.regionServerTableLatency_ = regionServerTableLatencyBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance()) return this;
        if (other.hasNumberOfRequests()) {
          setNumberOfRequests(other.getNumberOfRequests());
        }
        if (other.hasTotalNumberOfRequests()) {
          setTotalNumberOfRequests(other.getTotalNumberOfRequests());
        }
        if (other.hasUsedHeapMB()) {
          setUsedHeapMB(other.getUsedHeapMB());
        }
        if (other.hasMaxHeapMB()) {
          setMaxHeapMB(other.getMaxHeapMB());
        }
        if (regionLoadsBuilder_ == null) {
          if (!other.regionLoads_.isEmpty()) {
            if (regionLoads_.isEmpty()) {
              regionLoads_ = other.regionLoads_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureRegionLoadsIsMutable();
              regionLoads_.addAll(other.regionLoads_);
            }
            onChanged();
          }
        } else {
          if (!other.regionLoads_.isEmpty()) {
            if (regionLoadsBuilder_.isEmpty()) {
              regionLoadsBuilder_.dispose();
              regionLoadsBuilder_ = null;
              regionLoads_ = other.regionLoads_;
              bitField0_ = (bitField0_ & ~0x00000010);
              regionLoadsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getRegionLoadsFieldBuilder() : null;
            } else {
              regionLoadsBuilder_.addAllMessages(other.regionLoads_);
            }
          }
        }
        if (coprocessorsBuilder_ == null) {
          if (!other.coprocessors_.isEmpty()) {
            if (coprocessors_.isEmpty()) {
              coprocessors_ = other.coprocessors_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureCoprocessorsIsMutable();
              coprocessors_.addAll(other.coprocessors_);
            }
            onChanged();
          }
        } else {
          if (!other.coprocessors_.isEmpty()) {
            if (coprocessorsBuilder_.isEmpty()) {
              coprocessorsBuilder_.dispose();
              coprocessorsBuilder_ = null;
              coprocessors_ = other.coprocessors_;
              bitField0_ = (bitField0_ & ~0x00000020);
              coprocessorsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getCoprocessorsFieldBuilder() : null;
            } else {
              coprocessorsBuilder_.addAllMessages(other.coprocessors_);
            }
          }
        }
        if (other.hasReportStartTime()) {
          setReportStartTime(other.getReportStartTime());
        }
        if (other.hasReportEndTime()) {
          setReportEndTime(other.getReportEndTime());
        }
        if (other.hasInfoServerPort()) {
          setInfoServerPort(other.getInfoServerPort());
        }
        if (replLoadSourceBuilder_ == null) {
          if (!other.replLoadSource_.isEmpty()) {
            if (replLoadSource_.isEmpty()) {
              replLoadSource_ = other.replLoadSource_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureReplLoadSourceIsMutable();
              replLoadSource_.addAll(other.replLoadSource_);
            }
            onChanged();
          }
        } else {
          if (!other.replLoadSource_.isEmpty()) {
            if (replLoadSourceBuilder_.isEmpty()) {
              replLoadSourceBuilder_.dispose();
              replLoadSourceBuilder_ = null;
              replLoadSource_ = other.replLoadSource_;
              bitField0_ = (bitField0_ & ~0x00000200);
              replLoadSourceBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getReplLoadSourceFieldBuilder() : null;
            } else {
              replLoadSourceBuilder_.addAllMessages(other.replLoadSource_);
            }
          }
        }
        if (other.hasReplLoadSink()) {
          mergeReplLoadSink(other.getReplLoadSink());
        }
        if (other.hasReadRequestsPerSecond()) {
          setReadRequestsPerSecond(other.getReadRequestsPerSecond());
        }
        if (other.hasWriteRequestsPerSecond()) {
          setWriteRequestsPerSecond(other.getWriteRequestsPerSecond());
        }
        if (other.hasReadCellCountPerSecond()) {
          setReadCellCountPerSecond(other.getReadCellCountPerSecond());
        }
        if (other.hasReadRawCellCountPerSecond()) {
          setReadRawCellCountPerSecond(other.getReadRawCellCountPerSecond());
        }
        if (other.hasScanCountPerSecond()) {
          setScanCountPerSecond(other.getScanCountPerSecond());
        }
        if (other.hasScanRowsPerSecond()) {
          setScanRowsPerSecond(other.getScanRowsPerSecond());
        }
        if (regionServerTableLatencyBuilder_ == null) {
          if (!other.regionServerTableLatency_.isEmpty()) {
            if (regionServerTableLatency_.isEmpty()) {
              regionServerTableLatency_ = other.regionServerTableLatency_;
              bitField0_ = (bitField0_ & ~0x00020000);
            } else {
              ensureRegionServerTableLatencyIsMutable();
              regionServerTableLatency_.addAll(other.regionServerTableLatency_);
            }
            onChanged();
          }
        } else {
          if (!other.regionServerTableLatency_.isEmpty()) {
            if (regionServerTableLatencyBuilder_.isEmpty()) {
              regionServerTableLatencyBuilder_.dispose();
              regionServerTableLatencyBuilder_ = null;
              regionServerTableLatency_ = other.regionServerTableLatency_;
              bitField0_ = (bitField0_ & ~0x00020000);
              regionServerTableLatencyBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getRegionServerTableLatencyFieldBuilder() : null;
            } else {
              regionServerTableLatencyBuilder_.addAllMessages(other.regionServerTableLatency_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getRegionLoadsCount(); i++) {
          if (!getRegionLoads(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getCoprocessorsCount(); i++) {
          if (!getCoprocessors(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getReplLoadSourceCount(); i++) {
          if (!getReplLoadSource(i).isInitialized()) {
            
            return false;
          }
        }
        if (hasReplLoadSink()) {
          if (!getReplLoadSink().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getRegionServerTableLatencyCount(); i++) {
          if (!getRegionServerTableLatency(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional uint32 number_of_requests = 1;
      private int numberOfRequests_ ;
      /**
       * <code>optional uint32 number_of_requests = 1;</code>
       *
       * <pre>
       ** Number of requests since last report. 
       * </pre>
       */
      public boolean hasNumberOfRequests() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional uint32 number_of_requests = 1;</code>
       *
       * <pre>
       ** Number of requests since last report. 
       * </pre>
       */
      public int getNumberOfRequests() {
        return numberOfRequests_;
      }
      /**
       * <code>optional uint32 number_of_requests = 1;</code>
       *
       * <pre>
       ** Number of requests since last report. 
       * </pre>
       */
      public Builder setNumberOfRequests(int value) {
        bitField0_ |= 0x00000001;
        numberOfRequests_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 number_of_requests = 1;</code>
       *
       * <pre>
       ** Number of requests since last report. 
       * </pre>
       */
      public Builder clearNumberOfRequests() {
        bitField0_ = (bitField0_ & ~0x00000001);
        numberOfRequests_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 total_number_of_requests = 2;
      private int totalNumberOfRequests_ ;
      /**
       * <code>optional uint32 total_number_of_requests = 2;</code>
       *
       * <pre>
       ** Total Number of requests from the start of the region server. 
       * </pre>
       */
      public boolean hasTotalNumberOfRequests() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint32 total_number_of_requests = 2;</code>
       *
       * <pre>
       ** Total Number of requests from the start of the region server. 
       * </pre>
       */
      public int getTotalNumberOfRequests() {
        return totalNumberOfRequests_;
      }
      /**
       * <code>optional uint32 total_number_of_requests = 2;</code>
       *
       * <pre>
       ** Total Number of requests from the start of the region server. 
       * </pre>
       */
      public Builder setTotalNumberOfRequests(int value) {
        bitField0_ |= 0x00000002;
        totalNumberOfRequests_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 total_number_of_requests = 2;</code>
       *
       * <pre>
       ** Total Number of requests from the start of the region server. 
       * </pre>
       */
      public Builder clearTotalNumberOfRequests() {
        bitField0_ = (bitField0_ & ~0x00000002);
        totalNumberOfRequests_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 used_heap_MB = 3;
      private int usedHeapMB_ ;
      /**
       * <code>optional uint32 used_heap_MB = 3;</code>
       *
       * <pre>
       ** the amount of used heap, in MB. 
       * </pre>
       */
      public boolean hasUsedHeapMB() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint32 used_heap_MB = 3;</code>
       *
       * <pre>
       ** the amount of used heap, in MB. 
       * </pre>
       */
      public int getUsedHeapMB() {
        return usedHeapMB_;
      }
      /**
       * <code>optional uint32 used_heap_MB = 3;</code>
       *
       * <pre>
       ** the amount of used heap, in MB. 
       * </pre>
       */
      public Builder setUsedHeapMB(int value) {
        bitField0_ |= 0x00000004;
        usedHeapMB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 used_heap_MB = 3;</code>
       *
       * <pre>
       ** the amount of used heap, in MB. 
       * </pre>
       */
      public Builder clearUsedHeapMB() {
        bitField0_ = (bitField0_ & ~0x00000004);
        usedHeapMB_ = 0;
        onChanged();
        return this;
      }

      // optional uint32 max_heap_MB = 4;
      private int maxHeapMB_ ;
      /**
       * <code>optional uint32 max_heap_MB = 4;</code>
       *
       * <pre>
       ** the maximum allowable size of the heap, in MB. 
       * </pre>
       */
      public boolean hasMaxHeapMB() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint32 max_heap_MB = 4;</code>
       *
       * <pre>
       ** the maximum allowable size of the heap, in MB. 
       * </pre>
       */
      public int getMaxHeapMB() {
        return maxHeapMB_;
      }
      /**
       * <code>optional uint32 max_heap_MB = 4;</code>
       *
       * <pre>
       ** the maximum allowable size of the heap, in MB. 
       * </pre>
       */
      public Builder setMaxHeapMB(int value) {
        bitField0_ |= 0x00000008;
        maxHeapMB_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 max_heap_MB = 4;</code>
       *
       * <pre>
       ** the maximum allowable size of the heap, in MB. 
       * </pre>
       */
      public Builder clearMaxHeapMB() {
        bitField0_ = (bitField0_ & ~0x00000008);
        maxHeapMB_ = 0;
        onChanged();
        return this;
      }

      // repeated .RegionLoad region_loads = 5;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> regionLoads_ =
        java.util.Collections.emptyList();
      private void ensureRegionLoadsIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          regionLoads_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad>(regionLoads_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> regionLoadsBuilder_;

      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> getRegionLoadsList() {
        if (regionLoadsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(regionLoads_);
        } else {
          return regionLoadsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public int getRegionLoadsCount() {
        if (regionLoadsBuilder_ == null) {
          return regionLoads_.size();
        } else {
          return regionLoadsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index) {
        if (regionLoadsBuilder_ == null) {
          return regionLoads_.get(index);
        } else {
          return regionLoadsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder setRegionLoads(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
        if (regionLoadsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionLoadsIsMutable();
          regionLoads_.set(index, value);
          onChanged();
        } else {
          regionLoadsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder setRegionLoads(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
        if (regionLoadsBuilder_ == null) {
          ensureRegionLoadsIsMutable();
          regionLoads_.set(index, builderForValue.build());
          onChanged();
        } else {
          regionLoadsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder addRegionLoads(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
        if (regionLoadsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionLoadsIsMutable();
          regionLoads_.add(value);
          onChanged();
        } else {
          regionLoadsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder addRegionLoads(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
        if (regionLoadsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionLoadsIsMutable();
          regionLoads_.add(index, value);
          onChanged();
        } else {
          regionLoadsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder addRegionLoads(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
        if (regionLoadsBuilder_ == null) {
          ensureRegionLoadsIsMutable();
          regionLoads_.add(builderForValue.build());
          onChanged();
        } else {
          regionLoadsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder addRegionLoads(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
        if (regionLoadsBuilder_ == null) {
          ensureRegionLoadsIsMutable();
          regionLoads_.add(index, builderForValue.build());
          onChanged();
        } else {
          regionLoadsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder addAllRegionLoads(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> values) {
        if (regionLoadsBuilder_ == null) {
          ensureRegionLoadsIsMutable();
          super.addAll(values, regionLoads_);
          onChanged();
        } else {
          regionLoadsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder clearRegionLoads() {
        if (regionLoadsBuilder_ == null) {
          regionLoads_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          regionLoadsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public Builder removeRegionLoads(int index) {
        if (regionLoadsBuilder_ == null) {
          ensureRegionLoadsIsMutable();
          regionLoads_.remove(index);
          onChanged();
        } else {
          regionLoadsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder getRegionLoadsBuilder(
          int index) {
        return getRegionLoadsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
          int index) {
        if (regionLoadsBuilder_ == null) {
          return regionLoads_.get(index);  } else {
          return regionLoadsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
           getRegionLoadsOrBuilderList() {
        if (regionLoadsBuilder_ != null) {
          return regionLoadsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(regionLoads_);
        }
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder addRegionLoadsBuilder() {
        return getRegionLoadsFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance());
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder addRegionLoadsBuilder(
          int index) {
        return getRegionLoadsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance());
      }
      /**
       * <code>repeated .RegionLoad region_loads = 5;</code>
       *
       * <pre>
       ** Information on the load of individual regions. 
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder> 
           getRegionLoadsBuilderList() {
        return getRegionLoadsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
          getRegionLoadsFieldBuilder() {
        if (regionLoadsBuilder_ == null) {
          regionLoadsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder>(
                  regionLoads_,
                  ((bitField0_ & 0x00000010) == 0x00000010),
                  getParentForChildren(),
                  isClean());
          regionLoads_ = null;
        }
        return regionLoadsBuilder_;
      }

      // repeated .Coprocessor coprocessors = 6;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> coprocessors_ =
        java.util.Collections.emptyList();
      private void ensureCoprocessorsIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          coprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>(coprocessors_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> coprocessorsBuilder_;

      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getCoprocessorsList() {
        if (coprocessorsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(coprocessors_);
        } else {
          return coprocessorsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public int getCoprocessorsCount() {
        if (coprocessorsBuilder_ == null) {
          return coprocessors_.size();
        } else {
          return coprocessorsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index) {
        if (coprocessorsBuilder_ == null) {
          return coprocessors_.get(index);
        } else {
          return coprocessorsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder setCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
        if (coprocessorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCoprocessorsIsMutable();
          coprocessors_.set(index, value);
          onChanged();
        } else {
          coprocessorsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder setCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
        if (coprocessorsBuilder_ == null) {
          ensureCoprocessorsIsMutable();
          coprocessors_.set(index, builderForValue.build());
          onChanged();
        } else {
          coprocessorsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder addCoprocessors(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
        if (coprocessorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCoprocessorsIsMutable();
          coprocessors_.add(value);
          onChanged();
        } else {
          coprocessorsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder addCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
        if (coprocessorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCoprocessorsIsMutable();
          coprocessors_.add(index, value);
          onChanged();
        } else {
          coprocessorsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder addCoprocessors(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
        if (coprocessorsBuilder_ == null) {
          ensureCoprocessorsIsMutable();
          coprocessors_.add(builderForValue.build());
          onChanged();
        } else {
          coprocessorsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder addCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
        if (coprocessorsBuilder_ == null) {
          ensureCoprocessorsIsMutable();
          coprocessors_.add(index, builderForValue.build());
          onChanged();
        } else {
          coprocessorsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder addAllCoprocessors(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> values) {
        if (coprocessorsBuilder_ == null) {
          ensureCoprocessorsIsMutable();
          super.addAll(values, coprocessors_);
          onChanged();
        } else {
          coprocessorsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder clearCoprocessors() {
        if (coprocessorsBuilder_ == null) {
          coprocessors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          coprocessorsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public Builder removeCoprocessors(int index) {
        if (coprocessorsBuilder_ == null) {
          ensureCoprocessorsIsMutable();
          coprocessors_.remove(index);
          onChanged();
        } else {
          coprocessorsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder getCoprocessorsBuilder(
          int index) {
        return getCoprocessorsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
          int index) {
        if (coprocessorsBuilder_ == null) {
          return coprocessors_.get(index);  } else {
          return coprocessorsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
           getCoprocessorsOrBuilderList() {
        if (coprocessorsBuilder_ != null) {
          return coprocessorsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(coprocessors_);
        }
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addCoprocessorsBuilder() {
        return getCoprocessorsFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addCoprocessorsBuilder(
          int index) {
        return getCoprocessorsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
      }
      /**
       * <code>repeated .Coprocessor coprocessors = 6;</code>
       *
       * <pre>
       **
       * Regionserver-level coprocessors, e.g., WALObserver implementations.
       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
       * objects.
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder> 
           getCoprocessorsBuilderList() {
        return getCoprocessorsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
          getCoprocessorsFieldBuilder() {
        if (coprocessorsBuilder_ == null) {
          coprocessorsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder>(
                  coprocessors_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          coprocessors_ = null;
        }
        return coprocessorsBuilder_;
      }

      // optional uint64 report_start_time = 7;
      private long reportStartTime_ ;
      /**
       * <code>optional uint64 report_start_time = 7;</code>
       *
       * <pre>
       **
       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public boolean hasReportStartTime() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint64 report_start_time = 7;</code>
       *
       * <pre>
       **
       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public long getReportStartTime() {
        return reportStartTime_;
      }
      /**
       * <code>optional uint64 report_start_time = 7;</code>
       *
       * <pre>
       **
       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public Builder setReportStartTime(long value) {
        bitField0_ |= 0x00000040;
        reportStartTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 report_start_time = 7;</code>
       *
       * <pre>
       **
       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public Builder clearReportStartTime() {
        bitField0_ = (bitField0_ & ~0x00000040);
        reportStartTime_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 report_end_time = 8;
      private long reportEndTime_ ;
      /**
       * <code>optional uint64 report_end_time = 8;</code>
       *
       * <pre>
       **
       * Time when report was generated.
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public boolean hasReportEndTime() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional uint64 report_end_time = 8;</code>
       *
       * <pre>
       **
       * Time when report was generated.
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public long getReportEndTime() {
        return reportEndTime_;
      }
      /**
       * <code>optional uint64 report_end_time = 8;</code>
       *
       * <pre>
       **
       * Time when report was generated.
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public Builder setReportEndTime(long value) {
        bitField0_ |= 0x00000080;
        reportEndTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 report_end_time = 8;</code>
       *
       * <pre>
       **
       * Time when report was generated.
       * time is measured as the difference, measured in milliseconds, between the current time
       * and midnight, January 1, 1970 UTC.
       * </pre>
       */
      public Builder clearReportEndTime() {
        bitField0_ = (bitField0_ & ~0x00000080);
        reportEndTime_ = 0L;
        onChanged();
        return this;
      }

      // optional uint32 info_server_port = 9;
      private int infoServerPort_ ;
      /**
       * <code>optional uint32 info_server_port = 9;</code>
       *
       * <pre>
       **
       * The port number that this region server is hosing an info server on.
       * </pre>
       */
      public boolean hasInfoServerPort() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint32 info_server_port = 9;</code>
       *
       * <pre>
       **
       * The port number that this region server is hosing an info server on.
       * </pre>
       */
      public int getInfoServerPort() {
        return infoServerPort_;
      }
      /**
       * <code>optional uint32 info_server_port = 9;</code>
       *
       * <pre>
       **
       * The port number that this region server is hosing an info server on.
       * </pre>
       */
      public Builder setInfoServerPort(int value) {
        bitField0_ |= 0x00000100;
        infoServerPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 info_server_port = 9;</code>
       *
       * <pre>
       **
       * The port number that this region server is hosing an info server on.
       * </pre>
       */
      public Builder clearInfoServerPort() {
        bitField0_ = (bitField0_ & ~0x00000100);
        infoServerPort_ = 0;
        onChanged();
        return this;
      }

      // repeated .ReplicationLoadSource replLoadSource = 10;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> replLoadSource_ =
        java.util.Collections.emptyList();
      private void ensureReplLoadSourceIsMutable() {
        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
          replLoadSource_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource>(replLoadSource_);
          bitField0_ |= 0x00000200;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> replLoadSourceBuilder_;

      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> getReplLoadSourceList() {
        if (replLoadSourceBuilder_ == null) {
          return java.util.Collections.unmodifiableList(replLoadSource_);
        } else {
          return replLoadSourceBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public int getReplLoadSourceCount() {
        if (replLoadSourceBuilder_ == null) {
          return replLoadSource_.size();
        } else {
          return replLoadSourceBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getReplLoadSource(int index) {
        if (replLoadSourceBuilder_ == null) {
          return replLoadSource_.get(index);
        } else {
          return replLoadSourceBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder setReplLoadSource(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource value) {
        if (replLoadSourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReplLoadSourceIsMutable();
          replLoadSource_.set(index, value);
          onChanged();
        } else {
          replLoadSourceBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder setReplLoadSource(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder builderForValue) {
        if (replLoadSourceBuilder_ == null) {
          ensureReplLoadSourceIsMutable();
          replLoadSource_.set(index, builderForValue.build());
          onChanged();
        } else {
          replLoadSourceBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder addReplLoadSource(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource value) {
        if (replLoadSourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReplLoadSourceIsMutable();
          replLoadSource_.add(value);
          onChanged();
        } else {
          replLoadSourceBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder addReplLoadSource(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource value) {
        if (replLoadSourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReplLoadSourceIsMutable();
          replLoadSource_.add(index, value);
          onChanged();
        } else {
          replLoadSourceBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder addReplLoadSource(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder builderForValue) {
        if (replLoadSourceBuilder_ == null) {
          ensureReplLoadSourceIsMutable();
          replLoadSource_.add(builderForValue.build());
          onChanged();
        } else {
          replLoadSourceBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder addReplLoadSource(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder builderForValue) {
        if (replLoadSourceBuilder_ == null) {
          ensureReplLoadSourceIsMutable();
          replLoadSource_.add(index, builderForValue.build());
          onChanged();
        } else {
          replLoadSourceBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder addAllReplLoadSource(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> values) {
        if (replLoadSourceBuilder_ == null) {
          ensureReplLoadSourceIsMutable();
          super.addAll(values, replLoadSource_);
          onChanged();
        } else {
          replLoadSourceBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder clearReplLoadSource() {
        if (replLoadSourceBuilder_ == null) {
          replLoadSource_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          replLoadSourceBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public Builder removeReplLoadSource(int index) {
        if (replLoadSourceBuilder_ == null) {
          ensureReplLoadSourceIsMutable();
          replLoadSource_.remove(index);
          onChanged();
        } else {
          replLoadSourceBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder getReplLoadSourceBuilder(
          int index) {
        return getReplLoadSourceFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder getReplLoadSourceOrBuilder(
          int index) {
        if (replLoadSourceBuilder_ == null) {
          return replLoadSource_.get(index);  } else {
          return replLoadSourceBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
           getReplLoadSourceOrBuilderList() {
        if (replLoadSourceBuilder_ != null) {
          return replLoadSourceBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(replLoadSource_);
        }
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder addReplLoadSourceBuilder() {
        return getReplLoadSourceFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance());
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder addReplLoadSourceBuilder(
          int index) {
        return getReplLoadSourceFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance());
      }
      /**
       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
       *
       * <pre>
       **
       * The replicationLoadSource for the replication Source status of this region server.
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder> 
           getReplLoadSourceBuilderList() {
        return getReplLoadSourceFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
          getReplLoadSourceFieldBuilder() {
        if (replLoadSourceBuilder_ == null) {
          replLoadSourceBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder>(
                  replLoadSource_,
                  ((bitField0_ & 0x00000200) == 0x00000200),
                  getParentForChildren(),
                  isClean());
          replLoadSource_ = null;
        }
        return replLoadSourceBuilder_;
      }

      // optional .ReplicationLoadSink replLoadSink = 11;
      private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder> replLoadSinkBuilder_;
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public boolean hasReplLoadSink() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getReplLoadSink() {
        if (replLoadSinkBuilder_ == null) {
          return replLoadSink_;
        } else {
          return replLoadSinkBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public Builder setReplLoadSink(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink value) {
        if (replLoadSinkBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          replLoadSink_ = value;
          onChanged();
        } else {
          replLoadSinkBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public Builder setReplLoadSink(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder builderForValue) {
        if (replLoadSinkBuilder_ == null) {
          replLoadSink_ = builderForValue.build();
          onChanged();
        } else {
          replLoadSinkBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public Builder mergeReplLoadSink(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink value) {
        if (replLoadSinkBuilder_ == null) {
          if (((bitField0_ & 0x00000400) == 0x00000400) &&
              replLoadSink_ != org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance()) {
            replLoadSink_ =
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.newBuilder(replLoadSink_).mergeFrom(value).buildPartial();
          } else {
            replLoadSink_ = value;
          }
          onChanged();
        } else {
          replLoadSinkBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public Builder clearReplLoadSink() {
        if (replLoadSinkBuilder_ == null) {
          replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
          onChanged();
        } else {
          replLoadSinkBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder getReplLoadSinkBuilder() {
        bitField0_ |= 0x00000400;
        onChanged();
        return getReplLoadSinkFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder getReplLoadSinkOrBuilder() {
        if (replLoadSinkBuilder_ != null) {
          return replLoadSinkBuilder_.getMessageOrBuilder();
        } else {
          return replLoadSink_;
        }
      }
      /**
       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
       *
       * <pre>
       **
       * The replicationLoadSink for the replication Sink status of this region server.
       * </pre>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder> 
          getReplLoadSinkFieldBuilder() {
        if (replLoadSinkBuilder_ == null) {
          replLoadSinkBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder>(
                  replLoadSink_,
                  getParentForChildren(),
                  isClean());
          replLoadSink_ = null;
        }
        return replLoadSinkBuilder_;
      }

      // optional uint64 read_requests_per_second = 12;
      private long readRequestsPerSecond_ ;
      /**
       * <code>optional uint64 read_requests_per_second = 12;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public boolean hasReadRequestsPerSecond() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      /**
       * <code>optional uint64 read_requests_per_second = 12;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public long getReadRequestsPerSecond() {
        return readRequestsPerSecond_;
      }
      /**
       * <code>optional uint64 read_requests_per_second = 12;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public Builder setReadRequestsPerSecond(long value) {
        bitField0_ |= 0x00000800;
        readRequestsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_requests_per_second = 12;</code>
       *
       * <pre>
       ** read requests per second made to region 
       * </pre>
       */
      public Builder clearReadRequestsPerSecond() {
        bitField0_ = (bitField0_ & ~0x00000800);
        readRequestsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 write_requests_per_second = 13;
      private long writeRequestsPerSecond_ ;
      /**
       * <code>optional uint64 write_requests_per_second = 13;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public boolean hasWriteRequestsPerSecond() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      /**
       * <code>optional uint64 write_requests_per_second = 13;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public long getWriteRequestsPerSecond() {
        return writeRequestsPerSecond_;
      }
      /**
       * <code>optional uint64 write_requests_per_second = 13;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public Builder setWriteRequestsPerSecond(long value) {
        bitField0_ |= 0x00001000;
        writeRequestsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 write_requests_per_second = 13;</code>
       *
       * <pre>
       ** write requests per second made to region 
       * </pre>
       */
      public Builder clearWriteRequestsPerSecond() {
        bitField0_ = (bitField0_ & ~0x00001000);
        writeRequestsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 read_cell_count_per_second = 14;
      private long readCellCountPerSecond_ ;
      /**
       * <code>optional uint64 read_cell_count_per_second = 14;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public boolean hasReadCellCountPerSecond() {
        return ((bitField0_ & 0x00002000) == 0x00002000);
      }
      /**
       * <code>optional uint64 read_cell_count_per_second = 14;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public long getReadCellCountPerSecond() {
        return readCellCountPerSecond_;
      }
      /**
       * <code>optional uint64 read_cell_count_per_second = 14;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public Builder setReadCellCountPerSecond(long value) {
        bitField0_ |= 0x00002000;
        readCellCountPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_cell_count_per_second = 14;</code>
       *
       * <pre>
       ** cell read per second made to region 
       * </pre>
       */
      public Builder clearReadCellCountPerSecond() {
        bitField0_ = (bitField0_ & ~0x00002000);
        readCellCountPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 read_raw_cell_count_per_second = 15;
      private long readRawCellCountPerSecond_ ;
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public boolean hasReadRawCellCountPerSecond() {
        return ((bitField0_ & 0x00004000) == 0x00004000);
      }
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public long getReadRawCellCountPerSecond() {
        return readRawCellCountPerSecond_;
      }
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public Builder setReadRawCellCountPerSecond(long value) {
        bitField0_ |= 0x00004000;
        readRawCellCountPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 read_raw_cell_count_per_second = 15;</code>
       *
       * <pre>
       ** raw cell read per second made to region 
       * </pre>
       */
      public Builder clearReadRawCellCountPerSecond() {
        bitField0_ = (bitField0_ & ~0x00004000);
        readRawCellCountPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 scan_count_per_second = 16;
      private long scanCountPerSecond_ ;
      /**
       * <code>optional uint64 scan_count_per_second = 16;</code>
       */
      public boolean hasScanCountPerSecond() {
        return ((bitField0_ & 0x00008000) == 0x00008000);
      }
      /**
       * <code>optional uint64 scan_count_per_second = 16;</code>
       */
      public long getScanCountPerSecond() {
        return scanCountPerSecond_;
      }
      /**
       * <code>optional uint64 scan_count_per_second = 16;</code>
       */
      public Builder setScanCountPerSecond(long value) {
        bitField0_ |= 0x00008000;
        scanCountPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 scan_count_per_second = 16;</code>
       */
      public Builder clearScanCountPerSecond() {
        bitField0_ = (bitField0_ & ~0x00008000);
        scanCountPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 scan_rows_per_second = 17;
      private long scanRowsPerSecond_ ;
      /**
       * <code>optional uint64 scan_rows_per_second = 17;</code>
       */
      public boolean hasScanRowsPerSecond() {
        return ((bitField0_ & 0x00010000) == 0x00010000);
      }
      /**
       * <code>optional uint64 scan_rows_per_second = 17;</code>
       */
      public long getScanRowsPerSecond() {
        return scanRowsPerSecond_;
      }
      /**
       * <code>optional uint64 scan_rows_per_second = 17;</code>
       */
      public Builder setScanRowsPerSecond(long value) {
        bitField0_ |= 0x00010000;
        scanRowsPerSecond_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 scan_rows_per_second = 17;</code>
       */
      public Builder clearScanRowsPerSecond() {
        bitField0_ = (bitField0_ & ~0x00010000);
        scanRowsPerSecond_ = 0L;
        onChanged();
        return this;
      }

      // repeated .RegionServerTableLatency region_server_table_latency = 18;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency> regionServerTableLatency_ =
        java.util.Collections.emptyList();
      private void ensureRegionServerTableLatencyIsMutable() {
        if (!((bitField0_ & 0x00020000) == 0x00020000)) {
          regionServerTableLatency_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency>(regionServerTableLatency_);
          bitField0_ |= 0x00020000;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder> regionServerTableLatencyBuilder_;

      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency> getRegionServerTableLatencyList() {
        if (regionServerTableLatencyBuilder_ == null) {
          return java.util.Collections.unmodifiableList(regionServerTableLatency_);
        } else {
          return regionServerTableLatencyBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public int getRegionServerTableLatencyCount() {
        if (regionServerTableLatencyBuilder_ == null) {
          return regionServerTableLatency_.size();
        } else {
          return regionServerTableLatencyBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency getRegionServerTableLatency(int index) {
        if (regionServerTableLatencyBuilder_ == null) {
          return regionServerTableLatency_.get(index);
        } else {
          return regionServerTableLatencyBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder setRegionServerTableLatency(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency value) {
        if (regionServerTableLatencyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionServerTableLatencyIsMutable();
          regionServerTableLatency_.set(index, value);
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder setRegionServerTableLatency(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder builderForValue) {
        if (regionServerTableLatencyBuilder_ == null) {
          ensureRegionServerTableLatencyIsMutable();
          regionServerTableLatency_.set(index, builderForValue.build());
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder addRegionServerTableLatency(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency value) {
        if (regionServerTableLatencyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionServerTableLatencyIsMutable();
          regionServerTableLatency_.add(value);
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder addRegionServerTableLatency(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency value) {
        if (regionServerTableLatencyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionServerTableLatencyIsMutable();
          regionServerTableLatency_.add(index, value);
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder addRegionServerTableLatency(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder builderForValue) {
        if (regionServerTableLatencyBuilder_ == null) {
          ensureRegionServerTableLatencyIsMutable();
          regionServerTableLatency_.add(builderForValue.build());
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder addRegionServerTableLatency(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder builderForValue) {
        if (regionServerTableLatencyBuilder_ == null) {
          ensureRegionServerTableLatencyIsMutable();
          regionServerTableLatency_.add(index, builderForValue.build());
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder addAllRegionServerTableLatency(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency> values) {
        if (regionServerTableLatencyBuilder_ == null) {
          ensureRegionServerTableLatencyIsMutable();
          super.addAll(values, regionServerTableLatency_);
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder clearRegionServerTableLatency() {
        if (regionServerTableLatencyBuilder_ == null) {
          regionServerTableLatency_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00020000);
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public Builder removeRegionServerTableLatency(int index) {
        if (regionServerTableLatencyBuilder_ == null) {
          ensureRegionServerTableLatencyIsMutable();
          regionServerTableLatency_.remove(index);
          onChanged();
        } else {
          regionServerTableLatencyBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder getRegionServerTableLatencyBuilder(
          int index) {
        return getRegionServerTableLatencyFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder getRegionServerTableLatencyOrBuilder(
          int index) {
        if (regionServerTableLatencyBuilder_ == null) {
          return regionServerTableLatency_.get(index);  } else {
          return regionServerTableLatencyBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder> 
           getRegionServerTableLatencyOrBuilderList() {
        if (regionServerTableLatencyBuilder_ != null) {
          return regionServerTableLatencyBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(regionServerTableLatency_);
        }
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder addRegionServerTableLatencyBuilder() {
        return getRegionServerTableLatencyFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.getDefaultInstance());
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder addRegionServerTableLatencyBuilder(
          int index) {
        return getRegionServerTableLatencyFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.getDefaultInstance());
      }
      /**
       * <code>repeated .RegionServerTableLatency region_server_table_latency = 18;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder> 
           getRegionServerTableLatencyBuilderList() {
        return getRegionServerTableLatencyFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder> 
          getRegionServerTableLatencyFieldBuilder() {
        if (regionServerTableLatencyBuilder_ == null) {
          regionServerTableLatencyBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder>(
                  regionServerTableLatency_,
                  ((bitField0_ & 0x00020000) == 0x00020000),
                  getParentForChildren(),
                  isClean());
          regionServerTableLatency_ = null;
        }
        return regionServerTableLatencyBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:ServerLoad)
    }

    static {
      defaultInstance = new ServerLoad(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:ServerLoad)
  }

  public interface RegionServerTableLatencyOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string table_name = 1;
    /**
     * <code>required string table_name = 1;</code>
     */
    boolean hasTableName();
    /**
     * <code>required string table_name = 1;</code>
     */
    java.lang.String getTableName();
    /**
     * <code>required string table_name = 1;</code>
     */
    com.google.protobuf.ByteString
        getTableNameBytes();

    // optional uint64 get_time_mean = 2;
    /**
     * <code>optional uint64 get_time_mean = 2;</code>
     */
    boolean hasGetTimeMean();
    /**
     * <code>optional uint64 get_time_mean = 2;</code>
     */
    long getGetTimeMean();

    // optional uint64 put_time_mean = 3;
    /**
     * <code>optional uint64 put_time_mean = 3;</code>
     */
    boolean hasPutTimeMean();
    /**
     * <code>optional uint64 put_time_mean = 3;</code>
     */
    long getPutTimeMean();

    // optional uint64 scan_time_mean = 4;
    /**
     * <code>optional uint64 scan_time_mean = 4;</code>
     */
    boolean hasScanTimeMean();
    /**
     * <code>optional uint64 scan_time_mean = 4;</code>
     */
    long getScanTimeMean();

    // optional uint64 batch_time_mean = 5;
    /**
     * <code>optional uint64 batch_time_mean = 5;</code>
     */
    boolean hasBatchTimeMean();
    /**
     * <code>optional uint64 batch_time_mean = 5;</code>
     */
    long getBatchTimeMean();

    // optional uint64 append_time_mean = 6;
    /**
     * <code>optional uint64 append_time_mean = 6;</code>
     */
    boolean hasAppendTimeMean();
    /**
     * <code>optional uint64 append_time_mean = 6;</code>
     */
    long getAppendTimeMean();

    // optional uint64 delete_time_mean = 7;
    /**
     * <code>optional uint64 delete_time_mean = 7;</code>
     */
    boolean hasDeleteTimeMean();
    /**
     * <code>optional uint64 delete_time_mean = 7;</code>
     */
    long getDeleteTimeMean();

    // optional uint64 increment_time_mean = 8;
    /**
     * <code>optional uint64 increment_time_mean = 8;</code>
     */
    boolean hasIncrementTimeMean();
    /**
     * <code>optional uint64 increment_time_mean = 8;</code>
     */
    long getIncrementTimeMean();

    // optional uint64 get_operation_count = 9;
    /**
     * <code>optional uint64 get_operation_count = 9;</code>
     */
    boolean hasGetOperationCount();
    /**
     * <code>optional uint64 get_operation_count = 9;</code>
     */
    long getGetOperationCount();

    // optional uint64 put_operation_count = 10;
    /**
     * <code>optional uint64 put_operation_count = 10;</code>
     */
    boolean hasPutOperationCount();
    /**
     * <code>optional uint64 put_operation_count = 10;</code>
     */
    long getPutOperationCount();

    // optional uint64 scan_operation_count = 11;
    /**
     * <code>optional uint64 scan_operation_count = 11;</code>
     */
    boolean hasScanOperationCount();
    /**
     * <code>optional uint64 scan_operation_count = 11;</code>
     */
    long getScanOperationCount();

    // optional uint64 batch_operation_count = 12;
    /**
     * <code>optional uint64 batch_operation_count = 12;</code>
     */
    boolean hasBatchOperationCount();
    /**
     * <code>optional uint64 batch_operation_count = 12;</code>
     */
    long getBatchOperationCount();

    // optional uint64 append_operation_count = 13;
    /**
     * <code>optional uint64 append_operation_count = 13;</code>
     */
    boolean hasAppendOperationCount();
    /**
     * <code>optional uint64 append_operation_count = 13;</code>
     */
    long getAppendOperationCount();

    // optional uint64 delete_operation_count = 14;
    /**
     * <code>optional uint64 delete_operation_count = 14;</code>
     */
    boolean hasDeleteOperationCount();
    /**
     * <code>optional uint64 delete_operation_count = 14;</code>
     */
    long getDeleteOperationCount();

    // optional uint64 increment_operation_count = 15;
    /**
     * <code>optional uint64 increment_operation_count = 15;</code>
     */
    boolean hasIncrementOperationCount();
    /**
     * <code>optional uint64 increment_operation_count = 15;</code>
     */
    long getIncrementOperationCount();

    // optional uint64 get_time_99_percentile = 16;
    /**
     * <code>optional uint64 get_time_99_percentile = 16;</code>
     */
    boolean hasGetTime99Percentile();
    /**
     * <code>optional uint64 get_time_99_percentile = 16;</code>
     */
    long getGetTime99Percentile();

    // optional uint64 put_time_99_percentile = 17;
    /**
     * <code>optional uint64 put_time_99_percentile = 17;</code>
     */
    boolean hasPutTime99Percentile();
    /**
     * <code>optional uint64 put_time_99_percentile = 17;</code>
     */
    long getPutTime99Percentile();

    // optional uint64 scan_time_99_percentile = 18;
    /**
     * <code>optional uint64 scan_time_99_percentile = 18;</code>
     */
    boolean hasScanTime99Percentile();
    /**
     * <code>optional uint64 scan_time_99_percentile = 18;</code>
     */
    long getScanTime99Percentile();

    // optional uint64 batch_time_99_percentile = 19;
    /**
     * <code>optional uint64 batch_time_99_percentile = 19;</code>
     */
    boolean hasBatchTime99Percentile();
    /**
     * <code>optional uint64 batch_time_99_percentile = 19;</code>
     */
    long getBatchTime99Percentile();
  }
  /**
   * Protobuf type {@code RegionServerTableLatency}
   */
  public static final class RegionServerTableLatency extends
      com.google.protobuf.GeneratedMessage
      implements RegionServerTableLatencyOrBuilder {
    // Use RegionServerTableLatency.newBuilder() to construct.
    private RegionServerTableLatency(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegionServerTableLatency(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegionServerTableLatency defaultInstance;
    public static RegionServerTableLatency getDefaultInstance() {
      return defaultInstance;
    }

    public RegionServerTableLatency getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegionServerTableLatency(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              tableName_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              getTimeMean_ = input.readUInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              putTimeMean_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              scanTimeMean_ = input.readUInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              batchTimeMean_ = input.readUInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              appendTimeMean_ = input.readUInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              deleteTimeMean_ = input.readUInt64();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000080;
              incrementTimeMean_ = input.readUInt64();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000100;
              getOperationCount_ = input.readUInt64();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000200;
              putOperationCount_ = input.readUInt64();
              break;
            }
            case 88: {
              bitField0_ |= 0x00000400;
              scanOperationCount_ = input.readUInt64();
              break;
            }
            case 96: {
              bitField0_ |= 0x00000800;
              batchOperationCount_ = input.readUInt64();
              break;
            }
            case 104: {
              bitField0_ |= 0x00001000;
              appendOperationCount_ = input.readUInt64();
              break;
            }
            case 112: {
              bitField0_ |= 0x00002000;
              deleteOperationCount_ = input.readUInt64();
              break;
            }
            case 120: {
              bitField0_ |= 0x00004000;
              incrementOperationCount_ = input.readUInt64();
              break;
            }
            case 128: {
              bitField0_ |= 0x00008000;
              getTime99Percentile_ = input.readUInt64();
              break;
            }
            case 136: {
              bitField0_ |= 0x00010000;
              putTime99Percentile_ = input.readUInt64();
              break;
            }
            case 144: {
              bitField0_ |= 0x00020000;
              scanTime99Percentile_ = input.readUInt64();
              break;
            }
            case 152: {
              bitField0_ |= 0x00040000;
              batchTime99Percentile_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionServerTableLatency_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionServerTableLatency_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder.class);
    }

    public static com.google.protobuf.Parser<RegionServerTableLatency> PARSER =
        new com.google.protobuf.AbstractParser<RegionServerTableLatency>() {
      public RegionServerTableLatency parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegionServerTableLatency(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegionServerTableLatency> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string table_name = 1;
    public static final int TABLE_NAME_FIELD_NUMBER = 1;
    private java.lang.Object tableName_;
    /**
     * <code>required string table_name = 1;</code>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string table_name = 1;</code>
     */
    public java.lang.String getTableName() {
      java.lang.Object ref = tableName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          tableName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string table_name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getTableNameBytes() {
      java.lang.Object ref = tableName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        tableName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional uint64 get_time_mean = 2;
    public static final int GET_TIME_MEAN_FIELD_NUMBER = 2;
    private long getTimeMean_;
    /**
     * <code>optional uint64 get_time_mean = 2;</code>
     */
    public boolean hasGetTimeMean() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional uint64 get_time_mean = 2;</code>
     */
    public long getGetTimeMean() {
      return getTimeMean_;
    }

    // optional uint64 put_time_mean = 3;
    public static final int PUT_TIME_MEAN_FIELD_NUMBER = 3;
    private long putTimeMean_;
    /**
     * <code>optional uint64 put_time_mean = 3;</code>
     */
    public boolean hasPutTimeMean() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional uint64 put_time_mean = 3;</code>
     */
    public long getPutTimeMean() {
      return putTimeMean_;
    }

    // optional uint64 scan_time_mean = 4;
    public static final int SCAN_TIME_MEAN_FIELD_NUMBER = 4;
    private long scanTimeMean_;
    /**
     * <code>optional uint64 scan_time_mean = 4;</code>
     */
    public boolean hasScanTimeMean() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional uint64 scan_time_mean = 4;</code>
     */
    public long getScanTimeMean() {
      return scanTimeMean_;
    }

    // optional uint64 batch_time_mean = 5;
    public static final int BATCH_TIME_MEAN_FIELD_NUMBER = 5;
    private long batchTimeMean_;
    /**
     * <code>optional uint64 batch_time_mean = 5;</code>
     */
    public boolean hasBatchTimeMean() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional uint64 batch_time_mean = 5;</code>
     */
    public long getBatchTimeMean() {
      return batchTimeMean_;
    }

    // optional uint64 append_time_mean = 6;
    public static final int APPEND_TIME_MEAN_FIELD_NUMBER = 6;
    private long appendTimeMean_;
    /**
     * <code>optional uint64 append_time_mean = 6;</code>
     */
    public boolean hasAppendTimeMean() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional uint64 append_time_mean = 6;</code>
     */
    public long getAppendTimeMean() {
      return appendTimeMean_;
    }

    // optional uint64 delete_time_mean = 7;
    public static final int DELETE_TIME_MEAN_FIELD_NUMBER = 7;
    private long deleteTimeMean_;
    /**
     * <code>optional uint64 delete_time_mean = 7;</code>
     */
    public boolean hasDeleteTimeMean() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional uint64 delete_time_mean = 7;</code>
     */
    public long getDeleteTimeMean() {
      return deleteTimeMean_;
    }

    // optional uint64 increment_time_mean = 8;
    public static final int INCREMENT_TIME_MEAN_FIELD_NUMBER = 8;
    private long incrementTimeMean_;
    /**
     * <code>optional uint64 increment_time_mean = 8;</code>
     */
    public boolean hasIncrementTimeMean() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional uint64 increment_time_mean = 8;</code>
     */
    public long getIncrementTimeMean() {
      return incrementTimeMean_;
    }

    // optional uint64 get_operation_count = 9;
    public static final int GET_OPERATION_COUNT_FIELD_NUMBER = 9;
    private long getOperationCount_;
    /**
     * <code>optional uint64 get_operation_count = 9;</code>
     */
    public boolean hasGetOperationCount() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional uint64 get_operation_count = 9;</code>
     */
    public long getGetOperationCount() {
      return getOperationCount_;
    }

    // optional uint64 put_operation_count = 10;
    public static final int PUT_OPERATION_COUNT_FIELD_NUMBER = 10;
    private long putOperationCount_;
    /**
     * <code>optional uint64 put_operation_count = 10;</code>
     */
    public boolean hasPutOperationCount() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    /**
     * <code>optional uint64 put_operation_count = 10;</code>
     */
    public long getPutOperationCount() {
      return putOperationCount_;
    }

    // optional uint64 scan_operation_count = 11;
    public static final int SCAN_OPERATION_COUNT_FIELD_NUMBER = 11;
    private long scanOperationCount_;
    /**
     * <code>optional uint64 scan_operation_count = 11;</code>
     */
    public boolean hasScanOperationCount() {
      return ((bitField0_ & 0x00000400) == 0x00000400);
    }
    /**
     * <code>optional uint64 scan_operation_count = 11;</code>
     */
    public long getScanOperationCount() {
      return scanOperationCount_;
    }

    // optional uint64 batch_operation_count = 12;
    public static final int BATCH_OPERATION_COUNT_FIELD_NUMBER = 12;
    private long batchOperationCount_;
    /**
     * <code>optional uint64 batch_operation_count = 12;</code>
     */
    public boolean hasBatchOperationCount() {
      return ((bitField0_ & 0x00000800) == 0x00000800);
    }
    /**
     * <code>optional uint64 batch_operation_count = 12;</code>
     */
    public long getBatchOperationCount() {
      return batchOperationCount_;
    }

    // optional uint64 append_operation_count = 13;
    public static final int APPEND_OPERATION_COUNT_FIELD_NUMBER = 13;
    private long appendOperationCount_;
    /**
     * <code>optional uint64 append_operation_count = 13;</code>
     */
    public boolean hasAppendOperationCount() {
      return ((bitField0_ & 0x00001000) == 0x00001000);
    }
    /**
     * <code>optional uint64 append_operation_count = 13;</code>
     */
    public long getAppendOperationCount() {
      return appendOperationCount_;
    }

    // optional uint64 delete_operation_count = 14;
    public static final int DELETE_OPERATION_COUNT_FIELD_NUMBER = 14;
    private long deleteOperationCount_;
    /**
     * <code>optional uint64 delete_operation_count = 14;</code>
     */
    public boolean hasDeleteOperationCount() {
      return ((bitField0_ & 0x00002000) == 0x00002000);
    }
    /**
     * <code>optional uint64 delete_operation_count = 14;</code>
     */
    public long getDeleteOperationCount() {
      return deleteOperationCount_;
    }

    // optional uint64 increment_operation_count = 15;
    public static final int INCREMENT_OPERATION_COUNT_FIELD_NUMBER = 15;
    private long incrementOperationCount_;
    /**
     * <code>optional uint64 increment_operation_count = 15;</code>
     */
    public boolean hasIncrementOperationCount() {
      return ((bitField0_ & 0x00004000) == 0x00004000);
    }
    /**
     * <code>optional uint64 increment_operation_count = 15;</code>
     */
    public long getIncrementOperationCount() {
      return incrementOperationCount_;
    }

    // optional uint64 get_time_99_percentile = 16;
    public static final int GET_TIME_99_PERCENTILE_FIELD_NUMBER = 16;
    private long getTime99Percentile_;
    /**
     * <code>optional uint64 get_time_99_percentile = 16;</code>
     */
    public boolean hasGetTime99Percentile() {
      return ((bitField0_ & 0x00008000) == 0x00008000);
    }
    /**
     * <code>optional uint64 get_time_99_percentile = 16;</code>
     */
    public long getGetTime99Percentile() {
      return getTime99Percentile_;
    }

    // optional uint64 put_time_99_percentile = 17;
    public static final int PUT_TIME_99_PERCENTILE_FIELD_NUMBER = 17;
    private long putTime99Percentile_;
    /**
     * <code>optional uint64 put_time_99_percentile = 17;</code>
     */
    public boolean hasPutTime99Percentile() {
      return ((bitField0_ & 0x00010000) == 0x00010000);
    }
    /**
     * <code>optional uint64 put_time_99_percentile = 17;</code>
     */
    public long getPutTime99Percentile() {
      return putTime99Percentile_;
    }

    // optional uint64 scan_time_99_percentile = 18;
    public static final int SCAN_TIME_99_PERCENTILE_FIELD_NUMBER = 18;
    private long scanTime99Percentile_;
    /**
     * <code>optional uint64 scan_time_99_percentile = 18;</code>
     */
    public boolean hasScanTime99Percentile() {
      return ((bitField0_ & 0x00020000) == 0x00020000);
    }
    /**
     * <code>optional uint64 scan_time_99_percentile = 18;</code>
     */
    public long getScanTime99Percentile() {
      return scanTime99Percentile_;
    }

    // optional uint64 batch_time_99_percentile = 19;
    public static final int BATCH_TIME_99_PERCENTILE_FIELD_NUMBER = 19;
    private long batchTime99Percentile_;
    /**
     * <code>optional uint64 batch_time_99_percentile = 19;</code>
     */
    public boolean hasBatchTime99Percentile() {
      return ((bitField0_ & 0x00040000) == 0x00040000);
    }
    /**
     * <code>optional uint64 batch_time_99_percentile = 19;</code>
     */
    public long getBatchTime99Percentile() {
      return batchTime99Percentile_;
    }

    private void initFields() {
      tableName_ = "";
      getTimeMean_ = 0L;
      putTimeMean_ = 0L;
      scanTimeMean_ = 0L;
      batchTimeMean_ = 0L;
      appendTimeMean_ = 0L;
      deleteTimeMean_ = 0L;
      incrementTimeMean_ = 0L;
      getOperationCount_ = 0L;
      putOperationCount_ = 0L;
      scanOperationCount_ = 0L;
      batchOperationCount_ = 0L;
      appendOperationCount_ = 0L;
      deleteOperationCount_ = 0L;
      incrementOperationCount_ = 0L;
      getTime99Percentile_ = 0L;
      putTime99Percentile_ = 0L;
      scanTime99Percentile_ = 0L;
      batchTime99Percentile_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getTableNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, getTimeMean_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, putTimeMean_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, scanTimeMean_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeUInt64(5, batchTimeMean_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt64(6, appendTimeMean_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt64(7, deleteTimeMean_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeUInt64(8, incrementTimeMean_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeUInt64(9, getOperationCount_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeUInt64(10, putOperationCount_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        output.writeUInt64(11, scanOperationCount_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        output.writeUInt64(12, batchOperationCount_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        output.writeUInt64(13, appendOperationCount_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        output.writeUInt64(14, deleteOperationCount_);
      }
      if (((bitField0_ & 0x00004000) == 0x00004000)) {
        output.writeUInt64(15, incrementOperationCount_);
      }
      if (((bitField0_ & 0x00008000) == 0x00008000)) {
        output.writeUInt64(16, getTime99Percentile_);
      }
      if (((bitField0_ & 0x00010000) == 0x00010000)) {
        output.writeUInt64(17, putTime99Percentile_);
      }
      if (((bitField0_ & 0x00020000) == 0x00020000)) {
        output.writeUInt64(18, scanTime99Percentile_);
      }
      if (((bitField0_ & 0x00040000) == 0x00040000)) {
        output.writeUInt64(19, batchTime99Percentile_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getTableNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, getTimeMean_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, putTimeMean_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, scanTimeMean_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(5, batchTimeMean_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(6, appendTimeMean_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(7, deleteTimeMean_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(8, incrementTimeMean_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(9, getOperationCount_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(10, putOperationCount_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(11, scanOperationCount_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(12, batchOperationCount_);
      }
      if (((bitField0_ & 0x00001000) == 0x00001000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(13, appendOperationCount_);
      }
      if (((bitField0_ & 0x00002000) == 0x00002000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(14, deleteOperationCount_);
      }
      if (((bitField0_ & 0x00004000) == 0x00004000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(15, incrementOperationCount_);
      }
      if (((bitField0_ & 0x00008000) == 0x00008000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(16, getTime99Percentile_);
      }
      if (((bitField0_ & 0x00010000) == 0x00010000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(17, putTime99Percentile_);
      }
      if (((bitField0_ & 0x00020000) == 0x00020000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(18, scanTime99Percentile_);
      }
      if (((bitField0_ & 0x00040000) == 0x00040000)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(19, batchTime99Percentile_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency) obj;

      boolean result = true;
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasGetTimeMean() == other.hasGetTimeMean());
      if (hasGetTimeMean()) {
        result = result && (getGetTimeMean()
            == other.getGetTimeMean());
      }
      result = result && (hasPutTimeMean() == other.hasPutTimeMean());
      if (hasPutTimeMean()) {
        result = result && (getPutTimeMean()
            == other.getPutTimeMean());
      }
      result = result && (hasScanTimeMean() == other.hasScanTimeMean());
      if (hasScanTimeMean()) {
        result = result && (getScanTimeMean()
            == other.getScanTimeMean());
      }
      result = result && (hasBatchTimeMean() == other.hasBatchTimeMean());
      if (hasBatchTimeMean()) {
        result = result && (getBatchTimeMean()
            == other.getBatchTimeMean());
      }
      result = result && (hasAppendTimeMean() == other.hasAppendTimeMean());
      if (hasAppendTimeMean()) {
        result = result && (getAppendTimeMean()
            == other.getAppendTimeMean());
      }
      result = result && (hasDeleteTimeMean() == other.hasDeleteTimeMean());
      if (hasDeleteTimeMean()) {
        result = result && (getDeleteTimeMean()
            == other.getDeleteTimeMean());
      }
      result = result && (hasIncrementTimeMean() == other.hasIncrementTimeMean());
      if (hasIncrementTimeMean()) {
        result = result && (getIncrementTimeMean()
            == other.getIncrementTimeMean());
      }
      result = result && (hasGetOperationCount() == other.hasGetOperationCount());
      if (hasGetOperationCount()) {
        result = result && (getGetOperationCount()
            == other.getGetOperationCount());
      }
      result = result && (hasPutOperationCount() == other.hasPutOperationCount());
      if (hasPutOperationCount()) {
        result = result && (getPutOperationCount()
            == other.getPutOperationCount());
      }
      result = result && (hasScanOperationCount() == other.hasScanOperationCount());
      if (hasScanOperationCount()) {
        result = result && (getScanOperationCount()
            == other.getScanOperationCount());
      }
      result = result && (hasBatchOperationCount() == other.hasBatchOperationCount());
      if (hasBatchOperationCount()) {
        result = result && (getBatchOperationCount()
            == other.getBatchOperationCount());
      }
      result = result && (hasAppendOperationCount() == other.hasAppendOperationCount());
      if (hasAppendOperationCount()) {
        result = result && (getAppendOperationCount()
            == other.getAppendOperationCount());
      }
      result = result && (hasDeleteOperationCount() == other.hasDeleteOperationCount());
      if (hasDeleteOperationCount()) {
        result = result && (getDeleteOperationCount()
            == other.getDeleteOperationCount());
      }
      result = result && (hasIncrementOperationCount() == other.hasIncrementOperationCount());
      if (hasIncrementOperationCount()) {
        result = result && (getIncrementOperationCount()
            == other.getIncrementOperationCount());
      }
      result = result && (hasGetTime99Percentile() == other.hasGetTime99Percentile());
      if (hasGetTime99Percentile()) {
        result = result && (getGetTime99Percentile()
            == other.getGetTime99Percentile());
      }
      result = result && (hasPutTime99Percentile() == other.hasPutTime99Percentile());
      if (hasPutTime99Percentile()) {
        result = result && (getPutTime99Percentile()
            == other.getPutTime99Percentile());
      }
      result = result && (hasScanTime99Percentile() == other.hasScanTime99Percentile());
      if (hasScanTime99Percentile()) {
        result = result && (getScanTime99Percentile()
            == other.getScanTime99Percentile());
      }
      result = result && (hasBatchTime99Percentile() == other.hasBatchTime99Percentile());
      if (hasBatchTime99Percentile()) {
        result = result && (getBatchTime99Percentile()
            == other.getBatchTime99Percentile());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasGetTimeMean()) {
        hash = (37 * hash) + GET_TIME_MEAN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getGetTimeMean());
      }
      if (hasPutTimeMean()) {
        hash = (37 * hash) + PUT_TIME_MEAN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getPutTimeMean());
      }
      if (hasScanTimeMean()) {
        hash = (37 * hash) + SCAN_TIME_MEAN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getScanTimeMean());
      }
      if (hasBatchTimeMean()) {
        hash = (37 * hash) + BATCH_TIME_MEAN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getBatchTimeMean());
      }
      if (hasAppendTimeMean()) {
        hash = (37 * hash) + APPEND_TIME_MEAN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getAppendTimeMean());
      }
      if (hasDeleteTimeMean()) {
        hash = (37 * hash) + DELETE_TIME_MEAN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDeleteTimeMean());
      }
      if (hasIncrementTimeMean()) {
        hash = (37 * hash) + INCREMENT_TIME_MEAN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getIncrementTimeMean());
      }
      if (hasGetOperationCount()) {
        hash = (37 * hash) + GET_OPERATION_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getGetOperationCount());
      }
      if (hasPutOperationCount()) {
        hash = (37 * hash) + PUT_OPERATION_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getPutOperationCount());
      }
      if (hasScanOperationCount()) {
        hash = (37 * hash) + SCAN_OPERATION_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getScanOperationCount());
      }
      if (hasBatchOperationCount()) {
        hash = (37 * hash) + BATCH_OPERATION_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getBatchOperationCount());
      }
      if (hasAppendOperationCount()) {
        hash = (37 * hash) + APPEND_OPERATION_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getAppendOperationCount());
      }
      if (hasDeleteOperationCount()) {
        hash = (37 * hash) + DELETE_OPERATION_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDeleteOperationCount());
      }
      if (hasIncrementOperationCount()) {
        hash = (37 * hash) + INCREMENT_OPERATION_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getIncrementOperationCount());
      }
      if (hasGetTime99Percentile()) {
        hash = (37 * hash) + GET_TIME_99_PERCENTILE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getGetTime99Percentile());
      }
      if (hasPutTime99Percentile()) {
        hash = (37 * hash) + PUT_TIME_99_PERCENTILE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getPutTime99Percentile());
      }
      if (hasScanTime99Percentile()) {
        hash = (37 * hash) + SCAN_TIME_99_PERCENTILE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getScanTime99Percentile());
      }
      if (hasBatchTime99Percentile()) {
        hash = (37 * hash) + BATCH_TIME_99_PERCENTILE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getBatchTime99Percentile());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code RegionServerTableLatency}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatencyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionServerTableLatency_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionServerTableLatency_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        tableName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        getTimeMean_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        putTimeMean_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        scanTimeMean_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        batchTimeMean_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        appendTimeMean_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        deleteTimeMean_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        incrementTimeMean_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        getOperationCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000100);
        putOperationCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        scanOperationCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000400);
        batchOperationCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000800);
        appendOperationCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00001000);
        deleteOperationCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00002000);
        incrementOperationCount_ = 0L;
        bitField0_ = (bitField0_ & ~0x00004000);
        getTime99Percentile_ = 0L;
        bitField0_ = (bitField0_ & ~0x00008000);
        putTime99Percentile_ = 0L;
        bitField0_ = (bitField0_ & ~0x00010000);
        scanTime99Percentile_ = 0L;
        bitField0_ = (bitField0_ & ~0x00020000);
        batchTime99Percentile_ = 0L;
        bitField0_ = (bitField0_ & ~0x00040000);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_RegionServerTableLatency_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.tableName_ = tableName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.getTimeMean_ = getTimeMean_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.putTimeMean_ = putTimeMean_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.scanTimeMean_ = scanTimeMean_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.batchTimeMean_ = batchTimeMean_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.appendTimeMean_ = appendTimeMean_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.deleteTimeMean_ = deleteTimeMean_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        result.incrementTimeMean_ = incrementTimeMean_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000100;
        }
        result.getOperationCount_ = getOperationCount_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000200;
        }
        result.putOperationCount_ = putOperationCount_;
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000400;
        }
        result.scanOperationCount_ = scanOperationCount_;
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000800;
        }
        result.batchOperationCount_ = batchOperationCount_;
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00001000;
        }
        result.appendOperationCount_ = appendOperationCount_;
        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
          to_bitField0_ |= 0x00002000;
        }
        result.deleteOperationCount_ = deleteOperationCount_;
        if (((from_bitField0_ & 0x00004000) == 0x00004000)) {
          to_bitField0_ |= 0x00004000;
        }
        result.incrementOperationCount_ = incrementOperationCount_;
        if (((from_bitField0_ & 0x00008000) == 0x00008000)) {
          to_bitField0_ |= 0x00008000;
        }
        result.getTime99Percentile_ = getTime99Percentile_;
        if (((from_bitField0_ & 0x00010000) == 0x00010000)) {
          to_bitField0_ |= 0x00010000;
        }
        result.putTime99Percentile_ = putTime99Percentile_;
        if (((from_bitField0_ & 0x00020000) == 0x00020000)) {
          to_bitField0_ |= 0x00020000;
        }
        result.scanTime99Percentile_ = scanTime99Percentile_;
        if (((from_bitField0_ & 0x00040000) == 0x00040000)) {
          to_bitField0_ |= 0x00040000;
        }
        result.batchTime99Percentile_ = batchTime99Percentile_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency.getDefaultInstance()) return this;
        if (other.hasTableName()) {
          bitField0_ |= 0x00000001;
          tableName_ = other.tableName_;
          onChanged();
        }
        if (other.hasGetTimeMean()) {
          setGetTimeMean(other.getGetTimeMean());
        }
        if (other.hasPutTimeMean()) {
          setPutTimeMean(other.getPutTimeMean());
        }
        if (other.hasScanTimeMean()) {
          setScanTimeMean(other.getScanTimeMean());
        }
        if (other.hasBatchTimeMean()) {
          setBatchTimeMean(other.getBatchTimeMean());
        }
        if (other.hasAppendTimeMean()) {
          setAppendTimeMean(other.getAppendTimeMean());
        }
        if (other.hasDeleteTimeMean()) {
          setDeleteTimeMean(other.getDeleteTimeMean());
        }
        if (other.hasIncrementTimeMean()) {
          setIncrementTimeMean(other.getIncrementTimeMean());
        }
        if (other.hasGetOperationCount()) {
          setGetOperationCount(other.getGetOperationCount());
        }
        if (other.hasPutOperationCount()) {
          setPutOperationCount(other.getPutOperationCount());
        }
        if (other.hasScanOperationCount()) {
          setScanOperationCount(other.getScanOperationCount());
        }
        if (other.hasBatchOperationCount()) {
          setBatchOperationCount(other.getBatchOperationCount());
        }
        if (other.hasAppendOperationCount()) {
          setAppendOperationCount(other.getAppendOperationCount());
        }
        if (other.hasDeleteOperationCount()) {
          setDeleteOperationCount(other.getDeleteOperationCount());
        }
        if (other.hasIncrementOperationCount()) {
          setIncrementOperationCount(other.getIncrementOperationCount());
        }
        if (other.hasGetTime99Percentile()) {
          setGetTime99Percentile(other.getGetTime99Percentile());
        }
        if (other.hasPutTime99Percentile()) {
          setPutTime99Percentile(other.getPutTime99Percentile());
        }
        if (other.hasScanTime99Percentile()) {
          setScanTime99Percentile(other.getScanTime99Percentile());
        }
        if (other.hasBatchTime99Percentile()) {
          setBatchTime99Percentile(other.getBatchTime99Percentile());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasTableName()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionServerTableLatency) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string table_name = 1;
      private java.lang.Object tableName_ = "";
      /**
       * <code>required string table_name = 1;</code>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string table_name = 1;</code>
       */
      public java.lang.String getTableName() {
        java.lang.Object ref = tableName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          tableName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string table_name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getTableNameBytes() {
        java.lang.Object ref = tableName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          tableName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string table_name = 1;</code>
       */
      public Builder setTableName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string table_name = 1;</code>
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }
      /**
       * <code>required string table_name = 1;</code>
       */
      public Builder setTableNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }

      // optional uint64 get_time_mean = 2;
      private long getTimeMean_ ;
      /**
       * <code>optional uint64 get_time_mean = 2;</code>
       */
      public boolean hasGetTimeMean() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional uint64 get_time_mean = 2;</code>
       */
      public long getGetTimeMean() {
        return getTimeMean_;
      }
      /**
       * <code>optional uint64 get_time_mean = 2;</code>
       */
      public Builder setGetTimeMean(long value) {
        bitField0_ |= 0x00000002;
        getTimeMean_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 get_time_mean = 2;</code>
       */
      public Builder clearGetTimeMean() {
        bitField0_ = (bitField0_ & ~0x00000002);
        getTimeMean_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 put_time_mean = 3;
      private long putTimeMean_ ;
      /**
       * <code>optional uint64 put_time_mean = 3;</code>
       */
      public boolean hasPutTimeMean() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional uint64 put_time_mean = 3;</code>
       */
      public long getPutTimeMean() {
        return putTimeMean_;
      }
      /**
       * <code>optional uint64 put_time_mean = 3;</code>
       */
      public Builder setPutTimeMean(long value) {
        bitField0_ |= 0x00000004;
        putTimeMean_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 put_time_mean = 3;</code>
       */
      public Builder clearPutTimeMean() {
        bitField0_ = (bitField0_ & ~0x00000004);
        putTimeMean_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 scan_time_mean = 4;
      private long scanTimeMean_ ;
      /**
       * <code>optional uint64 scan_time_mean = 4;</code>
       */
      public boolean hasScanTimeMean() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional uint64 scan_time_mean = 4;</code>
       */
      public long getScanTimeMean() {
        return scanTimeMean_;
      }
      /**
       * <code>optional uint64 scan_time_mean = 4;</code>
       */
      public Builder setScanTimeMean(long value) {
        bitField0_ |= 0x00000008;
        scanTimeMean_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 scan_time_mean = 4;</code>
       */
      public Builder clearScanTimeMean() {
        bitField0_ = (bitField0_ & ~0x00000008);
        scanTimeMean_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 batch_time_mean = 5;
      private long batchTimeMean_ ;
      /**
       * <code>optional uint64 batch_time_mean = 5;</code>
       */
      public boolean hasBatchTimeMean() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional uint64 batch_time_mean = 5;</code>
       */
      public long getBatchTimeMean() {
        return batchTimeMean_;
      }
      /**
       * <code>optional uint64 batch_time_mean = 5;</code>
       */
      public Builder setBatchTimeMean(long value) {
        bitField0_ |= 0x00000010;
        batchTimeMean_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 batch_time_mean = 5;</code>
       */
      public Builder clearBatchTimeMean() {
        bitField0_ = (bitField0_ & ~0x00000010);
        batchTimeMean_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 append_time_mean = 6;
      private long appendTimeMean_ ;
      /**
       * <code>optional uint64 append_time_mean = 6;</code>
       */
      public boolean hasAppendTimeMean() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional uint64 append_time_mean = 6;</code>
       */
      public long getAppendTimeMean() {
        return appendTimeMean_;
      }
      /**
       * <code>optional uint64 append_time_mean = 6;</code>
       */
      public Builder setAppendTimeMean(long value) {
        bitField0_ |= 0x00000020;
        appendTimeMean_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 append_time_mean = 6;</code>
       */
      public Builder clearAppendTimeMean() {
        bitField0_ = (bitField0_ & ~0x00000020);
        appendTimeMean_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 delete_time_mean = 7;
      private long deleteTimeMean_ ;
      /**
       * <code>optional uint64 delete_time_mean = 7;</code>
       */
      public boolean hasDeleteTimeMean() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint64 delete_time_mean = 7;</code>
       */
      public long getDeleteTimeMean() {
        return deleteTimeMean_;
      }
      /**
       * <code>optional uint64 delete_time_mean = 7;</code>
       */
      public Builder setDeleteTimeMean(long value) {
        bitField0_ |= 0x00000040;
        deleteTimeMean_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 delete_time_mean = 7;</code>
       */
      public Builder clearDeleteTimeMean() {
        bitField0_ = (bitField0_ & ~0x00000040);
        deleteTimeMean_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 increment_time_mean = 8;
      private long incrementTimeMean_ ;
      /**
       * <code>optional uint64 increment_time_mean = 8;</code>
       */
      public boolean hasIncrementTimeMean() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional uint64 increment_time_mean = 8;</code>
       */
      public long getIncrementTimeMean() {
        return incrementTimeMean_;
      }
      /**
       * <code>optional uint64 increment_time_mean = 8;</code>
       */
      public Builder setIncrementTimeMean(long value) {
        bitField0_ |= 0x00000080;
        incrementTimeMean_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 increment_time_mean = 8;</code>
       */
      public Builder clearIncrementTimeMean() {
        bitField0_ = (bitField0_ & ~0x00000080);
        incrementTimeMean_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 get_operation_count = 9;
      private long getOperationCount_ ;
      /**
       * <code>optional uint64 get_operation_count = 9;</code>
       */
      public boolean hasGetOperationCount() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint64 get_operation_count = 9;</code>
       */
      public long getGetOperationCount() {
        return getOperationCount_;
      }
      /**
       * <code>optional uint64 get_operation_count = 9;</code>
       */
      public Builder setGetOperationCount(long value) {
        bitField0_ |= 0x00000100;
        getOperationCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 get_operation_count = 9;</code>
       */
      public Builder clearGetOperationCount() {
        bitField0_ = (bitField0_ & ~0x00000100);
        getOperationCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 put_operation_count = 10;
      private long putOperationCount_ ;
      /**
       * <code>optional uint64 put_operation_count = 10;</code>
       */
      public boolean hasPutOperationCount() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional uint64 put_operation_count = 10;</code>
       */
      public long getPutOperationCount() {
        return putOperationCount_;
      }
      /**
       * <code>optional uint64 put_operation_count = 10;</code>
       */
      public Builder setPutOperationCount(long value) {
        bitField0_ |= 0x00000200;
        putOperationCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 put_operation_count = 10;</code>
       */
      public Builder clearPutOperationCount() {
        bitField0_ = (bitField0_ & ~0x00000200);
        putOperationCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 scan_operation_count = 11;
      private long scanOperationCount_ ;
      /**
       * <code>optional uint64 scan_operation_count = 11;</code>
       */
      public boolean hasScanOperationCount() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional uint64 scan_operation_count = 11;</code>
       */
      public long getScanOperationCount() {
        return scanOperationCount_;
      }
      /**
       * <code>optional uint64 scan_operation_count = 11;</code>
       */
      public Builder setScanOperationCount(long value) {
        bitField0_ |= 0x00000400;
        scanOperationCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 scan_operation_count = 11;</code>
       */
      public Builder clearScanOperationCount() {
        bitField0_ = (bitField0_ & ~0x00000400);
        scanOperationCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 batch_operation_count = 12;
      private long batchOperationCount_ ;
      /**
       * <code>optional uint64 batch_operation_count = 12;</code>
       */
      public boolean hasBatchOperationCount() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      /**
       * <code>optional uint64 batch_operation_count = 12;</code>
       */
      public long getBatchOperationCount() {
        return batchOperationCount_;
      }
      /**
       * <code>optional uint64 batch_operation_count = 12;</code>
       */
      public Builder setBatchOperationCount(long value) {
        bitField0_ |= 0x00000800;
        batchOperationCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 batch_operation_count = 12;</code>
       */
      public Builder clearBatchOperationCount() {
        bitField0_ = (bitField0_ & ~0x00000800);
        batchOperationCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 append_operation_count = 13;
      private long appendOperationCount_ ;
      /**
       * <code>optional uint64 append_operation_count = 13;</code>
       */
      public boolean hasAppendOperationCount() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      /**
       * <code>optional uint64 append_operation_count = 13;</code>
       */
      public long getAppendOperationCount() {
        return appendOperationCount_;
      }
      /**
       * <code>optional uint64 append_operation_count = 13;</code>
       */
      public Builder setAppendOperationCount(long value) {
        bitField0_ |= 0x00001000;
        appendOperationCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 append_operation_count = 13;</code>
       */
      public Builder clearAppendOperationCount() {
        bitField0_ = (bitField0_ & ~0x00001000);
        appendOperationCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 delete_operation_count = 14;
      private long deleteOperationCount_ ;
      /**
       * <code>optional uint64 delete_operation_count = 14;</code>
       */
      public boolean hasDeleteOperationCount() {
        return ((bitField0_ & 0x00002000) == 0x00002000);
      }
      /**
       * <code>optional uint64 delete_operation_count = 14;</code>
       */
      public long getDeleteOperationCount() {
        return deleteOperationCount_;
      }
      /**
       * <code>optional uint64 delete_operation_count = 14;</code>
       */
      public Builder setDeleteOperationCount(long value) {
        bitField0_ |= 0x00002000;
        deleteOperationCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 delete_operation_count = 14;</code>
       */
      public Builder clearDeleteOperationCount() {
        bitField0_ = (bitField0_ & ~0x00002000);
        deleteOperationCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 increment_operation_count = 15;
      private long incrementOperationCount_ ;
      /**
       * <code>optional uint64 increment_operation_count = 15;</code>
       */
      public boolean hasIncrementOperationCount() {
        return ((bitField0_ & 0x00004000) == 0x00004000);
      }
      /**
       * <code>optional uint64 increment_operation_count = 15;</code>
       */
      public long getIncrementOperationCount() {
        return incrementOperationCount_;
      }
      /**
       * <code>optional uint64 increment_operation_count = 15;</code>
       */
      public Builder setIncrementOperationCount(long value) {
        bitField0_ |= 0x00004000;
        incrementOperationCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 increment_operation_count = 15;</code>
       */
      public Builder clearIncrementOperationCount() {
        bitField0_ = (bitField0_ & ~0x00004000);
        incrementOperationCount_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 get_time_99_percentile = 16;
      private long getTime99Percentile_ ;
      /**
       * <code>optional uint64 get_time_99_percentile = 16;</code>
       */
      public boolean hasGetTime99Percentile() {
        return ((bitField0_ & 0x00008000) == 0x00008000);
      }
      /**
       * <code>optional uint64 get_time_99_percentile = 16;</code>
       */
      public long getGetTime99Percentile() {
        return getTime99Percentile_;
      }
      /**
       * <code>optional uint64 get_time_99_percentile = 16;</code>
       */
      public Builder setGetTime99Percentile(long value) {
        bitField0_ |= 0x00008000;
        getTime99Percentile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 get_time_99_percentile = 16;</code>
       */
      public Builder clearGetTime99Percentile() {
        bitField0_ = (bitField0_ & ~0x00008000);
        getTime99Percentile_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 put_time_99_percentile = 17;
      private long putTime99Percentile_ ;
      /**
       * <code>optional uint64 put_time_99_percentile = 17;</code>
       */
      public boolean hasPutTime99Percentile() {
        return ((bitField0_ & 0x00010000) == 0x00010000);
      }
      /**
       * <code>optional uint64 put_time_99_percentile = 17;</code>
       */
      public long getPutTime99Percentile() {
        return putTime99Percentile_;
      }
      /**
       * <code>optional uint64 put_time_99_percentile = 17;</code>
       */
      public Builder setPutTime99Percentile(long value) {
        bitField0_ |= 0x00010000;
        putTime99Percentile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 put_time_99_percentile = 17;</code>
       */
      public Builder clearPutTime99Percentile() {
        bitField0_ = (bitField0_ & ~0x00010000);
        putTime99Percentile_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 scan_time_99_percentile = 18;
      private long scanTime99Percentile_ ;
      /**
       * <code>optional uint64 scan_time_99_percentile = 18;</code>
       */
      public boolean hasScanTime99Percentile() {
        return ((bitField0_ & 0x00020000) == 0x00020000);
      }
      /**
       * <code>optional uint64 scan_time_99_percentile = 18;</code>
       */
      public long getScanTime99Percentile() {
        return scanTime99Percentile_;
      }
      /**
       * <code>optional uint64 scan_time_99_percentile = 18;</code>
       */
      public Builder setScanTime99Percentile(long value) {
        bitField0_ |= 0x00020000;
        scanTime99Percentile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 scan_time_99_percentile = 18;</code>
       */
      public Builder clearScanTime99Percentile() {
        bitField0_ = (bitField0_ & ~0x00020000);
        scanTime99Percentile_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 batch_time_99_percentile = 19;
      private long batchTime99Percentile_ ;
      /**
       * <code>optional uint64 batch_time_99_percentile = 19;</code>
       */
      public boolean hasBatchTime99Percentile() {
        return ((bitField0_ & 0x00040000) == 0x00040000);
      }
      /**
       * <code>optional uint64 batch_time_99_percentile = 19;</code>
       */
      public long getBatchTime99Percentile() {
        return batchTime99Percentile_;
      }
      /**
       * <code>optional uint64 batch_time_99_percentile = 19;</code>
       */
      public Builder setBatchTime99Percentile(long value) {
        bitField0_ |= 0x00040000;
        batchTime99Percentile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 batch_time_99_percentile = 19;</code>
       */
      public Builder clearBatchTime99Percentile() {
        bitField0_ = (bitField0_ & ~0x00040000);
        batchTime99Percentile_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:RegionServerTableLatency)
    }

    static {
      defaultInstance = new RegionServerTableLatency(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:RegionServerTableLatency)
  }

  public interface LiveServerInfoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .ServerName server = 1;
    /**
     * <code>required .ServerName server = 1;</code>
     */
    boolean hasServer();
    /**
     * <code>required .ServerName server = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServer();
    /**
     * <code>required .ServerName server = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServerOrBuilder();

    // required .ServerLoad server_load = 2;
    /**
     * <code>required .ServerLoad server_load = 2;</code>
     */
    boolean hasServerLoad();
    /**
     * <code>required .ServerLoad server_load = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad getServerLoad();
    /**
     * <code>required .ServerLoad server_load = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder getServerLoadOrBuilder();
  }
  /**
   * Protobuf type {@code LiveServerInfo}
   */
  public static final class LiveServerInfo extends
      com.google.protobuf.GeneratedMessage
      implements LiveServerInfoOrBuilder {
    // Use LiveServerInfo.newBuilder() to construct.
    private LiveServerInfo(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private LiveServerInfo(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final LiveServerInfo defaultInstance;
    public static LiveServerInfo getDefaultInstance() {
      return defaultInstance;
    }

    public LiveServerInfo getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private LiveServerInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = server_.toBuilder();
              }
              server_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(server_);
                server_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = serverLoad_.toBuilder();
              }
              serverLoad_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(serverLoad_);
                serverLoad_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_LiveServerInfo_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_LiveServerInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder.class);
    }

    public static com.google.protobuf.Parser<LiveServerInfo> PARSER =
        new com.google.protobuf.AbstractParser<LiveServerInfo>() {
      public LiveServerInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new LiveServerInfo(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<LiveServerInfo> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .ServerName server = 1;
    public static final int SERVER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName server_;
    /**
     * <code>required .ServerName server = 1;</code>
     */
    public boolean hasServer() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .ServerName server = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServer() {
      return server_;
    }
    /**
     * <code>required .ServerName server = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServerOrBuilder() {
      return server_;
    }

    // required .ServerLoad server_load = 2;
    public static final int SERVER_LOAD_FIELD_NUMBER = 2;
    private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad serverLoad_;
    /**
     * <code>required .ServerLoad server_load = 2;</code>
     */
    public boolean hasServerLoad() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .ServerLoad server_load = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad getServerLoad() {
      return serverLoad_;
    }
    /**
     * <code>required .ServerLoad server_load = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder getServerLoadOrBuilder() {
      return serverLoad_;
    }

    private void initFields() {
      server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
      serverLoad_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasServer()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasServerLoad()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getServer().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getServerLoad().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, server_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, serverLoad_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, server_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, serverLoad_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo) obj;

      boolean result = true;
      result = result && (hasServer() == other.hasServer());
      if (hasServer()) {
        result = result && getServer()
            .equals(other.getServer());
      }
      result = result && (hasServerLoad() == other.hasServerLoad());
      if (hasServerLoad()) {
        result = result && getServerLoad()
            .equals(other.getServerLoad());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasServer()) {
        hash = (37 * hash) + SERVER_FIELD_NUMBER;
        hash = (53 * hash) + getServer().hashCode();
      }
      if (hasServerLoad()) {
        hash = (37 * hash) + SERVER_LOAD_FIELD_NUMBER;
        hash = (53 * hash) + getServerLoad().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code LiveServerInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_LiveServerInfo_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_LiveServerInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getServerFieldBuilder();
          getServerLoadFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (serverBuilder_ == null) {
          server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
        } else {
          serverBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (serverLoadBuilder_ == null) {
          serverLoad_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance();
        } else {
          serverLoadBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_LiveServerInfo_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (serverBuilder_ == null) {
          result.server_ = server_;
        } else {
          result.server_ = serverBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (serverLoadBuilder_ == null) {
          result.serverLoad_ = serverLoad_;
        } else {
          result.serverLoad_ = serverLoadBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.getDefaultInstance()) return this;
        if (other.hasServer()) {
          mergeServer(other.getServer());
        }
        if (other.hasServerLoad()) {
          mergeServerLoad(other.getServerLoad());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasServer()) {
          
          return false;
        }
        if (!hasServerLoad()) {
          
          return false;
        }
        if (!getServer().isInitialized()) {
          
          return false;
        }
        if (!getServerLoad().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .ServerName server = 1;
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> serverBuilder_;
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public boolean hasServer() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServer() {
        if (serverBuilder_ == null) {
          return server_;
        } else {
          return serverBuilder_.getMessage();
        }
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public Builder setServer(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (serverBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          server_ = value;
          onChanged();
        } else {
          serverBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public Builder setServer(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (serverBuilder_ == null) {
          server_ = builderForValue.build();
          onChanged();
        } else {
          serverBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public Builder mergeServer(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (serverBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              server_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance()) {
            server_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.newBuilder(server_).mergeFrom(value).buildPartial();
          } else {
            server_ = value;
          }
          onChanged();
        } else {
          serverBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public Builder clearServer() {
        if (serverBuilder_ == null) {
          server_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
          onChanged();
        } else {
          serverBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder getServerBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getServerFieldBuilder().getBuilder();
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServerOrBuilder() {
        if (serverBuilder_ != null) {
          return serverBuilder_.getMessageOrBuilder();
        } else {
          return server_;
        }
      }
      /**
       * <code>required .ServerName server = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
          getServerFieldBuilder() {
        if (serverBuilder_ == null) {
          serverBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder>(
                  server_,
                  getParentForChildren(),
                  isClean());
          server_ = null;
        }
        return serverBuilder_;
      }

      // required .ServerLoad server_load = 2;
      private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad serverLoad_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder> serverLoadBuilder_;
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public boolean hasServerLoad() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad getServerLoad() {
        if (serverLoadBuilder_ == null) {
          return serverLoad_;
        } else {
          return serverLoadBuilder_.getMessage();
        }
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public Builder setServerLoad(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad value) {
        if (serverLoadBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          serverLoad_ = value;
          onChanged();
        } else {
          serverLoadBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public Builder setServerLoad(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder builderForValue) {
        if (serverLoadBuilder_ == null) {
          serverLoad_ = builderForValue.build();
          onChanged();
        } else {
          serverLoadBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public Builder mergeServerLoad(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad value) {
        if (serverLoadBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              serverLoad_ != org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance()) {
            serverLoad_ =
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.newBuilder(serverLoad_).mergeFrom(value).buildPartial();
          } else {
            serverLoad_ = value;
          }
          onChanged();
        } else {
          serverLoadBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public Builder clearServerLoad() {
        if (serverLoadBuilder_ == null) {
          serverLoad_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance();
          onChanged();
        } else {
          serverLoadBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder getServerLoadBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getServerLoadFieldBuilder().getBuilder();
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder getServerLoadOrBuilder() {
        if (serverLoadBuilder_ != null) {
          return serverLoadBuilder_.getMessageOrBuilder();
        } else {
          return serverLoad_;
        }
      }
      /**
       * <code>required .ServerLoad server_load = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder> 
          getServerLoadFieldBuilder() {
        if (serverLoadBuilder_ == null) {
          serverLoadBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder>(
                  serverLoad_,
                  getParentForChildren(),
                  isClean());
          serverLoad_ = null;
        }
        return serverLoadBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:LiveServerInfo)
    }

    static {
      defaultInstance = new LiveServerInfo(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:LiveServerInfo)
  }

  public interface ClusterStatusOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .HBaseVersionFileContent hbase_version = 1;
    /**
     * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
     */
    boolean hasHbaseVersion();
    /**
     * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent getHbaseVersion();
    /**
     * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContentOrBuilder getHbaseVersionOrBuilder();

    // repeated .LiveServerInfo live_servers = 2;
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo> 
        getLiveServersList();
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo getLiveServers(int index);
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    int getLiveServersCount();
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder> 
        getLiveServersOrBuilderList();
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder getLiveServersOrBuilder(
        int index);

    // repeated .ServerName dead_servers = 3;
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> 
        getDeadServersList();
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getDeadServers(int index);
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    int getDeadServersCount();
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
        getDeadServersOrBuilderList();
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getDeadServersOrBuilder(
        int index);

    // repeated .RegionInTransition regions_in_transition = 4;
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition> 
        getRegionsInTransitionList();
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition getRegionsInTransition(int index);
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    int getRegionsInTransitionCount();
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder> 
        getRegionsInTransitionOrBuilderList();
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder getRegionsInTransitionOrBuilder(
        int index);

    // optional .ClusterId cluster_id = 5;
    /**
     * <code>optional .ClusterId cluster_id = 5;</code>
     */
    boolean hasClusterId();
    /**
     * <code>optional .ClusterId cluster_id = 5;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId getClusterId();
    /**
     * <code>optional .ClusterId cluster_id = 5;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterIdOrBuilder getClusterIdOrBuilder();

    // repeated .Coprocessor master_coprocessors = 6;
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> 
        getMasterCoprocessorsList();
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getMasterCoprocessors(int index);
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    int getMasterCoprocessorsCount();
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
        getMasterCoprocessorsOrBuilderList();
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getMasterCoprocessorsOrBuilder(
        int index);

    // optional .ServerName master = 7;
    /**
     * <code>optional .ServerName master = 7;</code>
     */
    boolean hasMaster();
    /**
     * <code>optional .ServerName master = 7;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getMaster();
    /**
     * <code>optional .ServerName master = 7;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getMasterOrBuilder();

    // repeated .ServerName backup_masters = 8;
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> 
        getBackupMastersList();
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getBackupMasters(int index);
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    int getBackupMastersCount();
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
        getBackupMastersOrBuilderList();
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getBackupMastersOrBuilder(
        int index);

    // optional bool balancer_on = 9;
    /**
     * <code>optional bool balancer_on = 9;</code>
     */
    boolean hasBalancerOn();
    /**
     * <code>optional bool balancer_on = 9;</code>
     */
    boolean getBalancerOn();

    // repeated .ServerName servers_name = 10;
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> 
        getServersNameList();
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServersName(int index);
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    int getServersNameCount();
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
        getServersNameOrBuilderList();
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServersNameOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code ClusterStatus}
   */
  public static final class ClusterStatus extends
      com.google.protobuf.GeneratedMessage
      implements ClusterStatusOrBuilder {
    // Use ClusterStatus.newBuilder() to construct.
    private ClusterStatus(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ClusterStatus(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ClusterStatus defaultInstance;
    public static ClusterStatus getDefaultInstance() {
      return defaultInstance;
    }

    public ClusterStatus getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ClusterStatus(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = hbaseVersion_.toBuilder();
              }
              hbaseVersion_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(hbaseVersion_);
                hbaseVersion_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                liveServers_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo>();
                mutable_bitField0_ |= 0x00000002;
              }
              liveServers_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.PARSER, extensionRegistry));
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                deadServers_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>();
                mutable_bitField0_ |= 0x00000004;
              }
              deadServers_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.PARSER, extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                regionsInTransition_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition>();
                mutable_bitField0_ |= 0x00000008;
              }
              regionsInTransition_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.PARSER, extensionRegistry));
              break;
            }
            case 42: {
              org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = clusterId_.toBuilder();
              }
              clusterId_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(clusterId_);
                clusterId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                masterCoprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>();
                mutable_bitField0_ |= 0x00000020;
              }
              masterCoprocessors_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.PARSER, extensionRegistry));
              break;
            }
            case 58: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) == 0x00000004)) {
                subBuilder = master_.toBuilder();
              }
              master_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(master_);
                master_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 66: {
              if (!((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
                backupMasters_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>();
                mutable_bitField0_ |= 0x00000080;
              }
              backupMasters_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.PARSER, extensionRegistry));
              break;
            }
            case 72: {
              bitField0_ |= 0x00000008;
              balancerOn_ = input.readBool();
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
                serversName_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>();
                mutable_bitField0_ |= 0x00000200;
              }
              serversName_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          liveServers_ = java.util.Collections.unmodifiableList(liveServers_);
        }
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          deadServers_ = java.util.Collections.unmodifiableList(deadServers_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          regionsInTransition_ = java.util.Collections.unmodifiableList(regionsInTransition_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          masterCoprocessors_ = java.util.Collections.unmodifiableList(masterCoprocessors_);
        }
        if (((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
          backupMasters_ = java.util.Collections.unmodifiableList(backupMasters_);
        }
        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
          serversName_ = java.util.Collections.unmodifiableList(serversName_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ClusterStatus_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ClusterStatus_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus.Builder.class);
    }

    public static com.google.protobuf.Parser<ClusterStatus> PARSER =
        new com.google.protobuf.AbstractParser<ClusterStatus>() {
      public ClusterStatus parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ClusterStatus(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ClusterStatus> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .HBaseVersionFileContent hbase_version = 1;
    public static final int HBASE_VERSION_FIELD_NUMBER = 1;
    private org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent hbaseVersion_;
    /**
     * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
     */
    public boolean hasHbaseVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent getHbaseVersion() {
      return hbaseVersion_;
    }
    /**
     * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContentOrBuilder getHbaseVersionOrBuilder() {
      return hbaseVersion_;
    }

    // repeated .LiveServerInfo live_servers = 2;
    public static final int LIVE_SERVERS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo> liveServers_;
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo> getLiveServersList() {
      return liveServers_;
    }
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder> 
        getLiveServersOrBuilderList() {
      return liveServers_;
    }
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    public int getLiveServersCount() {
      return liveServers_.size();
    }
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo getLiveServers(int index) {
      return liveServers_.get(index);
    }
    /**
     * <code>repeated .LiveServerInfo live_servers = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder getLiveServersOrBuilder(
        int index) {
      return liveServers_.get(index);
    }

    // repeated .ServerName dead_servers = 3;
    public static final int DEAD_SERVERS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> deadServers_;
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getDeadServersList() {
      return deadServers_;
    }
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
        getDeadServersOrBuilderList() {
      return deadServers_;
    }
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    public int getDeadServersCount() {
      return deadServers_.size();
    }
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getDeadServers(int index) {
      return deadServers_.get(index);
    }
    /**
     * <code>repeated .ServerName dead_servers = 3;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getDeadServersOrBuilder(
        int index) {
      return deadServers_.get(index);
    }

    // repeated .RegionInTransition regions_in_transition = 4;
    public static final int REGIONS_IN_TRANSITION_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition> regionsInTransition_;
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition> getRegionsInTransitionList() {
      return regionsInTransition_;
    }
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder> 
        getRegionsInTransitionOrBuilderList() {
      return regionsInTransition_;
    }
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    public int getRegionsInTransitionCount() {
      return regionsInTransition_.size();
    }
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition getRegionsInTransition(int index) {
      return regionsInTransition_.get(index);
    }
    /**
     * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder getRegionsInTransitionOrBuilder(
        int index) {
      return regionsInTransition_.get(index);
    }

    // optional .ClusterId cluster_id = 5;
    public static final int CLUSTER_ID_FIELD_NUMBER = 5;
    private org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId clusterId_;
    /**
     * <code>optional .ClusterId cluster_id = 5;</code>
     */
    public boolean hasClusterId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .ClusterId cluster_id = 5;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId getClusterId() {
      return clusterId_;
    }
    /**
     * <code>optional .ClusterId cluster_id = 5;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterIdOrBuilder getClusterIdOrBuilder() {
      return clusterId_;
    }

    // repeated .Coprocessor master_coprocessors = 6;
    public static final int MASTER_COPROCESSORS_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> masterCoprocessors_;
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getMasterCoprocessorsList() {
      return masterCoprocessors_;
    }
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
        getMasterCoprocessorsOrBuilderList() {
      return masterCoprocessors_;
    }
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    public int getMasterCoprocessorsCount() {
      return masterCoprocessors_.size();
    }
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getMasterCoprocessors(int index) {
      return masterCoprocessors_.get(index);
    }
    /**
     * <code>repeated .Coprocessor master_coprocessors = 6;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getMasterCoprocessorsOrBuilder(
        int index) {
      return masterCoprocessors_.get(index);
    }

    // optional .ServerName master = 7;
    public static final int MASTER_FIELD_NUMBER = 7;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName master_;
    /**
     * <code>optional .ServerName master = 7;</code>
     */
    public boolean hasMaster() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional .ServerName master = 7;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getMaster() {
      return master_;
    }
    /**
     * <code>optional .ServerName master = 7;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getMasterOrBuilder() {
      return master_;
    }

    // repeated .ServerName backup_masters = 8;
    public static final int BACKUP_MASTERS_FIELD_NUMBER = 8;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> backupMasters_;
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getBackupMastersList() {
      return backupMasters_;
    }
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
        getBackupMastersOrBuilderList() {
      return backupMasters_;
    }
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    public int getBackupMastersCount() {
      return backupMasters_.size();
    }
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getBackupMasters(int index) {
      return backupMasters_.get(index);
    }
    /**
     * <code>repeated .ServerName backup_masters = 8;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getBackupMastersOrBuilder(
        int index) {
      return backupMasters_.get(index);
    }

    // optional bool balancer_on = 9;
    public static final int BALANCER_ON_FIELD_NUMBER = 9;
    private boolean balancerOn_;
    /**
     * <code>optional bool balancer_on = 9;</code>
     */
    public boolean hasBalancerOn() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional bool balancer_on = 9;</code>
     */
    public boolean getBalancerOn() {
      return balancerOn_;
    }

    // repeated .ServerName servers_name = 10;
    public static final int SERVERS_NAME_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> serversName_;
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getServersNameList() {
      return serversName_;
    }
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
        getServersNameOrBuilderList() {
      return serversName_;
    }
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    public int getServersNameCount() {
      return serversName_.size();
    }
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServersName(int index) {
      return serversName_.get(index);
    }
    /**
     * <code>repeated .ServerName servers_name = 10;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServersNameOrBuilder(
        int index) {
      return serversName_.get(index);
    }

    private void initFields() {
      hbaseVersion_ = org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.getDefaultInstance();
      liveServers_ = java.util.Collections.emptyList();
      deadServers_ = java.util.Collections.emptyList();
      regionsInTransition_ = java.util.Collections.emptyList();
      clusterId_ = org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.getDefaultInstance();
      masterCoprocessors_ = java.util.Collections.emptyList();
      master_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
      backupMasters_ = java.util.Collections.emptyList();
      balancerOn_ = false;
      serversName_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasHbaseVersion()) {
        if (!getHbaseVersion().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getLiveServersCount(); i++) {
        if (!getLiveServers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getDeadServersCount(); i++) {
        if (!getDeadServers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getRegionsInTransitionCount(); i++) {
        if (!getRegionsInTransition(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasClusterId()) {
        if (!getClusterId().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getMasterCoprocessorsCount(); i++) {
        if (!getMasterCoprocessors(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasMaster()) {
        if (!getMaster().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getBackupMastersCount(); i++) {
        if (!getBackupMasters(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getServersNameCount(); i++) {
        if (!getServersName(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, hbaseVersion_);
      }
      for (int i = 0; i < liveServers_.size(); i++) {
        output.writeMessage(2, liveServers_.get(i));
      }
      for (int i = 0; i < deadServers_.size(); i++) {
        output.writeMessage(3, deadServers_.get(i));
      }
      for (int i = 0; i < regionsInTransition_.size(); i++) {
        output.writeMessage(4, regionsInTransition_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(5, clusterId_);
      }
      for (int i = 0; i < masterCoprocessors_.size(); i++) {
        output.writeMessage(6, masterCoprocessors_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeMessage(7, master_);
      }
      for (int i = 0; i < backupMasters_.size(); i++) {
        output.writeMessage(8, backupMasters_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBool(9, balancerOn_);
      }
      for (int i = 0; i < serversName_.size(); i++) {
        output.writeMessage(10, serversName_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, hbaseVersion_);
      }
      for (int i = 0; i < liveServers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, liveServers_.get(i));
      }
      for (int i = 0; i < deadServers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, deadServers_.get(i));
      }
      for (int i = 0; i < regionsInTransition_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, regionsInTransition_.get(i));
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, clusterId_);
      }
      for (int i = 0; i < masterCoprocessors_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, masterCoprocessors_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, master_);
      }
      for (int i = 0; i < backupMasters_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, backupMasters_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(9, balancerOn_);
      }
      for (int i = 0; i < serversName_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, serversName_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus) obj;

      boolean result = true;
      result = result && (hasHbaseVersion() == other.hasHbaseVersion());
      if (hasHbaseVersion()) {
        result = result && getHbaseVersion()
            .equals(other.getHbaseVersion());
      }
      result = result && getLiveServersList()
          .equals(other.getLiveServersList());
      result = result && getDeadServersList()
          .equals(other.getDeadServersList());
      result = result && getRegionsInTransitionList()
          .equals(other.getRegionsInTransitionList());
      result = result && (hasClusterId() == other.hasClusterId());
      if (hasClusterId()) {
        result = result && getClusterId()
            .equals(other.getClusterId());
      }
      result = result && getMasterCoprocessorsList()
          .equals(other.getMasterCoprocessorsList());
      result = result && (hasMaster() == other.hasMaster());
      if (hasMaster()) {
        result = result && getMaster()
            .equals(other.getMaster());
      }
      result = result && getBackupMastersList()
          .equals(other.getBackupMastersList());
      result = result && (hasBalancerOn() == other.hasBalancerOn());
      if (hasBalancerOn()) {
        result = result && (getBalancerOn()
            == other.getBalancerOn());
      }
      result = result && getServersNameList()
          .equals(other.getServersNameList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasHbaseVersion()) {
        hash = (37 * hash) + HBASE_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getHbaseVersion().hashCode();
      }
      if (getLiveServersCount() > 0) {
        hash = (37 * hash) + LIVE_SERVERS_FIELD_NUMBER;
        hash = (53 * hash) + getLiveServersList().hashCode();
      }
      if (getDeadServersCount() > 0) {
        hash = (37 * hash) + DEAD_SERVERS_FIELD_NUMBER;
        hash = (53 * hash) + getDeadServersList().hashCode();
      }
      if (getRegionsInTransitionCount() > 0) {
        hash = (37 * hash) + REGIONS_IN_TRANSITION_FIELD_NUMBER;
        hash = (53 * hash) + getRegionsInTransitionList().hashCode();
      }
      if (hasClusterId()) {
        hash = (37 * hash) + CLUSTER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getClusterId().hashCode();
      }
      if (getMasterCoprocessorsCount() > 0) {
        hash = (37 * hash) + MASTER_COPROCESSORS_FIELD_NUMBER;
        hash = (53 * hash) + getMasterCoprocessorsList().hashCode();
      }
      if (hasMaster()) {
        hash = (37 * hash) + MASTER_FIELD_NUMBER;
        hash = (53 * hash) + getMaster().hashCode();
      }
      if (getBackupMastersCount() > 0) {
        hash = (37 * hash) + BACKUP_MASTERS_FIELD_NUMBER;
        hash = (53 * hash) + getBackupMastersList().hashCode();
      }
      if (hasBalancerOn()) {
        hash = (37 * hash) + BALANCER_ON_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getBalancerOn());
      }
      if (getServersNameCount() > 0) {
        hash = (37 * hash) + SERVERS_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getServersNameList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code ClusterStatus}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatusOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ClusterStatus_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ClusterStatus_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getHbaseVersionFieldBuilder();
          getLiveServersFieldBuilder();
          getDeadServersFieldBuilder();
          getRegionsInTransitionFieldBuilder();
          getClusterIdFieldBuilder();
          getMasterCoprocessorsFieldBuilder();
          getMasterFieldBuilder();
          getBackupMastersFieldBuilder();
          getServersNameFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (hbaseVersionBuilder_ == null) {
          hbaseVersion_ = org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.getDefaultInstance();
        } else {
          hbaseVersionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (liveServersBuilder_ == null) {
          liveServers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          liveServersBuilder_.clear();
        }
        if (deadServersBuilder_ == null) {
          deadServers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          deadServersBuilder_.clear();
        }
        if (regionsInTransitionBuilder_ == null) {
          regionsInTransition_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          regionsInTransitionBuilder_.clear();
        }
        if (clusterIdBuilder_ == null) {
          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.getDefaultInstance();
        } else {
          clusterIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        if (masterCoprocessorsBuilder_ == null) {
          masterCoprocessors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          masterCoprocessorsBuilder_.clear();
        }
        if (masterBuilder_ == null) {
          master_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
        } else {
          masterBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        if (backupMastersBuilder_ == null) {
          backupMasters_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
        } else {
          backupMastersBuilder_.clear();
        }
        balancerOn_ = false;
        bitField0_ = (bitField0_ & ~0x00000100);
        if (serversNameBuilder_ == null) {
          serversName_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          serversNameBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ClusterStatus_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus build() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (hbaseVersionBuilder_ == null) {
          result.hbaseVersion_ = hbaseVersion_;
        } else {
          result.hbaseVersion_ = hbaseVersionBuilder_.build();
        }
        if (liveServersBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            liveServers_ = java.util.Collections.unmodifiableList(liveServers_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.liveServers_ = liveServers_;
        } else {
          result.liveServers_ = liveServersBuilder_.build();
        }
        if (deadServersBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            deadServers_ = java.util.Collections.unmodifiableList(deadServers_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.deadServers_ = deadServers_;
        } else {
          result.deadServers_ = deadServersBuilder_.build();
        }
        if (regionsInTransitionBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008)) {
            regionsInTransition_ = java.util.Collections.unmodifiableList(regionsInTransition_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.regionsInTransition_ = regionsInTransition_;
        } else {
          result.regionsInTransition_ = regionsInTransitionBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000002;
        }
        if (clusterIdBuilder_ == null) {
          result.clusterId_ = clusterId_;
        } else {
          result.clusterId_ = clusterIdBuilder_.build();
        }
        if (masterCoprocessorsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            masterCoprocessors_ = java.util.Collections.unmodifiableList(masterCoprocessors_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.masterCoprocessors_ = masterCoprocessors_;
        } else {
          result.masterCoprocessors_ = masterCoprocessorsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000004;
        }
        if (masterBuilder_ == null) {
          result.master_ = master_;
        } else {
          result.master_ = masterBuilder_.build();
        }
        if (backupMastersBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080)) {
            backupMasters_ = java.util.Collections.unmodifiableList(backupMasters_);
            bitField0_ = (bitField0_ & ~0x00000080);
          }
          result.backupMasters_ = backupMasters_;
        } else {
          result.backupMasters_ = backupMastersBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000008;
        }
        result.balancerOn_ = balancerOn_;
        if (serversNameBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200)) {
            serversName_ = java.util.Collections.unmodifiableList(serversName_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.serversName_ = serversName_;
        } else {
          result.serversName_ = serversNameBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus.getDefaultInstance()) return this;
        if (other.hasHbaseVersion()) {
          mergeHbaseVersion(other.getHbaseVersion());
        }
        if (liveServersBuilder_ == null) {
          if (!other.liveServers_.isEmpty()) {
            if (liveServers_.isEmpty()) {
              liveServers_ = other.liveServers_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureLiveServersIsMutable();
              liveServers_.addAll(other.liveServers_);
            }
            onChanged();
          }
        } else {
          if (!other.liveServers_.isEmpty()) {
            if (liveServersBuilder_.isEmpty()) {
              liveServersBuilder_.dispose();
              liveServersBuilder_ = null;
              liveServers_ = other.liveServers_;
              bitField0_ = (bitField0_ & ~0x00000002);
              liveServersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getLiveServersFieldBuilder() : null;
            } else {
              liveServersBuilder_.addAllMessages(other.liveServers_);
            }
          }
        }
        if (deadServersBuilder_ == null) {
          if (!other.deadServers_.isEmpty()) {
            if (deadServers_.isEmpty()) {
              deadServers_ = other.deadServers_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureDeadServersIsMutable();
              deadServers_.addAll(other.deadServers_);
            }
            onChanged();
          }
        } else {
          if (!other.deadServers_.isEmpty()) {
            if (deadServersBuilder_.isEmpty()) {
              deadServersBuilder_.dispose();
              deadServersBuilder_ = null;
              deadServers_ = other.deadServers_;
              bitField0_ = (bitField0_ & ~0x00000004);
              deadServersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getDeadServersFieldBuilder() : null;
            } else {
              deadServersBuilder_.addAllMessages(other.deadServers_);
            }
          }
        }
        if (regionsInTransitionBuilder_ == null) {
          if (!other.regionsInTransition_.isEmpty()) {
            if (regionsInTransition_.isEmpty()) {
              regionsInTransition_ = other.regionsInTransition_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureRegionsInTransitionIsMutable();
              regionsInTransition_.addAll(other.regionsInTransition_);
            }
            onChanged();
          }
        } else {
          if (!other.regionsInTransition_.isEmpty()) {
            if (regionsInTransitionBuilder_.isEmpty()) {
              regionsInTransitionBuilder_.dispose();
              regionsInTransitionBuilder_ = null;
              regionsInTransition_ = other.regionsInTransition_;
              bitField0_ = (bitField0_ & ~0x00000008);
              regionsInTransitionBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getRegionsInTransitionFieldBuilder() : null;
            } else {
              regionsInTransitionBuilder_.addAllMessages(other.regionsInTransition_);
            }
          }
        }
        if (other.hasClusterId()) {
          mergeClusterId(other.getClusterId());
        }
        if (masterCoprocessorsBuilder_ == null) {
          if (!other.masterCoprocessors_.isEmpty()) {
            if (masterCoprocessors_.isEmpty()) {
              masterCoprocessors_ = other.masterCoprocessors_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureMasterCoprocessorsIsMutable();
              masterCoprocessors_.addAll(other.masterCoprocessors_);
            }
            onChanged();
          }
        } else {
          if (!other.masterCoprocessors_.isEmpty()) {
            if (masterCoprocessorsBuilder_.isEmpty()) {
              masterCoprocessorsBuilder_.dispose();
              masterCoprocessorsBuilder_ = null;
              masterCoprocessors_ = other.masterCoprocessors_;
              bitField0_ = (bitField0_ & ~0x00000020);
              masterCoprocessorsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getMasterCoprocessorsFieldBuilder() : null;
            } else {
              masterCoprocessorsBuilder_.addAllMessages(other.masterCoprocessors_);
            }
          }
        }
        if (other.hasMaster()) {
          mergeMaster(other.getMaster());
        }
        if (backupMastersBuilder_ == null) {
          if (!other.backupMasters_.isEmpty()) {
            if (backupMasters_.isEmpty()) {
              backupMasters_ = other.backupMasters_;
              bitField0_ = (bitField0_ & ~0x00000080);
            } else {
              ensureBackupMastersIsMutable();
              backupMasters_.addAll(other.backupMasters_);
            }
            onChanged();
          }
        } else {
          if (!other.backupMasters_.isEmpty()) {
            if (backupMastersBuilder_.isEmpty()) {
              backupMastersBuilder_.dispose();
              backupMastersBuilder_ = null;
              backupMasters_ = other.backupMasters_;
              bitField0_ = (bitField0_ & ~0x00000080);
              backupMastersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getBackupMastersFieldBuilder() : null;
            } else {
              backupMastersBuilder_.addAllMessages(other.backupMasters_);
            }
          }
        }
        if (other.hasBalancerOn()) {
          setBalancerOn(other.getBalancerOn());
        }
        if (serversNameBuilder_ == null) {
          if (!other.serversName_.isEmpty()) {
            if (serversName_.isEmpty()) {
              serversName_ = other.serversName_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureServersNameIsMutable();
              serversName_.addAll(other.serversName_);
            }
            onChanged();
          }
        } else {
          if (!other.serversName_.isEmpty()) {
            if (serversNameBuilder_.isEmpty()) {
              serversNameBuilder_.dispose();
              serversNameBuilder_ = null;
              serversName_ = other.serversName_;
              bitField0_ = (bitField0_ & ~0x00000200);
              serversNameBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getServersNameFieldBuilder() : null;
            } else {
              serversNameBuilder_.addAllMessages(other.serversName_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasHbaseVersion()) {
          if (!getHbaseVersion().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getLiveServersCount(); i++) {
          if (!getLiveServers(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getDeadServersCount(); i++) {
          if (!getDeadServers(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getRegionsInTransitionCount(); i++) {
          if (!getRegionsInTransition(i).isInitialized()) {
            
            return false;
          }
        }
        if (hasClusterId()) {
          if (!getClusterId().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getMasterCoprocessorsCount(); i++) {
          if (!getMasterCoprocessors(i).isInitialized()) {
            
            return false;
          }
        }
        if (hasMaster()) {
          if (!getMaster().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getBackupMastersCount(); i++) {
          if (!getBackupMasters(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getServersNameCount(); i++) {
          if (!getServersName(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ClusterStatus) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .HBaseVersionFileContent hbase_version = 1;
      private org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent hbaseVersion_ = org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent, org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.Builder, org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContentOrBuilder> hbaseVersionBuilder_;
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public boolean hasHbaseVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent getHbaseVersion() {
        if (hbaseVersionBuilder_ == null) {
          return hbaseVersion_;
        } else {
          return hbaseVersionBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public Builder setHbaseVersion(org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent value) {
        if (hbaseVersionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          hbaseVersion_ = value;
          onChanged();
        } else {
          hbaseVersionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public Builder setHbaseVersion(
          org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.Builder builderForValue) {
        if (hbaseVersionBuilder_ == null) {
          hbaseVersion_ = builderForValue.build();
          onChanged();
        } else {
          hbaseVersionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public Builder mergeHbaseVersion(org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent value) {
        if (hbaseVersionBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              hbaseVersion_ != org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.getDefaultInstance()) {
            hbaseVersion_ =
              org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.newBuilder(hbaseVersion_).mergeFrom(value).buildPartial();
          } else {
            hbaseVersion_ = value;
          }
          onChanged();
        } else {
          hbaseVersionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public Builder clearHbaseVersion() {
        if (hbaseVersionBuilder_ == null) {
          hbaseVersion_ = org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.getDefaultInstance();
          onChanged();
        } else {
          hbaseVersionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.Builder getHbaseVersionBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getHbaseVersionFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContentOrBuilder getHbaseVersionOrBuilder() {
        if (hbaseVersionBuilder_ != null) {
          return hbaseVersionBuilder_.getMessageOrBuilder();
        } else {
          return hbaseVersion_;
        }
      }
      /**
       * <code>optional .HBaseVersionFileContent hbase_version = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent, org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.Builder, org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContentOrBuilder> 
          getHbaseVersionFieldBuilder() {
        if (hbaseVersionBuilder_ == null) {
          hbaseVersionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent, org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContent.Builder, org.apache.hadoop.hbase.protobuf.generated.FSProtos.HBaseVersionFileContentOrBuilder>(
                  hbaseVersion_,
                  getParentForChildren(),
                  isClean());
          hbaseVersion_ = null;
        }
        return hbaseVersionBuilder_;
      }

      // repeated .LiveServerInfo live_servers = 2;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo> liveServers_ =
        java.util.Collections.emptyList();
      private void ensureLiveServersIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          liveServers_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo>(liveServers_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder> liveServersBuilder_;

      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo> getLiveServersList() {
        if (liveServersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(liveServers_);
        } else {
          return liveServersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public int getLiveServersCount() {
        if (liveServersBuilder_ == null) {
          return liveServers_.size();
        } else {
          return liveServersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo getLiveServers(int index) {
        if (liveServersBuilder_ == null) {
          return liveServers_.get(index);
        } else {
          return liveServersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder setLiveServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo value) {
        if (liveServersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLiveServersIsMutable();
          liveServers_.set(index, value);
          onChanged();
        } else {
          liveServersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder setLiveServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder builderForValue) {
        if (liveServersBuilder_ == null) {
          ensureLiveServersIsMutable();
          liveServers_.set(index, builderForValue.build());
          onChanged();
        } else {
          liveServersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder addLiveServers(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo value) {
        if (liveServersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLiveServersIsMutable();
          liveServers_.add(value);
          onChanged();
        } else {
          liveServersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder addLiveServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo value) {
        if (liveServersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLiveServersIsMutable();
          liveServers_.add(index, value);
          onChanged();
        } else {
          liveServersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder addLiveServers(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder builderForValue) {
        if (liveServersBuilder_ == null) {
          ensureLiveServersIsMutable();
          liveServers_.add(builderForValue.build());
          onChanged();
        } else {
          liveServersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder addLiveServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder builderForValue) {
        if (liveServersBuilder_ == null) {
          ensureLiveServersIsMutable();
          liveServers_.add(index, builderForValue.build());
          onChanged();
        } else {
          liveServersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder addAllLiveServers(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo> values) {
        if (liveServersBuilder_ == null) {
          ensureLiveServersIsMutable();
          super.addAll(values, liveServers_);
          onChanged();
        } else {
          liveServersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder clearLiveServers() {
        if (liveServersBuilder_ == null) {
          liveServers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          liveServersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public Builder removeLiveServers(int index) {
        if (liveServersBuilder_ == null) {
          ensureLiveServersIsMutable();
          liveServers_.remove(index);
          onChanged();
        } else {
          liveServersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder getLiveServersBuilder(
          int index) {
        return getLiveServersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder getLiveServersOrBuilder(
          int index) {
        if (liveServersBuilder_ == null) {
          return liveServers_.get(index);  } else {
          return liveServersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder> 
           getLiveServersOrBuilderList() {
        if (liveServersBuilder_ != null) {
          return liveServersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(liveServers_);
        }
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder addLiveServersBuilder() {
        return getLiveServersFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder addLiveServersBuilder(
          int index) {
        return getLiveServersFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.getDefaultInstance());
      }
      /**
       * <code>repeated .LiveServerInfo live_servers = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder> 
           getLiveServersBuilderList() {
        return getLiveServersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder> 
          getLiveServersFieldBuilder() {
        if (liveServersBuilder_ == null) {
          liveServersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.LiveServerInfoOrBuilder>(
                  liveServers_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          liveServers_ = null;
        }
        return liveServersBuilder_;
      }

      // repeated .ServerName dead_servers = 3;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> deadServers_ =
        java.util.Collections.emptyList();
      private void ensureDeadServersIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          deadServers_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>(deadServers_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> deadServersBuilder_;

      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getDeadServersList() {
        if (deadServersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(deadServers_);
        } else {
          return deadServersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public int getDeadServersCount() {
        if (deadServersBuilder_ == null) {
          return deadServers_.size();
        } else {
          return deadServersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getDeadServers(int index) {
        if (deadServersBuilder_ == null) {
          return deadServers_.get(index);
        } else {
          return deadServersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder setDeadServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (deadServersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDeadServersIsMutable();
          deadServers_.set(index, value);
          onChanged();
        } else {
          deadServersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder setDeadServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (deadServersBuilder_ == null) {
          ensureDeadServersIsMutable();
          deadServers_.set(index, builderForValue.build());
          onChanged();
        } else {
          deadServersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder addDeadServers(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (deadServersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDeadServersIsMutable();
          deadServers_.add(value);
          onChanged();
        } else {
          deadServersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder addDeadServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (deadServersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDeadServersIsMutable();
          deadServers_.add(index, value);
          onChanged();
        } else {
          deadServersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder addDeadServers(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (deadServersBuilder_ == null) {
          ensureDeadServersIsMutable();
          deadServers_.add(builderForValue.build());
          onChanged();
        } else {
          deadServersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder addDeadServers(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (deadServersBuilder_ == null) {
          ensureDeadServersIsMutable();
          deadServers_.add(index, builderForValue.build());
          onChanged();
        } else {
          deadServersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder addAllDeadServers(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> values) {
        if (deadServersBuilder_ == null) {
          ensureDeadServersIsMutable();
          super.addAll(values, deadServers_);
          onChanged();
        } else {
          deadServersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder clearDeadServers() {
        if (deadServersBuilder_ == null) {
          deadServers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          deadServersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public Builder removeDeadServers(int index) {
        if (deadServersBuilder_ == null) {
          ensureDeadServersIsMutable();
          deadServers_.remove(index);
          onChanged();
        } else {
          deadServersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder getDeadServersBuilder(
          int index) {
        return getDeadServersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getDeadServersOrBuilder(
          int index) {
        if (deadServersBuilder_ == null) {
          return deadServers_.get(index);  } else {
          return deadServersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
           getDeadServersOrBuilderList() {
        if (deadServersBuilder_ != null) {
          return deadServersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(deadServers_);
        }
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addDeadServersBuilder() {
        return getDeadServersFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addDeadServersBuilder(
          int index) {
        return getDeadServersFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
      }
      /**
       * <code>repeated .ServerName dead_servers = 3;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder> 
           getDeadServersBuilderList() {
        return getDeadServersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
          getDeadServersFieldBuilder() {
        if (deadServersBuilder_ == null) {
          deadServersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder>(
                  deadServers_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          deadServers_ = null;
        }
        return deadServersBuilder_;
      }

      // repeated .RegionInTransition regions_in_transition = 4;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition> regionsInTransition_ =
        java.util.Collections.emptyList();
      private void ensureRegionsInTransitionIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          regionsInTransition_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition>(regionsInTransition_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder> regionsInTransitionBuilder_;

      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition> getRegionsInTransitionList() {
        if (regionsInTransitionBuilder_ == null) {
          return java.util.Collections.unmodifiableList(regionsInTransition_);
        } else {
          return regionsInTransitionBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public int getRegionsInTransitionCount() {
        if (regionsInTransitionBuilder_ == null) {
          return regionsInTransition_.size();
        } else {
          return regionsInTransitionBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition getRegionsInTransition(int index) {
        if (regionsInTransitionBuilder_ == null) {
          return regionsInTransition_.get(index);
        } else {
          return regionsInTransitionBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder setRegionsInTransition(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition value) {
        if (regionsInTransitionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionsInTransitionIsMutable();
          regionsInTransition_.set(index, value);
          onChanged();
        } else {
          regionsInTransitionBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder setRegionsInTransition(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder builderForValue) {
        if (regionsInTransitionBuilder_ == null) {
          ensureRegionsInTransitionIsMutable();
          regionsInTransition_.set(index, builderForValue.build());
          onChanged();
        } else {
          regionsInTransitionBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder addRegionsInTransition(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition value) {
        if (regionsInTransitionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionsInTransitionIsMutable();
          regionsInTransition_.add(value);
          onChanged();
        } else {
          regionsInTransitionBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder addRegionsInTransition(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition value) {
        if (regionsInTransitionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegionsInTransitionIsMutable();
          regionsInTransition_.add(index, value);
          onChanged();
        } else {
          regionsInTransitionBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder addRegionsInTransition(
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder builderForValue) {
        if (regionsInTransitionBuilder_ == null) {
          ensureRegionsInTransitionIsMutable();
          regionsInTransition_.add(builderForValue.build());
          onChanged();
        } else {
          regionsInTransitionBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder addRegionsInTransition(
          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder builderForValue) {
        if (regionsInTransitionBuilder_ == null) {
          ensureRegionsInTransitionIsMutable();
          regionsInTransition_.add(index, builderForValue.build());
          onChanged();
        } else {
          regionsInTransitionBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder addAllRegionsInTransition(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition> values) {
        if (regionsInTransitionBuilder_ == null) {
          ensureRegionsInTransitionIsMutable();
          super.addAll(values, regionsInTransition_);
          onChanged();
        } else {
          regionsInTransitionBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder clearRegionsInTransition() {
        if (regionsInTransitionBuilder_ == null) {
          regionsInTransition_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          regionsInTransitionBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public Builder removeRegionsInTransition(int index) {
        if (regionsInTransitionBuilder_ == null) {
          ensureRegionsInTransitionIsMutable();
          regionsInTransition_.remove(index);
          onChanged();
        } else {
          regionsInTransitionBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder getRegionsInTransitionBuilder(
          int index) {
        return getRegionsInTransitionFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder getRegionsInTransitionOrBuilder(
          int index) {
        if (regionsInTransitionBuilder_ == null) {
          return regionsInTransition_.get(index);  } else {
          return regionsInTransitionBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder> 
           getRegionsInTransitionOrBuilderList() {
        if (regionsInTransitionBuilder_ != null) {
          return regionsInTransitionBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(regionsInTransition_);
        }
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder addRegionsInTransitionBuilder() {
        return getRegionsInTransitionFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.getDefaultInstance());
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder addRegionsInTransitionBuilder(
          int index) {
        return getRegionsInTransitionFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.getDefaultInstance());
      }
      /**
       * <code>repeated .RegionInTransition regions_in_transition = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder> 
           getRegionsInTransitionBuilderList() {
        return getRegionsInTransitionFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder> 
          getRegionsInTransitionFieldBuilder() {
        if (regionsInTransitionBuilder_ == null) {
          regionsInTransitionBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransition.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionInTransitionOrBuilder>(
                  regionsInTransition_,
                  ((bitField0_ & 0x00000008) == 0x00000008),
                  getParentForChildren(),
                  isClean());
          regionsInTransition_ = null;
        }
        return regionsInTransitionBuilder_;
      }

      // optional .ClusterId cluster_id = 5;
      private org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId clusterId_ = org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId, org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterIdOrBuilder> clusterIdBuilder_;
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public boolean hasClusterId() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId getClusterId() {
        if (clusterIdBuilder_ == null) {
          return clusterId_;
        } else {
          return clusterIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public Builder setClusterId(org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId value) {
        if (clusterIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          clusterId_ = value;
          onChanged();
        } else {
          clusterIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public Builder setClusterId(
          org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.Builder builderForValue) {
        if (clusterIdBuilder_ == null) {
          clusterId_ = builderForValue.build();
          onChanged();
        } else {
          clusterIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public Builder mergeClusterId(org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId value) {
        if (clusterIdBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              clusterId_ != org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.getDefaultInstance()) {
            clusterId_ =
              org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.newBuilder(clusterId_).mergeFrom(value).buildPartial();
          } else {
            clusterId_ = value;
          }
          onChanged();
        } else {
          clusterIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public Builder clearClusterId() {
        if (clusterIdBuilder_ == null) {
          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.getDefaultInstance();
          onChanged();
        } else {
          clusterIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.Builder getClusterIdBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getClusterIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterIdOrBuilder getClusterIdOrBuilder() {
        if (clusterIdBuilder_ != null) {
          return clusterIdBuilder_.getMessageOrBuilder();
        } else {
          return clusterId_;
        }
      }
      /**
       * <code>optional .ClusterId cluster_id = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId, org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterIdOrBuilder> 
          getClusterIdFieldBuilder() {
        if (clusterIdBuilder_ == null) {
          clusterIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId, org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterId.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.ClusterIdOrBuilder>(
                  clusterId_,
                  getParentForChildren(),
                  isClean());
          clusterId_ = null;
        }
        return clusterIdBuilder_;
      }

      // repeated .Coprocessor master_coprocessors = 6;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> masterCoprocessors_ =
        java.util.Collections.emptyList();
      private void ensureMasterCoprocessorsIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          masterCoprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>(masterCoprocessors_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> masterCoprocessorsBuilder_;

      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getMasterCoprocessorsList() {
        if (masterCoprocessorsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(masterCoprocessors_);
        } else {
          return masterCoprocessorsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public int getMasterCoprocessorsCount() {
        if (masterCoprocessorsBuilder_ == null) {
          return masterCoprocessors_.size();
        } else {
          return masterCoprocessorsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getMasterCoprocessors(int index) {
        if (masterCoprocessorsBuilder_ == null) {
          return masterCoprocessors_.get(index);
        } else {
          return masterCoprocessorsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder setMasterCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
        if (masterCoprocessorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMasterCoprocessorsIsMutable();
          masterCoprocessors_.set(index, value);
          onChanged();
        } else {
          masterCoprocessorsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder setMasterCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
        if (masterCoprocessorsBuilder_ == null) {
          ensureMasterCoprocessorsIsMutable();
          masterCoprocessors_.set(index, builderForValue.build());
          onChanged();
        } else {
          masterCoprocessorsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder addMasterCoprocessors(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
        if (masterCoprocessorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMasterCoprocessorsIsMutable();
          masterCoprocessors_.add(value);
          onChanged();
        } else {
          masterCoprocessorsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder addMasterCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
        if (masterCoprocessorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMasterCoprocessorsIsMutable();
          masterCoprocessors_.add(index, value);
          onChanged();
        } else {
          masterCoprocessorsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder addMasterCoprocessors(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
        if (masterCoprocessorsBuilder_ == null) {
          ensureMasterCoprocessorsIsMutable();
          masterCoprocessors_.add(builderForValue.build());
          onChanged();
        } else {
          masterCoprocessorsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder addMasterCoprocessors(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
        if (masterCoprocessorsBuilder_ == null) {
          ensureMasterCoprocessorsIsMutable();
          masterCoprocessors_.add(index, builderForValue.build());
          onChanged();
        } else {
          masterCoprocessorsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder addAllMasterCoprocessors(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> values) {
        if (masterCoprocessorsBuilder_ == null) {
          ensureMasterCoprocessorsIsMutable();
          super.addAll(values, masterCoprocessors_);
          onChanged();
        } else {
          masterCoprocessorsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder clearMasterCoprocessors() {
        if (masterCoprocessorsBuilder_ == null) {
          masterCoprocessors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          masterCoprocessorsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public Builder removeMasterCoprocessors(int index) {
        if (masterCoprocessorsBuilder_ == null) {
          ensureMasterCoprocessorsIsMutable();
          masterCoprocessors_.remove(index);
          onChanged();
        } else {
          masterCoprocessorsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder getMasterCoprocessorsBuilder(
          int index) {
        return getMasterCoprocessorsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getMasterCoprocessorsOrBuilder(
          int index) {
        if (masterCoprocessorsBuilder_ == null) {
          return masterCoprocessors_.get(index);  } else {
          return masterCoprocessorsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
           getMasterCoprocessorsOrBuilderList() {
        if (masterCoprocessorsBuilder_ != null) {
          return masterCoprocessorsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(masterCoprocessors_);
        }
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addMasterCoprocessorsBuilder() {
        return getMasterCoprocessorsFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addMasterCoprocessorsBuilder(
          int index) {
        return getMasterCoprocessorsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
      }
      /**
       * <code>repeated .Coprocessor master_coprocessors = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder> 
           getMasterCoprocessorsBuilderList() {
        return getMasterCoprocessorsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
          getMasterCoprocessorsFieldBuilder() {
        if (masterCoprocessorsBuilder_ == null) {
          masterCoprocessorsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder>(
                  masterCoprocessors_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          masterCoprocessors_ = null;
        }
        return masterCoprocessorsBuilder_;
      }

      // optional .ServerName master = 7;
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName master_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> masterBuilder_;
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public boolean hasMaster() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getMaster() {
        if (masterBuilder_ == null) {
          return master_;
        } else {
          return masterBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public Builder setMaster(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (masterBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          master_ = value;
          onChanged();
        } else {
          masterBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public Builder setMaster(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (masterBuilder_ == null) {
          master_ = builderForValue.build();
          onChanged();
        } else {
          masterBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public Builder mergeMaster(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (masterBuilder_ == null) {
          if (((bitField0_ & 0x00000040) == 0x00000040) &&
              master_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance()) {
            master_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.newBuilder(master_).mergeFrom(value).buildPartial();
          } else {
            master_ = value;
          }
          onChanged();
        } else {
          masterBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public Builder clearMaster() {
        if (masterBuilder_ == null) {
          master_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
          onChanged();
        } else {
          masterBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder getMasterBuilder() {
        bitField0_ |= 0x00000040;
        onChanged();
        return getMasterFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getMasterOrBuilder() {
        if (masterBuilder_ != null) {
          return masterBuilder_.getMessageOrBuilder();
        } else {
          return master_;
        }
      }
      /**
       * <code>optional .ServerName master = 7;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
          getMasterFieldBuilder() {
        if (masterBuilder_ == null) {
          masterBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder>(
                  master_,
                  getParentForChildren(),
                  isClean());
          master_ = null;
        }
        return masterBuilder_;
      }

      // repeated .ServerName backup_masters = 8;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> backupMasters_ =
        java.util.Collections.emptyList();
      private void ensureBackupMastersIsMutable() {
        if (!((bitField0_ & 0x00000080) == 0x00000080)) {
          backupMasters_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>(backupMasters_);
          bitField0_ |= 0x00000080;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> backupMastersBuilder_;

      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getBackupMastersList() {
        if (backupMastersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(backupMasters_);
        } else {
          return backupMastersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public int getBackupMastersCount() {
        if (backupMastersBuilder_ == null) {
          return backupMasters_.size();
        } else {
          return backupMastersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getBackupMasters(int index) {
        if (backupMastersBuilder_ == null) {
          return backupMasters_.get(index);
        } else {
          return backupMastersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder setBackupMasters(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (backupMastersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBackupMastersIsMutable();
          backupMasters_.set(index, value);
          onChanged();
        } else {
          backupMastersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder setBackupMasters(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (backupMastersBuilder_ == null) {
          ensureBackupMastersIsMutable();
          backupMasters_.set(index, builderForValue.build());
          onChanged();
        } else {
          backupMastersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder addBackupMasters(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (backupMastersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBackupMastersIsMutable();
          backupMasters_.add(value);
          onChanged();
        } else {
          backupMastersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder addBackupMasters(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (backupMastersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBackupMastersIsMutable();
          backupMasters_.add(index, value);
          onChanged();
        } else {
          backupMastersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder addBackupMasters(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (backupMastersBuilder_ == null) {
          ensureBackupMastersIsMutable();
          backupMasters_.add(builderForValue.build());
          onChanged();
        } else {
          backupMastersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder addBackupMasters(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (backupMastersBuilder_ == null) {
          ensureBackupMastersIsMutable();
          backupMasters_.add(index, builderForValue.build());
          onChanged();
        } else {
          backupMastersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder addAllBackupMasters(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> values) {
        if (backupMastersBuilder_ == null) {
          ensureBackupMastersIsMutable();
          super.addAll(values, backupMasters_);
          onChanged();
        } else {
          backupMastersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder clearBackupMasters() {
        if (backupMastersBuilder_ == null) {
          backupMasters_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
          onChanged();
        } else {
          backupMastersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public Builder removeBackupMasters(int index) {
        if (backupMastersBuilder_ == null) {
          ensureBackupMastersIsMutable();
          backupMasters_.remove(index);
          onChanged();
        } else {
          backupMastersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder getBackupMastersBuilder(
          int index) {
        return getBackupMastersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getBackupMastersOrBuilder(
          int index) {
        if (backupMastersBuilder_ == null) {
          return backupMasters_.get(index);  } else {
          return backupMastersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
           getBackupMastersOrBuilderList() {
        if (backupMastersBuilder_ != null) {
          return backupMastersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(backupMasters_);
        }
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addBackupMastersBuilder() {
        return getBackupMastersFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addBackupMastersBuilder(
          int index) {
        return getBackupMastersFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
      }
      /**
       * <code>repeated .ServerName backup_masters = 8;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder> 
           getBackupMastersBuilderList() {
        return getBackupMastersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
          getBackupMastersFieldBuilder() {
        if (backupMastersBuilder_ == null) {
          backupMastersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder>(
                  backupMasters_,
                  ((bitField0_ & 0x00000080) == 0x00000080),
                  getParentForChildren(),
                  isClean());
          backupMasters_ = null;
        }
        return backupMastersBuilder_;
      }

      // optional bool balancer_on = 9;
      private boolean balancerOn_ ;
      /**
       * <code>optional bool balancer_on = 9;</code>
       */
      public boolean hasBalancerOn() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional bool balancer_on = 9;</code>
       */
      public boolean getBalancerOn() {
        return balancerOn_;
      }
      /**
       * <code>optional bool balancer_on = 9;</code>
       */
      public Builder setBalancerOn(boolean value) {
        bitField0_ |= 0x00000100;
        balancerOn_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool balancer_on = 9;</code>
       */
      public Builder clearBalancerOn() {
        bitField0_ = (bitField0_ & ~0x00000100);
        balancerOn_ = false;
        onChanged();
        return this;
      }

      // repeated .ServerName servers_name = 10;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> serversName_ =
        java.util.Collections.emptyList();
      private void ensureServersNameIsMutable() {
        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
          serversName_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>(serversName_);
          bitField0_ |= 0x00000200;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> serversNameBuilder_;

      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getServersNameList() {
        if (serversNameBuilder_ == null) {
          return java.util.Collections.unmodifiableList(serversName_);
        } else {
          return serversNameBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public int getServersNameCount() {
        if (serversNameBuilder_ == null) {
          return serversName_.size();
        } else {
          return serversNameBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getServersName(int index) {
        if (serversNameBuilder_ == null) {
          return serversName_.get(index);
        } else {
          return serversNameBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder setServersName(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (serversNameBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServersNameIsMutable();
          serversName_.set(index, value);
          onChanged();
        } else {
          serversNameBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder setServersName(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (serversNameBuilder_ == null) {
          ensureServersNameIsMutable();
          serversName_.set(index, builderForValue.build());
          onChanged();
        } else {
          serversNameBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder addServersName(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (serversNameBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServersNameIsMutable();
          serversName_.add(value);
          onChanged();
        } else {
          serversNameBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder addServersName(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
        if (serversNameBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServersNameIsMutable();
          serversName_.add(index, value);
          onChanged();
        } else {
          serversNameBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder addServersName(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (serversNameBuilder_ == null) {
          ensureServersNameIsMutable();
          serversName_.add(builderForValue.build());
          onChanged();
        } else {
          serversNameBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder addServersName(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
        if (serversNameBuilder_ == null) {
          ensureServersNameIsMutable();
          serversName_.add(index, builderForValue.build());
          onChanged();
        } else {
          serversNameBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder addAllServersName(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> values) {
        if (serversNameBuilder_ == null) {
          ensureServersNameIsMutable();
          super.addAll(values, serversName_);
          onChanged();
        } else {
          serversNameBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder clearServersName() {
        if (serversNameBuilder_ == null) {
          serversName_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          serversNameBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public Builder removeServersName(int index) {
        if (serversNameBuilder_ == null) {
          ensureServersNameIsMutable();
          serversName_.remove(index);
          onChanged();
        } else {
          serversNameBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder getServersNameBuilder(
          int index) {
        return getServersNameFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServersNameOrBuilder(
          int index) {
        if (serversNameBuilder_ == null) {
          return serversName_.get(index);  } else {
          return serversNameBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
           getServersNameOrBuilderList() {
        if (serversNameBuilder_ != null) {
          return serversNameBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(serversName_);
        }
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addServersNameBuilder() {
        return getServersNameFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addServersNameBuilder(
          int index) {
        return getServersNameFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
      }
      /**
       * <code>repeated .ServerName servers_name = 10;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder> 
           getServersNameBuilderList() {
        return getServersNameFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
          getServersNameFieldBuilder() {
        if (serversNameBuilder_ == null) {
          serversNameBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder>(
                  serversName_,
                  ((bitField0_ & 0x00000200) == 0x00000200),
                  getParentForChildren(),
                  isClean());
          serversName_ = null;
        }
        return serversNameBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:ClusterStatus)
    }

    static {
      defaultInstance = new ClusterStatus(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:ClusterStatus)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_RegionState_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_RegionState_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_RegionInTransition_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_RegionInTransition_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_FamilyInfo_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_FamilyInfo_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_RegionLoad_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_RegionLoad_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_ReplicationLoadSink_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_ReplicationLoadSink_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_ReplicationLoadSource_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_ReplicationLoadSource_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_ServerLoad_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_ServerLoad_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_RegionServerTableLatency_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_RegionServerTableLatency_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LiveServerInfo_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LiveServerInfo_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_ClusterStatus_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_ClusterStatus_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\023ClusterStatus.proto\032\013HBase.proto\032\017Clus" +
      "terId.proto\032\010FS.proto\"\307\002\n\013RegionState\022 \n" +
      "\013region_info\030\001 \002(\0132\013.RegionInfo\022!\n\005state" +
      "\030\002 \002(\0162\022.RegionState.State\022\r\n\005stamp\030\003 \001(" +
      "\004\"\343\001\n\005State\022\013\n\007OFFLINE\020\000\022\020\n\014PENDING_OPEN" +
      "\020\001\022\013\n\007OPENING\020\002\022\010\n\004OPEN\020\003\022\021\n\rPENDING_CLO" +
      "SE\020\004\022\013\n\007CLOSING\020\005\022\n\n\006CLOSED\020\006\022\r\n\tSPLITTI" +
      "NG\020\007\022\t\n\005SPLIT\020\010\022\017\n\013FAILED_OPEN\020\t\022\020\n\014FAIL" +
      "ED_CLOSE\020\n\022\013\n\007MERGING\020\013\022\n\n\006MERGED\020\014\022\021\n\rS" +
      "PLITTING_NEW\020\r\022\017\n\013MERGING_NEW\020\016\"X\n\022Regio",
      "nInTransition\022\036\n\004spec\030\001 \002(\0132\020.RegionSpec" +
      "ifier\022\"\n\014region_state\030\002 \002(\0132\014.RegionStat" +
      "e\"u\n\nFamilyInfo\022\022\n\nfamilyname\030\001 \002(\t\022\021\n\tr" +
      "ow_count\030\002 \002(\004\022\020\n\010kv_count\030\003 \002(\004\022\024\n\014del_" +
      "kv_count\030\004 \002(\004\022\030\n\020del_family_count\030\005 \002(\004" +
      "\"\351\010\n\nRegionLoad\022*\n\020region_specifier\030\001 \002(" +
      "\0132\020.RegionSpecifier\022\016\n\006stores\030\002 \001(\r\022\022\n\ns" +
      "torefiles\030\003 \001(\r\022\"\n\032store_uncompressed_si" +
      "ze_MB\030\004 \001(\r\022\031\n\021storefile_size_MB\030\005 \001(\r\022\030" +
      "\n\020memstore_size_MB\030\006 \001(\r\022\037\n\027storefile_in",
      "dex_size_MB\030\007 \001(\r\022\033\n\023read_requests_count" +
      "\030\010 \001(\004\022\034\n\024write_requests_count\030\t \001(\004\022\034\n\024" +
      "total_compacting_KVs\030\n \001(\004\022\035\n\025current_co" +
      "mpacted_KVs\030\013 \001(\004\022\032\n\022root_index_size_KB\030" +
      "\014 \001(\r\022\"\n\032total_static_index_size_KB\030\r \001(" +
      "\r\022\"\n\032total_static_bloom_size_KB\030\016 \001(\r\022\034\n" +
      "\024complete_sequence_id\030\017 \001(\004\022\025\n\rdata_loca" +
      "lity\030\020 \001(\002\022 \n\030read_requests_per_second\030\021" +
      " \001(\004\022!\n\031write_requests_per_second\030\022 \001(\004\022" +
      "1\n)read_requests_by_capacity_unit_per_se",
      "cond\030\023 \001(\004\0222\n*write_requests_by_capacity" +
      "_unit_per_second\030\024 \001(\004\022%\n\035throttled_read" +
      "_requests_count\030\025 \001(\004\022&\n\036throttled_write" +
      "_requests_count\030\026 \001(\004\022\032\n\022get_requests_co" +
      "unt\030\027 \001(\004\022\"\n\032read_cell_count_per_second\030" +
      "\030 \001(\004\022&\n\036read_raw_cell_count_per_second\030" +
      "\031 \001(\004\022\035\n\025scan_count_per_second\030\032 \001(\004\022\034\n\024" +
      "scan_rows_per_second\030\033 \001(\004\022 \n\013family_inf" +
      "o\030\034 \003(\0132\013.FamilyInfo\022%\n\035user_read_reques" +
      "ts_per_second\030\035 \001(\004\022&\n\036user_write_reques",
      "ts_per_second\030\036 \001(\004\0226\n.user_read_request" +
      "s_by_capacity_unit_per_second\030\037 \001(\004\0227\n/u" +
      "ser_write_requests_by_capacity_unit_per_" +
      "second\030  \001(\004\"T\n\023ReplicationLoadSink\022\032\n\022a" +
      "geOfLastAppliedOp\030\001 \002(\004\022!\n\031timeStampsOfL" +
      "astAppliedOp\030\002 \002(\004\"\225\001\n\025ReplicationLoadSo" +
      "urce\022\016\n\006peerID\030\001 \002(\t\022\032\n\022ageOfLastShipped" +
      "Op\030\002 \002(\004\022\026\n\016sizeOfLogQueue\030\003 \002(\r\022 \n\030time" +
      "StampOfLastShippedOp\030\004 \002(\004\022\026\n\016replicatio" +
      "nLag\030\005 \002(\004\"\364\004\n\nServerLoad\022\032\n\022number_of_r",
      "equests\030\001 \001(\r\022 \n\030total_number_of_request" +
      "s\030\002 \001(\r\022\024\n\014used_heap_MB\030\003 \001(\r\022\023\n\013max_hea" +
      "p_MB\030\004 \001(\r\022!\n\014region_loads\030\005 \003(\0132\013.Regio" +
      "nLoad\022\"\n\014coprocessors\030\006 \003(\0132\014.Coprocesso" +
      "r\022\031\n\021report_start_time\030\007 \001(\004\022\027\n\017report_e" +
      "nd_time\030\010 \001(\004\022\030\n\020info_server_port\030\t \001(\r\022" +
      ".\n\016replLoadSource\030\n \003(\0132\026.ReplicationLoa" +
      "dSource\022*\n\014replLoadSink\030\013 \001(\0132\024.Replicat" +
      "ionLoadSink\022 \n\030read_requests_per_second\030" +
      "\014 \001(\004\022!\n\031write_requests_per_second\030\r \001(\004",
      "\022\"\n\032read_cell_count_per_second\030\016 \001(\004\022&\n\036" +
      "read_raw_cell_count_per_second\030\017 \001(\004\022\035\n\025" +
      "scan_count_per_second\030\020 \001(\004\022\034\n\024scan_rows" +
      "_per_second\030\021 \001(\004\022>\n\033region_server_table" +
      "_latency\030\022 \003(\0132\031.RegionServerTableLatenc" +
      "y\"\273\004\n\030RegionServerTableLatency\022\022\n\ntable_" +
      "name\030\001 \002(\t\022\025\n\rget_time_mean\030\002 \001(\004\022\025\n\rput" +
      "_time_mean\030\003 \001(\004\022\026\n\016scan_time_mean\030\004 \001(\004" +
      "\022\027\n\017batch_time_mean\030\005 \001(\004\022\030\n\020append_time" +
      "_mean\030\006 \001(\004\022\030\n\020delete_time_mean\030\007 \001(\004\022\033\n",
      "\023increment_time_mean\030\010 \001(\004\022\033\n\023get_operat" +
      "ion_count\030\t \001(\004\022\033\n\023put_operation_count\030\n" +
      " \001(\004\022\034\n\024scan_operation_count\030\013 \001(\004\022\035\n\025ba" +
      "tch_operation_count\030\014 \001(\004\022\036\n\026append_oper" +
      "ation_count\030\r \001(\004\022\036\n\026delete_operation_co" +
      "unt\030\016 \001(\004\022!\n\031increment_operation_count\030\017" +
      " \001(\004\022\036\n\026get_time_99_percentile\030\020 \001(\004\022\036\n\026" +
      "put_time_99_percentile\030\021 \001(\004\022\037\n\027scan_tim" +
      "e_99_percentile\030\022 \001(\004\022 \n\030batch_time_99_p" +
      "ercentile\030\023 \001(\004\"O\n\016LiveServerInfo\022\033\n\006ser",
      "ver\030\001 \002(\0132\013.ServerName\022 \n\013server_load\030\002 " +
      "\002(\0132\013.ServerLoad\"\203\003\n\rClusterStatus\022/\n\rhb" +
      "ase_version\030\001 \001(\0132\030.HBaseVersionFileCont" +
      "ent\022%\n\014live_servers\030\002 \003(\0132\017.LiveServerIn" +
      "fo\022!\n\014dead_servers\030\003 \003(\0132\013.ServerName\0222\n" +
      "\025regions_in_transition\030\004 \003(\0132\023.RegionInT" +
      "ransition\022\036\n\ncluster_id\030\005 \001(\0132\n.ClusterI" +
      "d\022)\n\023master_coprocessors\030\006 \003(\0132\014.Coproce" +
      "ssor\022\033\n\006master\030\007 \001(\0132\013.ServerName\022#\n\016bac" +
      "kup_masters\030\010 \003(\0132\013.ServerName\022\023\n\013balanc",
      "er_on\030\t \001(\010\022!\n\014servers_name\030\n \003(\0132\013.Serv" +
      "erName*\334\001\n\006Option\022\021\n\rHBASE_VERSION\020\000\022\016\n\n" +
      "CLUSTER_ID\020\001\022\020\n\014LIVE_SERVERS\020\002\022\020\n\014DEAD_S" +
      "ERVERS\020\003\022\n\n\006MASTER\020\004\022\022\n\016BACKUP_MASTERS\020\005" +
      "\022\027\n\023MASTER_COPROCESSORS\020\006\022\031\n\025REGIONS_IN_" +
      "TRANSITION\020\007\022\017\n\013BALANCER_ON\020\010\022\024\n\020MASTER_" +
      "INFO_PORT\020\t\022\020\n\014SERVERS_NAME\020\nBF\n*org.apa" +
      "che.hadoop.hbase.protobuf.generatedB\023Clu" +
      "sterStatusProtosH\001\240\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_RegionState_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_RegionState_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_RegionState_descriptor,
              new java.lang.String[] { "RegionInfo", "State", "Stamp", });
          internal_static_RegionInTransition_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_RegionInTransition_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_RegionInTransition_descriptor,
              new java.lang.String[] { "Spec", "RegionState", });
          internal_static_FamilyInfo_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_FamilyInfo_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_FamilyInfo_descriptor,
              new java.lang.String[] { "Familyname", "RowCount", "KvCount", "DelKvCount", "DelFamilyCount", });
          internal_static_RegionLoad_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_RegionLoad_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_RegionLoad_descriptor,
              new java.lang.String[] { "RegionSpecifier", "Stores", "Storefiles", "StoreUncompressedSizeMB", "StorefileSizeMB", "MemstoreSizeMB", "StorefileIndexSizeMB", "ReadRequestsCount", "WriteRequestsCount", "TotalCompactingKVs", "CurrentCompactedKVs", "RootIndexSizeKB", "TotalStaticIndexSizeKB", "TotalStaticBloomSizeKB", "CompleteSequenceId", "DataLocality", "ReadRequestsPerSecond", "WriteRequestsPerSecond", "ReadRequestsByCapacityUnitPerSecond", "WriteRequestsByCapacityUnitPerSecond", "ThrottledReadRequestsCount", "ThrottledWriteRequestsCount", "GetRequestsCount", "ReadCellCountPerSecond", "ReadRawCellCountPerSecond", "ScanCountPerSecond", "ScanRowsPerSecond", "FamilyInfo", "UserReadRequestsPerSecond", "UserWriteRequestsPerSecond", "UserReadRequestsByCapacityUnitPerSecond", "UserWriteRequestsByCapacityUnitPerSecond", });
          internal_static_ReplicationLoadSink_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_ReplicationLoadSink_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_ReplicationLoadSink_descriptor,
              new java.lang.String[] { "AgeOfLastAppliedOp", "TimeStampsOfLastAppliedOp", });
          internal_static_ReplicationLoadSource_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_ReplicationLoadSource_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_ReplicationLoadSource_descriptor,
              new java.lang.String[] { "PeerID", "AgeOfLastShippedOp", "SizeOfLogQueue", "TimeStampOfLastShippedOp", "ReplicationLag", });
          internal_static_ServerLoad_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_ServerLoad_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_ServerLoad_descriptor,
              new java.lang.String[] { "NumberOfRequests", "TotalNumberOfRequests", "UsedHeapMB", "MaxHeapMB", "RegionLoads", "Coprocessors", "ReportStartTime", "ReportEndTime", "InfoServerPort", "ReplLoadSource", "ReplLoadSink", "ReadRequestsPerSecond", "WriteRequestsPerSecond", "ReadCellCountPerSecond", "ReadRawCellCountPerSecond", "ScanCountPerSecond", "ScanRowsPerSecond", "RegionServerTableLatency", });
          internal_static_RegionServerTableLatency_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_RegionServerTableLatency_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_RegionServerTableLatency_descriptor,
              new java.lang.String[] { "TableName", "GetTimeMean", "PutTimeMean", "ScanTimeMean", "BatchTimeMean", "AppendTimeMean", "DeleteTimeMean", "IncrementTimeMean", "GetOperationCount", "PutOperationCount", "ScanOperationCount", "BatchOperationCount", "AppendOperationCount", "DeleteOperationCount", "IncrementOperationCount", "GetTime99Percentile", "PutTime99Percentile", "ScanTime99Percentile", "BatchTime99Percentile", });
          internal_static_LiveServerInfo_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_LiveServerInfo_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LiveServerInfo_descriptor,
              new java.lang.String[] { "Server", "ServerLoad", });
          internal_static_ClusterStatus_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_ClusterStatus_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_ClusterStatus_descriptor,
              new java.lang.String[] { "HbaseVersion", "LiveServers", "DeadServers", "RegionsInTransition", "ClusterId", "MasterCoprocessors", "Master", "BackupMasters", "BalancerOn", "ServersName", });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.getDescriptor(),
          org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.getDescriptor(),
          org.apache.hadoop.hbase.protobuf.generated.FSProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
